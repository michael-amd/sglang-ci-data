INFO 10-13 13:14:30 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:33] server_args=ServerArgs(model_path='/data/models/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_path='/data/models/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.95, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=131072, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=482040014, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, crash_on_nan=False, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/data/models/amd-DeepSeek-R1-MXFP4-Preview', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.2, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-13 13:14:33] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:34] Using default HuggingFace chat template with detected content format: string
INFO 10-13 13:14:38 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:38 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:41] Launch DP0 starting at GPU #0.
[2025-10-13 13:14:41] Launch DP1 starting at GPU #8.
[2025-10-13 13:14:41] Launch DP2 starting at GPU #16.
[2025-10-13 13:14:41] Launch DP3 starting at GPU #24.
[2025-10-13 13:14:41] Launch DP4 starting at GPU #32.
[2025-10-13 13:14:41] Launch DP5 starting at GPU #40.
[2025-10-13 13:14:41] Launch DP6 starting at GPU #48.
[2025-10-13 13:14:41] Launch DP7 starting at GPU #56.
INFO 10-13 13:14:47 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:47 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:48 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:48 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:49 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:49 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:50 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
[2025-10-13 13:14:51 DP4 TP5] Process 56948 gpu_id 37 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:14:51 DP4 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
[2025-10-13 13:14:51 DP4 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
[2025-10-13 13:14:51 DP4 TP5] Context: self.device='cuda' self.gpu_id=37 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:14:51 DP4 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:51 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
[2025-10-13 13:14:52 DP3 TP0] Process 55922 gpu_id 24 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:14:52 DP3 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
INFO 10-13 13:14:52 [__init__.py:241] Automatically detected platform rocm.
[2025-10-13 13:14:52 DP3 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:52 DP3 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:14:52 DP3 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:14:52 DP3 TP0] Init torch distributed begin.
[2025-10-13 13:14:52 DP3 TP0] Context: self.device='cuda' self.gpu_id=24 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:14:52 DP3 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:53 DP1 TP3] Process 55925 gpu_id 11 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:14:53 DP1 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:53 DP1 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:53 DP1 TP3] Context: self.device='cuda' self.gpu_id=11 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:14:53 DP1 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:53 DP2 TP6] Process 56958 gpu_id 22 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:14:53 DP2 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:53 DP2 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:53 DP2 TP6] Context: self.device='cuda' self.gpu_id=22 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:14:53 DP2 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:54 DP2 TP3] Process 56955 gpu_id 19 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:14:54 DP2 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:55 DP2 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:55 DP2 TP3] Context: self.device='cuda' self.gpu_id=19 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:14:55 DP2 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:55 DP1 TP6] Process 56962 gpu_id 14 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:14:55 DP1 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:55 DP1 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:55 DP1 TP6] Context: self.device='cuda' self.gpu_id=14 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:14:55 DP1 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:56 DP0 TP4] Process 56964 gpu_id 4 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:14:56 DP0 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:57 DP0 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:57 DP4 TP4] Process 56947 gpu_id 36 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:14:57 DP4 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:57 DP4 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:57 DP4 TP4] Context: self.device='cuda' self.gpu_id=36 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:14:57 DP4 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:57 DP3 TP3] Process 58116 gpu_id 27 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:14:57 DP3 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:58 DP3 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP5 TP1] Process 56953 gpu_id 41 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:58 DP5 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP3 TP3] Context: self.device='cuda' self.gpu_id=27 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:14:58 DP3 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:58 DP5 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP5 TP1] Context: self.device='cuda' self.gpu_id=41 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:14:58 DP5 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:58 DP7 TP6] Process 58378 gpu_id 62 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:14:58 DP7 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP6 TP3] Process 56951 gpu_id 51 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:58 DP6 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP4 TP1] Process 56877 gpu_id 33 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:58 DP4 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP2 TP2] Process 55929 gpu_id 18 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:14:58 DP0 TP7] Process 58115 gpu_id 7 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:14:58 DP4 TP2] Process 56945 gpu_id 34 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:14:58 DP0 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP2 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP4 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:58 DP3 TP1] Process 55927 gpu_id 25 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:58 DP3 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:58 DP6 TP1] Process 56878 gpu_id 49 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:58 DP6 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:59 DP1 TP4] Process 55928 gpu_id 12 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:14:59 DP1 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP7 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP1 TP1] Process 55918 gpu_id 9 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:59 DP1 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP0 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP6 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP7 TP6] Context: self.device='cuda' self.gpu_id=62 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:14:59 DP7 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP6 TP0] Process 55930 gpu_id 48 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:14:59 DP6 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP6 TP3] Context: self.device='cuda' self.gpu_id=51 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:14:59 DP6 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP4 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP2 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP3 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP4 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP4 TP1] Context: self.device='cuda' self.gpu_id=33 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:14:59 DP4 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP2 TP2] Context: self.device='cuda' self.gpu_id=18 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:14:59 DP2 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP1 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP3 TP1] Context: self.device='cuda' self.gpu_id=25 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:14:59 DP3 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP6 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:59 DP4 TP2] Context: self.device='cuda' self.gpu_id=34 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:14:59 DP4 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP1 TP4] Context: self.device='cuda' self.gpu_id=12 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:14:59 DP1 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP6 TP1] Context: self.device='cuda' self.gpu_id=49 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:14:59 DP6 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP1 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP1 TP2] Process 55921 gpu_id 10 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:14:59 DP1 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP6 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP6 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:14:59 DP6 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:14:59 DP6 TP0] Init torch distributed begin.
[2025-10-13 13:14:59 DP1 TP1] Context: self.device='cuda' self.gpu_id=9 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:14:59 DP1 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP6 TP0] Context: self.device='cuda' self.gpu_id=48 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:14:59 DP6 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:59 DP2 TP5] Process 56957 gpu_id 21 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:14:59 DP2 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:14:59 DP1 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP0 TP1] Process 55920 gpu_id 1 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:14:59 DP0 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP4 TP0] Process 55924 gpu_id 32 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:14:59 DP4 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP1 TP2] Context: self.device='cuda' self.gpu_id=10 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:14:59 DP1 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP5 TP5] Process 59644 gpu_id 45 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:14:59 DP5 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP4 TP6] Process 56949 gpu_id 38 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:14:59 DP4 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP2 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP2 TP5] Context: self.device='cuda' self.gpu_id=21 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:14:59 DP2 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:14:59 DP5 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:14:59 DP4 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP5 TP5] Context: self.device='cuda' self.gpu_id=45 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:15:00 DP5 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:00 DP4 TP6] Context: self.device='cuda' self.gpu_id=38 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:15:00 DP4 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:00 DP4 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP4 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:00 DP4 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:00 DP4 TP0] Init torch distributed begin.
[2025-10-13 13:15:00 DP0 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP4 TP0] Context: self.device='cuda' self.gpu_id=32 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:15:00 DP4 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:00 DP2 TP0] Process 55916 gpu_id 16 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:15:00 DP2 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:00 DP7 TP4] Process 58171 gpu_id 60 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:15:00 DP0 TP2] Process 55926 gpu_id 2 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:15:00 DP7 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP0 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP7 TP5] Process 58352 gpu_id 61 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:15:00 DP7 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP7 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP7 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP2 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:00 DP2 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:00 DP2 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:00 DP2 TP0] Init torch distributed begin.
[2025-10-13 13:15:00 DP7 TP4] Context: self.device='cuda' self.gpu_id=60 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:15:00 DP7 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:00 DP7 TP5] Context: self.device='cuda' self.gpu_id=61 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:15:00 DP7 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:00 DP2 TP0] Context: self.device='cuda' self.gpu_id=16 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:15:00 DP2 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:00 DP0 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:01 DP0 TP0] Process 55912 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:15:01 DP0 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP1 TP7] Process 56963 gpu_id 15 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:01 DP1 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:01 DP1 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP4 TP7] Process 56950 gpu_id 39 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:01 DP3 TP7] Process 58810 gpu_id 31 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:01 DP4 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP1 TP7] Context: self.device='cuda' self.gpu_id=15 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:01 DP1 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:01 DP7 TP7] Process 58808 gpu_id 63 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:01 DP3 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP7 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP0 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP0 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:01 DP0 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:01 DP0 TP0] Init torch distributed begin.
[2025-10-13 13:15:01 DP3 TP6] Process 58809 gpu_id 30 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:15:01 DP3 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP4 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP3 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP4 TP7] Context: self.device='cuda' self.gpu_id=39 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:01 DP4 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:01 DP7 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP3 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP3 TP6] Context: self.device='cuda' self.gpu_id=30 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:15:01 DP3 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:01 DP3 TP7] Context: self.device='cuda' self.gpu_id=31 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:01 DP7 TP7] Context: self.device='cuda' self.gpu_id=63 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:01 DP7 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:01 DP3 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:01 DP7 TP1] Process 56966 gpu_id 57 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:15:01 DP7 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP5 TP4] Process 59495 gpu_id 44 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:15:01 DP5 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP6 TP7] Process 58173 gpu_id 55 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:01 DP1 TP0] Process 55913 gpu_id 8 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:15:01 DP6 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP1 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:01 DP7 TP0] Process 56942 gpu_id 56 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:15:01 DP7 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP6 TP5] Process 56960 gpu_id 53 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:15:01 DP3 TP4] Process 58172 gpu_id 28 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:15:01 DP6 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP3 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:01 DP5 TP0] Process 56944 gpu_id 40 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-10-13 13:15:01 DP0 TP5] Process 56965 gpu_id 5 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:15:02 DP5 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP0 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP4 TP3] Process 56946 gpu_id 35 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:15:02 DP4 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP6 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP7 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP6 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP3 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP6 TP7] Context: self.device='cuda' self.gpu_id=55 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:02 DP6 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP7 TP1] Context: self.device='cuda' self.gpu_id=57 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:15:02 DP7 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP0 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP1 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP1 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:02 DP1 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:02 DP1 TP0] Init torch distributed begin.
[2025-10-13 13:15:02 DP7 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP7 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:02 DP7 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:02 DP7 TP0] Init torch distributed begin.
[2025-10-13 13:15:02 DP6 TP5] Context: self.device='cuda' self.gpu_id=53 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:15:02 DP6 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP5 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP5 TP0] Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-13 13:15:02 DP5 TP0] Chunked prefix cache is turned on.
[2025-10-13 13:15:02 DP5 TP0] Init torch distributed begin.
[2025-10-13 13:15:02 DP3 TP4] Context: self.device='cuda' self.gpu_id=28 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:15:02 DP3 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP5 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP4 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP1 TP0] Context: self.device='cuda' self.gpu_id=8 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:15:02 DP1 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP7 TP0] Context: self.device='cuda' self.gpu_id=56 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:15:02 DP7 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP5 TP0] Context: self.device='cuda' self.gpu_id=40 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=0 self.tp_size=8
[2025-10-13 13:15:02 DP5 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP4 TP3] Context: self.device='cuda' self.gpu_id=35 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:15:02 DP4 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP5 TP4] Context: self.device='cuda' self.gpu_id=44 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:15:02 DP5 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:02 DP5 TP6] Process 59645 gpu_id 46 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:15:02 DP5 TP3] Process 58544 gpu_id 43 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:15:02 DP5 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP2 TP7] Process 56959 gpu_id 23 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:02 DP5 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP2 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:02 DP5 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP5 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP2 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:02 DP5 TP3] Context: self.device='cuda' self.gpu_id=43 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:15:02 DP5 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP3 TP2] Process 56968 gpu_id 26 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:15:02 DP5 TP6] Context: self.device='cuda' self.gpu_id=46 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:15:02 DP5 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:02 DP3 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP2 TP7] Context: self.device='cuda' self.gpu_id=23 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:03 DP2 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP5 TP7] Process 59709 gpu_id 47 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-10-13 13:15:03 DP5 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP3 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP3 TP2] Context: self.device='cuda' self.gpu_id=26 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:15:03 DP3 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP7 TP2] Process 56967 gpu_id 58 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:15:03 DP7 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP5 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP5 TP7] Context: self.device='cuda' self.gpu_id=47 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=7 self.tp_size=8
[2025-10-13 13:15:03 DP5 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP7 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP0 TP3] Process 55931 gpu_id 3 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:15:03 DP0 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP3 TP5] Process 58759 gpu_id 29 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:15:03 DP3 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP7 TP2] Context: self.device='cuda' self.gpu_id=58 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:15:03 DP7 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP7 TP3] Process 57323 gpu_id 59 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-10-13 13:15:03 DP7 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP6] Process 58047 gpu_id 54 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:15:03 DP6 TP2] Process 56943 gpu_id 50 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:15:03 DP6 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP0 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 13:15:03 DP6 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP3 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP7 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP5 TP2] Process 56954 gpu_id 42 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-10-13 13:15:03 DP2 TP4] Process 56956 gpu_id 20 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:15:03 DP5 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP2 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP4] Process 56952 gpu_id 52 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-10-13 13:15:03 DP6 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP0 TP6] Process 56971 gpu_id 6 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-10-13 13:15:03 DP6 TP2] Context: self.device='cuda' self.gpu_id=50 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:15:03 DP6 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP3 TP5] Context: self.device='cuda' self.gpu_id=29 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:15:03 DP3 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP0 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP7 TP3] Context: self.device='cuda' self.gpu_id=59 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=3 self.tp_size=8
[2025-10-13 13:15:03 DP7 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP2 TP1] Process 55923 gpu_id 17 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-10-13 13:15:03 DP1 TP5] Process 56961 gpu_id 13 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-10-13 13:15:03 DP2 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP1 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP6] Context: self.device='cuda' self.gpu_id=54 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=6 self.tp_size=8
[2025-10-13 13:15:03 DP6 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP5 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP2 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP6 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP2 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP0 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:03 DP5 TP2] Context: self.device='cuda' self.gpu_id=42 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=2 self.tp_size=8
[2025-10-13 13:15:03 DP5 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:03 DP1 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-10-13 13:15:04 DP2 TP4] Context: self.device='cuda' self.gpu_id=20 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:15:04 DP2 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:04 DP6 TP4] Context: self.device='cuda' self.gpu_id=52 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=4 self.tp_size=8
[2025-10-13 13:15:04 DP6 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:04 DP2 TP1] Context: self.device='cuda' self.gpu_id=17 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=1 self.tp_size=8
[2025-10-13 13:15:04 DP2 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[2025-10-13 13:15:04 DP1 TP5] Context: self.device='cuda' self.gpu_id=13 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=5 self.tp_size=8
[2025-10-13 13:15:04 DP1 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 3048, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 418, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 293, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 643, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: HIP error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.


[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-10-13 13:15:04 DP0 TP0] sglang is using nccl==2.26.6
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
