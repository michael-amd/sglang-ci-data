/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:02:57 __init__.py:179] Automatically detected platform rocm.
WARNING 10-07 10:02:57 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:02:58] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/lmzheng-grok-1', tokenizer_path='/mnt/raid/models/huggingface/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=213297846, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/lmzheng-grok-1', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-07 10:02:59] No chat template found, defaulting to 'string' content format
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-07 10:03:08 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:03:10 TP0] Process 82148 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:03:10 TP7] Process 82155 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:03:10 TP1] Process 82149 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:03:10 TP2] Process 82150 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-10-07 10:03:10 TP4] Process 82152 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-07 10:03:10 TP3] Process 82151 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-10-07 10:03:10 TP5] Process 82153 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-07 10:03:10 TP6] Process 82154 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-07 10:03:10 TP0] Init torch distributed begin.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-07 10:03:11 TP0] sglang is using nccl==2.21.5
[2025-10-07 10:03:12 TP0] Init torch distributed ends. mem usage=2.09 GB
[2025-10-07 10:03:13 TP1] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP1] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP7] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP7] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP4] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP4] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP3] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP3] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP0] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP0] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP6] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP6] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP2] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP2] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP5] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP5] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-07 10:03:13 TP0] Load weight begin. avail mem=189.38 GB
[2025-10-07 10:03:13 TP0] #parameters (analytical): 316.49 B, #parameters (actual): 316.58 B
Loading pt checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
Loading pt checkpoint shards:   4% Completed | 1/25 [00:03<01:15,  3.14s/it]
Loading pt checkpoint shards:   8% Completed | 2/25 [00:06<01:10,  3.05s/it]
Loading pt checkpoint shards:  12% Completed | 3/25 [00:08<01:03,  2.90s/it]
Loading pt checkpoint shards:  16% Completed | 4/25 [00:11<01:00,  2.88s/it]
Loading pt checkpoint shards:  20% Completed | 5/25 [00:13<00:53,  2.67s/it]
Loading pt checkpoint shards:  24% Completed | 6/25 [00:16<00:49,  2.60s/it]
Loading pt checkpoint shards:  28% Completed | 7/25 [00:18<00:44,  2.49s/it]
Loading pt checkpoint shards:  32% Completed | 8/25 [00:21<00:45,  2.66s/it]
Loading pt checkpoint shards:  36% Completed | 9/25 [00:25<00:46,  2.88s/it]
Loading pt checkpoint shards:  40% Completed | 10/25 [00:28<00:43,  2.92s/it]
Loading pt checkpoint shards:  44% Completed | 11/25 [00:31<00:42,  3.04s/it]
Loading pt checkpoint shards:  52% Completed | 13/25 [00:34<00:28,  2.37s/it]
Loading pt checkpoint shards:  56% Completed | 14/25 [00:37<00:28,  2.58s/it]
Loading pt checkpoint shards:  60% Completed | 15/25 [00:41<00:27,  2.79s/it]
Loading pt checkpoint shards:  64% Completed | 16/25 [00:44<00:27,  3.03s/it]
Loading pt checkpoint shards:  68% Completed | 17/25 [00:48<00:24,  3.06s/it]
Loading pt checkpoint shards:  72% Completed | 18/25 [00:51<00:21,  3.04s/it]
Loading pt checkpoint shards:  76% Completed | 19/25 [00:54<00:19,  3.20s/it]
Loading pt checkpoint shards:  80% Completed | 20/25 [01:01<00:21,  4.24s/it]
Loading pt checkpoint shards:  84% Completed | 21/25 [01:04<00:15,  3.79s/it]
Loading pt checkpoint shards:  88% Completed | 22/25 [01:06<00:10,  3.51s/it]
Loading pt checkpoint shards:  92% Completed | 23/25 [01:09<00:06,  3.17s/it]
[2025-10-07 10:04:24 TP4] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:24 TP4] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:24 TP5] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:24 TP5] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:25 TP7] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:25 TP7] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:25 TP6] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:25 TP6] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards:  96% Completed | 24/25 [01:13<00:03,  3.39s/it]
[2025-10-07 10:04:26 TP3] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:26 TP3] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:27 TP1] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:27 TP1] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:29 TP2] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:29 TP2] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards: 100% Completed | 25/25 [01:15<00:00,  3.08s/it]
Loading pt checkpoint shards: 100% Completed | 25/25 [01:15<00:00,  3.02s/it]

[2025-10-07 10:04:29 TP0] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:29 TP0] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-07 10:04:29 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=151.94 GB, mem usage=37.45 GB.
[2025-10-07 10:04:29 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-07 10:04:29 TP0] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP7] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP2] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP5] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP0] Memory pool end. avail mem=52.16 GB
[2025-10-07 10:04:29 TP6] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP4] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP3] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:29 TP1] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-07 10:04:32 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=50.43 GB
[2025-10-07 10:04:32 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.10 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[2025-10-07 10:04:36 TP5] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP4] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP7] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP3] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP0] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP2] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP1] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-07 10:04:36 TP6] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
hipcc -fPIC -DUSE_ROCM -DENABLE_FP8 -O3 -std=c++17 -DLEGACY_HIPBLAS_DIRECT -DUSE_PROF_API=1 -D__HIP_PLATFORM_HCC__=1 -D__HIP_PLATFORM_AMD__=1 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -mllvm --amdgpu-kernarg-preload-count=16 -Wno-unused-result -Wno-switch-bool -Wno-vla-cxx-extension -Wno-undefined-func-template -fgpu-flush-denormals-to-zero -mllvm --lsr-drop-solution=1 -fno-offload-uniform-block -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -amdgpu-coerce-illegal-types=1 --offload-arch=gfx942 -I/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include -c pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp -o pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for gfx942.
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for host.
hipcc -shared pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o -o lib.so
[aiter] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.35860190s
[2025-10-07 10:04:39 TP5] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.35860190s
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP5] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP5] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP4] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP4] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP7] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP7] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP3] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP1] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP0] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP2] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:04:39 TP6] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-07 10:04:39 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP2] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP0] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP3] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP1] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-07 10:04:39 TP6] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-07 10:04:39 TP5] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
clang (LLVM option parsing): Unknown command line argument '--amdgpu-use-amdgpu-trackers=1'.  Try: 'clang (LLVM option parsing) --help'
clang (LLVM option parsing): Did you mean '--amdgpu-use-aa-in-codegen=1'?
failed to execute:/opt/rocm/lib/llvm/bin/clang++  --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 -O3  -mllvm --amdgpu-use-amdgpu-trackers=1 -x hip -c /dev/null -o "/dev/null"
[aiter] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[2025-10-07 10:04:51 TP5] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 186.22332059s
[2025-10-07 10:07:45 TP5] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 186.22332059s
Capturing batches (bs=512 avail_mem=50.10 GB):   2%|         | 1/52 [03:16<2:46:37, 196.03s/it]Capturing batches (bs=496 avail_mem=49.27 GB):   2%|         | 1/52 [03:16<2:46:37, 196.03s/it][aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP5] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP0] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP2] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP7] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP4] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP1] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP6] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP3] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=496 avail_mem=49.27 GB):   4%|         | 2/52 [03:16<1:07:30, 81.02s/it] Capturing batches (bs=480 avail_mem=49.27 GB):   4%|         | 2/52 [03:16<1:07:30, 81.02s/it][aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP5] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP6] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP7] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP4] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP3] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP0] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP1] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP2] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=480 avail_mem=49.27 GB):   6%|         | 3/52 [03:16<36:01, 44.12s/it]  Capturing batches (bs=464 avail_mem=49.26 GB):   6%|         | 3/52 [03:16<36:01, 44.12s/it][aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP4] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP5] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP7] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP6] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP2] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP3] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP1] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:49 TP0] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=464 avail_mem=49.26 GB):   8%|         | 4/52 [03:16<21:25, 26.78s/it]Capturing batches (bs=448 avail_mem=49.26 GB):   8%|         | 4/52 [03:16<21:25, 26.78s/it][aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP4] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP7] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP5] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP2] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP0] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP3] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP1] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP6] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=448 avail_mem=49.26 GB):  10%|         | 5/52 [03:17<13:28, 17.19s/it]Capturing batches (bs=432 avail_mem=49.26 GB):  10%|         | 5/52 [03:17<13:28, 17.19s/it][aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP4] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP7] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP5] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP6] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP3] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP2] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP1] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP0] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=432 avail_mem=49.26 GB):  12%|        | 6/52 [03:17<08:45, 11.41s/it]Capturing batches (bs=416 avail_mem=49.26 GB):  12%|        | 6/52 [03:17<08:45, 11.41s/it][aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP4] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP2] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP3] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP5] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP1] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP6] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP7] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP0] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=416 avail_mem=49.26 GB):  13%|        | 7/52 [03:17<05:48,  7.75s/it]Capturing batches (bs=400 avail_mem=49.26 GB):  13%|        | 7/52 [03:17<05:48,  7.75s/it][aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP2] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP4] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP5] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP3] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP0] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP7] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP1] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP6] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=400 avail_mem=49.26 GB):  15%|        | 8/52 [03:17<03:55,  5.35s/it]Capturing batches (bs=384 avail_mem=49.25 GB):  15%|        | 8/52 [03:17<03:55,  5.35s/it][aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP2] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP5] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP3] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP4] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP1] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP0] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP7] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:50 TP6] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=384 avail_mem=49.25 GB):  17%|        | 9/52 [03:17<02:40,  3.74s/it]Capturing batches (bs=368 avail_mem=49.25 GB):  17%|        | 9/52 [03:17<02:40,  3.74s/it][aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP3] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP7] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP1] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP6] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP0] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP5] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP2] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP4] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=368 avail_mem=49.25 GB):  19%|        | 10/52 [03:18<01:51,  2.65s/it]Capturing batches (bs=352 avail_mem=49.25 GB):  19%|        | 10/52 [03:18<01:51,  2.65s/it][aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP3] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP7] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP6] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP2] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP1] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP5] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP4] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP0] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=352 avail_mem=49.25 GB):  21%|        | 11/52 [03:18<01:17,  1.90s/it]Capturing batches (bs=336 avail_mem=49.25 GB):  21%|        | 11/52 [03:18<01:17,  1.90s/it][aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP3] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP2] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP7] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP6] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP4] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP5] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP1] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP0] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=336 avail_mem=49.25 GB):  23%|       | 12/52 [03:18<00:55,  1.38s/it]Capturing batches (bs=320 avail_mem=49.25 GB):  23%|       | 12/52 [03:18<00:55,  1.38s/it][aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP0] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP7] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP2] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP3] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP1] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP5] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP4] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP6] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
Capturing batches (bs=320 avail_mem=49.25 GB):  25%|       | 13/52 [03:18<00:40,  1.03s/it]Capturing batches (bs=304 avail_mem=49.24 GB):  25%|       | 13/52 [03:18<00:40,  1.03s/it][aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP3] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP7] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP2] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP5] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP0] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP1] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP6] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:51 TP4] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=304 avail_mem=49.24 GB):  27%|       | 14/52 [03:18<00:29,  1.29it/s]Capturing batches (bs=288 avail_mem=49.24 GB):  27%|       | 14/52 [03:18<00:29,  1.29it/s][aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP2] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP3] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP7] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP5] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP1] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP6] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP4] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP0] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=288 avail_mem=49.24 GB):  29%|       | 15/52 [03:19<00:22,  1.66it/s]Capturing batches (bs=272 avail_mem=49.24 GB):  29%|       | 15/52 [03:19<00:22,  1.66it/s][aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP3] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP2] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP5] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP1] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP7] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP6] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP4] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP0] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=272 avail_mem=49.24 GB):  31%|       | 16/52 [03:19<00:17,  2.09it/s]Capturing batches (bs=256 avail_mem=49.23 GB):  31%|       | 16/52 [03:19<00:17,  2.09it/s][aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP3] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP7] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP5] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP1] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP2] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP0] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP4] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP6] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=49.23 GB):  33%|      | 17/52 [03:19<00:13,  2.54it/s]Capturing batches (bs=248 avail_mem=49.23 GB):  33%|      | 17/52 [03:19<00:13,  2.54it/s][aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP3] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP5] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP2] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP7] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP1] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP6] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP4] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP0] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=248 avail_mem=49.23 GB):  35%|      | 18/52 [03:19<00:11,  2.98it/s]Capturing batches (bs=240 avail_mem=49.23 GB):  35%|      | 18/52 [03:19<00:11,  2.98it/s][aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP2] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP7] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP5] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP3] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP1] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP6] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP0] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:52 TP4] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=240 avail_mem=49.23 GB):  37%|      | 19/52 [03:19<00:09,  3.41it/s]Capturing batches (bs=232 avail_mem=49.23 GB):  37%|      | 19/52 [03:19<00:09,  3.41it/s][aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP3] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP2] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP7] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP4] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP1] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP5] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP6] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP0] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=232 avail_mem=49.23 GB):  38%|      | 20/52 [03:20<00:08,  3.77it/s]Capturing batches (bs=224 avail_mem=49.23 GB):  38%|      | 20/52 [03:20<00:08,  3.77it/s][aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP7] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP5] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP2] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP6] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP3] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP1] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP0] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP4] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=224 avail_mem=49.23 GB):  40%|      | 21/52 [03:20<00:07,  4.03it/s]Capturing batches (bs=216 avail_mem=49.22 GB):  40%|      | 21/52 [03:20<00:07,  4.03it/s][aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP3] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP4] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP5] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP2] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP6] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP0] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP1] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP7] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=216 avail_mem=49.22 GB):  42%|     | 22/52 [03:20<00:07,  4.28it/s]Capturing batches (bs=208 avail_mem=49.22 GB):  42%|     | 22/52 [03:20<00:07,  4.28it/s][aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP3] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP5] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP2] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP4] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP7] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP6] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP1] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP0] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=208 avail_mem=49.22 GB):  44%|     | 23/52 [03:20<00:06,  4.50it/s]Capturing batches (bs=200 avail_mem=49.22 GB):  44%|     | 23/52 [03:20<00:06,  4.50it/s][aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP2] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP7] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP6] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP3] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP5] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP1] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP4] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:53 TP0] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=200 avail_mem=49.22 GB):  46%|     | 24/52 [03:20<00:06,  4.65it/s]Capturing batches (bs=192 avail_mem=49.22 GB):  46%|     | 24/52 [03:20<00:06,  4.65it/s][aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP2] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP7] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP0] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP1] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP4] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP3] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP6] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP5] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=192 avail_mem=49.22 GB):  48%|     | 25/52 [03:21<00:05,  4.68it/s]Capturing batches (bs=184 avail_mem=49.22 GB):  48%|     | 25/52 [03:21<00:05,  4.68it/s][aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP3] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP4] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP7] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP5] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP2] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP0] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP1] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP6] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=184 avail_mem=49.22 GB):  50%|     | 26/52 [03:21<00:05,  4.74it/s]Capturing batches (bs=176 avail_mem=49.21 GB):  50%|     | 26/52 [03:21<00:05,  4.74it/s][aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP3] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP7] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP5] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP1] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP4] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP2] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP0] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP6] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=176 avail_mem=49.21 GB):  52%|    | 27/52 [03:21<00:05,  4.81it/s]Capturing batches (bs=168 avail_mem=49.21 GB):  52%|    | 27/52 [03:21<00:05,  4.81it/s][aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP3] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP2] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP5] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP7] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP1] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP4] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP0] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP6] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=168 avail_mem=49.21 GB):  54%|    | 28/52 [03:21<00:04,  4.88it/s]Capturing batches (bs=160 avail_mem=49.21 GB):  54%|    | 28/52 [03:21<00:04,  4.88it/s][aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP3] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP5] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP2] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP7] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP6] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP0] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP4] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:54 TP1] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
Capturing batches (bs=160 avail_mem=49.21 GB):  56%|    | 29/52 [03:21<00:04,  4.92it/s]Capturing batches (bs=152 avail_mem=49.21 GB):  56%|    | 29/52 [03:21<00:04,  4.92it/s][aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP2] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP7] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP0] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP1] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP3] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP4] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP5] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP6] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=152 avail_mem=49.21 GB):  58%|    | 30/52 [03:22<00:04,  4.97it/s]Capturing batches (bs=144 avail_mem=49.20 GB):  58%|    | 30/52 [03:22<00:04,  4.97it/s][aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP1] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP3] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP7] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP2] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP4] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP6] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP0] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP5] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=144 avail_mem=49.20 GB):  60%|    | 31/52 [03:22<00:04,  5.01it/s]Capturing batches (bs=136 avail_mem=49.20 GB):  60%|    | 31/52 [03:22<00:04,  5.01it/s][aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP3] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP0] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP5] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP7] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP6] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP1] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP2] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP4] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=136 avail_mem=49.20 GB):  62%|   | 32/52 [03:22<00:03,  5.03it/s]Capturing batches (bs=128 avail_mem=49.20 GB):  62%|   | 32/52 [03:22<00:03,  5.03it/s][aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP5] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP6] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP2] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP0] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP7] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP1] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP3] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP4] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=49.20 GB):  63%|   | 33/52 [03:22<00:03,  4.97it/s]Capturing batches (bs=120 avail_mem=49.20 GB):  63%|   | 33/52 [03:22<00:03,  4.97it/s][aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP3] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP5] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP0] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP2] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP6] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP1] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP4] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:55 TP7] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=120 avail_mem=49.20 GB):  65%|   | 34/52 [03:22<00:03,  5.00it/s]Capturing batches (bs=112 avail_mem=49.20 GB):  65%|   | 34/52 [03:22<00:03,  5.00it/s][aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP2] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP3] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP1] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP4] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP7] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP5] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP0] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP6] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=112 avail_mem=49.20 GB):  67%|   | 35/52 [03:23<00:03,  5.00it/s]Capturing batches (bs=104 avail_mem=49.19 GB):  67%|   | 35/52 [03:23<00:03,  5.00it/s][aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP5] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP6] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP4] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP2] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP7] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP3] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP1] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP0] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=104 avail_mem=49.19 GB):  69%|   | 36/52 [03:23<00:03,  5.01it/s]Capturing batches (bs=96 avail_mem=49.19 GB):  69%|   | 36/52 [03:23<00:03,  5.01it/s] [aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP7] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP2] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP5] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP6] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP1] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP3] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP4] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP0] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=96 avail_mem=49.19 GB):  71%|   | 37/52 [03:23<00:02,  5.03it/s]Capturing batches (bs=88 avail_mem=49.19 GB):  71%|   | 37/52 [03:23<00:02,  5.03it/s][aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP3] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP4] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP5] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP0] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP1] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP2] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP7] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP6] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=88 avail_mem=49.19 GB):  73%|  | 38/52 [03:23<00:02,  5.05it/s]Capturing batches (bs=80 avail_mem=49.19 GB):  73%|  | 38/52 [03:23<00:02,  5.05it/s][aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP2] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP3] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP7] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP1] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP0] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP5] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP4] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:56 TP6] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=80 avail_mem=49.19 GB):  75%|  | 39/52 [03:23<00:02,  4.98it/s]Capturing batches (bs=72 avail_mem=49.18 GB):  75%|  | 39/52 [03:23<00:02,  4.98it/s][aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP3] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP2] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP7] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP5] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP6] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP0] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP4] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP1] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=72 avail_mem=49.18 GB):  77%|  | 40/52 [03:24<00:02,  4.99it/s]Capturing batches (bs=64 avail_mem=49.18 GB):  77%|  | 40/52 [03:24<00:02,  4.99it/s][aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP7] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP3] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP2] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP1] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP6] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP5] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP0] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP4] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=49.18 GB):  79%|  | 41/52 [03:24<00:02,  4.93it/s]Capturing batches (bs=56 avail_mem=49.18 GB):  79%|  | 41/52 [03:24<00:02,  4.93it/s][aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP3] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP2] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP7] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP5] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP1] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP4] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP6] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP0] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=56 avail_mem=49.18 GB):  81%|  | 42/52 [03:24<00:02,  4.98it/s]Capturing batches (bs=48 avail_mem=49.18 GB):  81%|  | 42/52 [03:24<00:02,  4.98it/s][aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP3] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP2] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP1] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP7] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP5] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP6] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP0] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP4] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=48 avail_mem=49.18 GB):  83%| | 43/52 [03:24<00:01,  4.92it/s]Capturing batches (bs=40 avail_mem=49.17 GB):  83%| | 43/52 [03:24<00:01,  4.92it/s][aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP3] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP1] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP2] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP7] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP5] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP0] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP4] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:57 TP6] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=40 avail_mem=49.17 GB):  85%| | 44/52 [03:24<00:01,  4.97it/s]Capturing batches (bs=32 avail_mem=49.17 GB):  85%| | 44/52 [03:24<00:01,  4.97it/s][aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP3] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP7] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP5] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP4] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP6] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP1] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP2] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP0] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=49.17 GB):  87%| | 45/52 [03:25<00:01,  5.00it/s]Capturing batches (bs=24 avail_mem=49.17 GB):  87%| | 45/52 [03:25<00:01,  5.00it/s][aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP3] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP2] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP7] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP5] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP1] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP4] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP0] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP6] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=24 avail_mem=49.17 GB):  88%| | 46/52 [03:25<00:01,  5.02it/s]Capturing batches (bs=16 avail_mem=49.17 GB):  88%| | 46/52 [03:25<00:01,  5.02it/s][aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP3] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP7] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP2] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP4] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP1] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP6] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP5] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP0] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
Capturing batches (bs=16 avail_mem=49.17 GB):  90%| | 47/52 [03:25<00:00,  5.00it/s]Capturing batches (bs=12 avail_mem=49.16 GB):  90%| | 47/52 [03:25<00:00,  5.00it/s][aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP3] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP1] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP2] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP0] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP7] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP6] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP5] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP4] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=12 avail_mem=49.16 GB):  92%|| 48/52 [03:25<00:00,  4.91it/s]Capturing batches (bs=8 avail_mem=49.16 GB):  92%|| 48/52 [03:25<00:00,  4.91it/s] [aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP2] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP1] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP7] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP4] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP0] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP5] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP3] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:58 TP6] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
Capturing batches (bs=8 avail_mem=49.16 GB):  94%|| 49/52 [03:25<00:00,  4.90it/s]Capturing batches (bs=4 avail_mem=49.16 GB):  94%|| 49/52 [03:25<00:00,  4.90it/s][aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP3] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP2] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP0] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP5] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP6] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP7] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP1] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP4] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=4 avail_mem=49.16 GB):  96%|| 50/52 [03:26<00:00,  4.86it/s]Capturing batches (bs=2 avail_mem=49.16 GB):  96%|| 50/52 [03:26<00:00,  4.86it/s][aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP5] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP7] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP3] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP2] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP1] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP4] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP0] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP6] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=2 avail_mem=49.16 GB):  98%|| 51/52 [03:26<00:00,  4.91it/s]Capturing batches (bs=1 avail_mem=49.15 GB):  98%|| 51/52 [03:26<00:00,  4.91it/s][aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP7] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP1] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP4] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP3] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP0] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP5] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP2] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:07:59 TP6] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=1 avail_mem=49.15 GB): 100%|| 52/52 [03:26<00:00,  3.34it/s]Capturing batches (bs=1 avail_mem=49.15 GB): 100%|| 52/52 [03:26<00:00,  3.98s/it]
[2025-10-07 10:08:00 TP7] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP3] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP4] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP1] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP2] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP5] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP0] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP6] Registering 6708 cuda graph addresses
[2025-10-07 10:08:00 TP0] Capture cuda graph end. Time elapsed: 208.35 s. mem usage=1.29 GB. avail mem=49.15 GB.
[2025-10-07 10:08:01 TP0] max_total_num_tokens=3246295, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=49.15 GB
[2025-10-07 10:08:01] INFO:     Started server process [81918]
[2025-10-07 10:08:01] INFO:     Waiting for application startup.
[2025-10-07 10:08:01] INFO:     Application startup complete.
[2025-10-07 10:08:01] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-07 10:08:02] INFO:     127.0.0.1:39704 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-07 10:08:02 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP5] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP4] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP3] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP2] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP7] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP1] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-07 10:08:03 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP6] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:03 TP0] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:04] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:04] The server is fired up and ready to roll!
[2025-10-07 10:08:10] INFO:     127.0.0.1:50748 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-07 10:08:10 TP0] Prefill batch. #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP0] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP3] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP5] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP6] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP2] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP1] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP4] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP7] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:11 TP0] Prefill batch. #new-seq: 3, #new-token: 161, #cached-token: 2388, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP4] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP7] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP5] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP3] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP1] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP2] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP0] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP6] [fused_moe] using default for (161, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP0] Prefill batch. #new-seq: 39, #new-token: 2484, #cached-token: 31044, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP6] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP0] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP5] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP1] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP2] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP3] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP7] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:11 TP4] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:12 TP0] Prefill batch. #new-seq: 259, #new-token: 16247, #cached-token: 206758, token usage: 0.00, #running-req: 42, #queue-req: 0, 
[2025-10-07 10:08:12 TP0] Prefill batch. #new-seq: 104, #new-token: 6283, #cached-token: 83034, token usage: 0.01, #running-req: 301, #queue-req: 0, 
[2025-10-07 10:08:13 TP0] Prefill batch. #new-seq: 264, #new-token: 16362, #cached-token: 210824, token usage: 0.01, #running-req: 405, #queue-req: 241, 
[2025-10-07 10:08:13 TP0] Prefill batch. #new-seq: 263, #new-token: 16325, #cached-token: 210062, token usage: 0.01, #running-req: 669, #queue-req: 141, 
[2025-10-07 10:08:14 TP0] Prefill batch. #new-seq: 255, #new-token: 16352, #cached-token: 203693, token usage: 0.02, #running-req: 932, #queue-req: 132, 
[2025-10-07 10:08:14 TP0] Prefill batch. #new-seq: 132, #new-token: 8591, #cached-token: 105458, token usage: 0.02, #running-req: 1187, #queue-req: 0, 
[2025-10-07 10:08:18 TP0] Decode batch. #running-req: 1319, #token: 124191, token usage: 0.04, cuda graph: False, gen throughput (token/s): 2280.79, #queue-req: 0, 
[2025-10-07 10:08:19] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:20] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:21] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22 TP0] Decode batch. #running-req: 1250, #token: 168152, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12782.92, #queue-req: 0, 
[2025-10-07 10:08:22] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:22] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:23] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:24] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (1016, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (960, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (951, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (933, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (922, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:25] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP3] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP0] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP7] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP2] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP6] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP4] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP1] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:25 TP5] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (891, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] Decode batch. #running-req: 891, #token: 157624, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12080.40, #queue-req: 0, 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (859, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (849, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (831, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:57924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (807, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:26] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP3] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP7] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP1] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP5] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP0] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP4] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP2] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:26 TP6] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (689, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (676, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP3] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP4] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP1] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP7] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP5] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:27] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP0] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP2] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:27 TP6] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (651, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (620, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (585, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (576, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:28] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP4] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP3] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP7] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP1] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP5] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP0] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP2] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:28 TP6] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP4] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP7] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP3] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP1] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP5] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP0] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP2] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP6] [fused_moe] using default for (545, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29 TP4] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP1] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP5] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29 TP3] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP7] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP0] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP2] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP6] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP3] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP4] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP1] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP7] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP5] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP0] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP2] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP6] [fused_moe] using default for (524, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP4] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP7] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP5] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP3] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP1] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP6] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP0] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP2] [fused_moe] using default for (523, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP4] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP3] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP1] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP7] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP5] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP0] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP2] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29 TP6] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:29] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29 TP0] Decode batch. #running-req: 523, #token: 115397, token usage: 0.04, cuda graph: False, gen throughput (token/s): 8931.12, #queue-req: 0, 
[2025-10-07 10:08:29] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:29] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:30] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31 TP0] Decode batch. #running-req: 291, #token: 77655, token usage: 0.02, cuda graph: True, gen throughput (token/s): 9228.42, #queue-req: 0, 
[2025-10-07 10:08:31] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:31] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32 TP0] Decode batch. #running-req: 227, #token: 71531, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7760.49, #queue-req: 0, 
[2025-10-07 10:08:32] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:32] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33 TP0] Decode batch. #running-req: 209, #token: 73931, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7303.20, #queue-req: 0, 
[2025-10-07 10:08:33] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:33] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:34 TP0] Decode batch. #running-req: 196, #token: 77191, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6751.70, #queue-req: 0, 
[2025-10-07 10:08:34] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:35] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:35 TP0] Decode batch. #running-req: 194, #token: 84544, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6751.40, #queue-req: 0, 
[2025-10-07 10:08:37] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:37] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:37 TP0] Decode batch. #running-req: 192, #token: 91320, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6155.99, #queue-req: 0, 
[2025-10-07 10:08:38 TP0] Decode batch. #running-req: 192, #token: 99000, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6201.61, #queue-req: 0, 
[2025-10-07 10:08:39 TP0] Decode batch. #running-req: 192, #token: 106680, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6483.34, #queue-req: 0, 
[2025-10-07 10:08:40 TP0] Decode batch. #running-req: 192, #token: 114360, token usage: 0.04, cuda graph: True, gen throughput (token/s): 6517.62, #queue-req: 0, 
[2025-10-07 10:08:40] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:40] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:49] INFO:     127.0.0.1:39166 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-07 10:08:50] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2546, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 39, #new-token: 39, #cached-token: 33400, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (39, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 40343, token usage: 0.00, #running-req: 42, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49224, token usage: 0.00, #running-req: 89, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 50029, token usage: 0.00, #running-req: 146, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51393, token usage: 0.01, #running-req: 204, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52382, token usage: 0.01, #running-req: 264, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP4] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP0] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP6] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP7] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP5] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP1] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP3] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:50 TP2] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53371, token usage: 0.01, #running-req: 325, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51837, token usage: 0.01, #running-req: 387, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 52963, token usage: 0.01, #running-req: 447, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55695, token usage: 0.01, #running-req: 509, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53390, token usage: 0.01, #running-req: 574, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52426, token usage: 0.01, #running-req: 636, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53328, token usage: 0.01, #running-req: 697, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54316, token usage: 0.02, #running-req: 759, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47199, token usage: 0.02, #running-req: 822, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51615, token usage: 0.02, #running-req: 877, #queue-req: 0, 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 58916, token usage: 0.02, #running-req: 937, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP4] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] Prefill batch. #new-seq: 27, #new-token: 27, #cached-token: 23357, token usage: 0.02, #running-req: 1005, #queue-req: 0, 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP2] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP6] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP1] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP3] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP7] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP5] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:51 TP0] [fused_moe] using default for (27, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP0] Prefill batch. #new-seq: 32, #new-token: 32, #cached-token: 27436, token usage: 0.02, #running-req: 1032, #queue-req: 0, 
[2025-10-07 10:08:52 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 64485, token usage: 0.02, #running-req: 1064, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP0] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP1] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP3] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP2] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP7] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP5] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP6] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP4] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:08:52 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 49827, token usage: 0.02, #running-req: 1139, #queue-req: 0, 
[2025-10-07 10:08:52 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53571, token usage: 0.03, #running-req: 1197, #queue-req: 0, 
[2025-10-07 10:08:52 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51698, token usage: 0.03, #running-req: 1259, #queue-req: 0, 
[2025-10-07 10:08:54] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:55] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:56 TP0] Decode batch. #running-req: 1317, #token: 132278, token usage: 0.04, cuda graph: False, gen throughput (token/s): 3122.42, #queue-req: 0, 
[2025-10-07 10:08:56] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:43914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:57] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:58] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:08:59] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00 TP0] Decode batch. #running-req: 1221, #token: 171280, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12613.67, #queue-req: 0, 
[2025-10-07 10:09:00] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:00] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:44580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:42652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:01] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (974, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (967, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (944, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:02] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP4] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP3] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP7] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP0] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP2] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP1] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP6] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:02 TP5] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (908, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:39826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] Decode batch. #running-req: 831, #token: 152614, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11535.42, #queue-req: 0, 
[2025-10-07 10:09:03] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (814, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP3] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:03 TP7] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP1] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP4] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP5] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP0] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP6] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:03 TP2] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (751, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (733, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:47394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:39934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:04] INFO:     127.0.0.1:50352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP3] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP7] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP4] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP0] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP1] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP5] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP6] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:04 TP2] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (642, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (611, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:39946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:05] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP3] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP4] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP1] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP7] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP5] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP0] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP6] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:05 TP2] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06 TP3] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP7] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP4] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP1] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06 TP5] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP0] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP6] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP2] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP3] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP7] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP4] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06 TP1] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP5] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP0] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP6] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP2] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP1] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP3] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP7] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP4] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP5] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP0] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP2] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP6] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP7] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP3] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP4] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP1] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP6] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP0] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP2] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06 TP5] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:06] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06 TP0] Decode batch. #running-req: 468, #token: 105540, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8761.74, #queue-req: 0, 
[2025-10-07 10:09:06] INFO:     127.0.0.1:40468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:40926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:45788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:06] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:40748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:07] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08 TP0] Decode batch. #running-req: 277, #token: 76237, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8890.35, #queue-req: 0, 
[2025-10-07 10:09:08] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:08] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09 TP0] Decode batch. #running-req: 226, #token: 72354, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7711.51, #queue-req: 0, 
[2025-10-07 10:09:09] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:41302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:09] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10 TP0] Decode batch. #running-req: 211, #token: 75640, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6957.76, #queue-req: 0, 
[2025-10-07 10:09:10] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:10] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:11] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:11] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:11] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:11] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:11] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:12 TP0] Decode batch. #running-req: 202, #token: 81157, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6991.86, #queue-req: 0, 
[2025-10-07 10:09:12] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:12] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:12] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:13 TP0] Decode batch. #running-req: 199, #token: 87879, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6923.66, #queue-req: 0, 
[2025-10-07 10:09:13] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:13] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:14 TP0] Decode batch. #running-req: 197, #token: 94862, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6806.59, #queue-req: 0, 
[2025-10-07 10:09:15 TP0] Decode batch. #running-req: 197, #token: 102742, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6855.56, #queue-req: 0, 
[2025-10-07 10:09:15] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:16 TP0] Decode batch. #running-req: 196, #token: 110051, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6671.57, #queue-req: 0, 
[2025-10-07 10:09:17] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:17] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:26] INFO:     127.0.0.1:48912 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-07 10:09:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-07 10:09:27] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 3380, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 34, #new-token: 34, #cached-token: 29226, token usage: 0.00, #running-req: 4, #queue-req: 0, 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP4] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP2] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP7] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP3] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP6] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP1] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP5] [fused_moe] using default for (34, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 43896, token usage: 0.00, #running-req: 38, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP4] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP7] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP2] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP3] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP6] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP5] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP1] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 48881, token usage: 0.00, #running-req: 89, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57761, token usage: 0.00, #running-req: 146, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP4] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP7] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP2] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP3] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP1] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP6] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP5] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54162, token usage: 0.01, #running-req: 213, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 54908, token usage: 0.01, #running-req: 276, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 54940, token usage: 0.01, #running-req: 340, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 61372, token usage: 0.01, #running-req: 404, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP4] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP2] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP6] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP7] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP1] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP3] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP5] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 60692, token usage: 0.01, #running-req: 475, #queue-req: 0, 
[2025-10-07 10:09:27 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55948, token usage: 0.01, #running-req: 546, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54022, token usage: 0.01, #running-req: 611, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54237, token usage: 0.01, #running-req: 674, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53366, token usage: 0.02, #running-req: 737, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 42254, token usage: 0.02, #running-req: 799, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP0] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP4] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP2] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP7] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP6] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP3] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP5] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP1] [fused_moe] using default for (49, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP4] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP0] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP1] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP5] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP2] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP6] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP3] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP7] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP0] Decode batch. #running-req: 848, #token: 55093, token usage: 0.02, cuda graph: False, gen throughput (token/s): 666.74, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 44479, token usage: 0.02, #running-req: 848, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP1] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP0] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP4] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP3] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP5] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP7] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP2] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP6] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51751, token usage: 0.02, #running-req: 900, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51926, token usage: 0.02, #running-req: 960, #queue-req: 0, 
[2025-10-07 10:09:28 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 50086, token usage: 0.02, #running-req: 1020, #queue-req: 0, 
[2025-10-07 10:09:29 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55785, token usage: 0.02, #running-req: 1078, #queue-req: 0, 
[2025-10-07 10:09:29 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52480, token usage: 0.02, #running-req: 1143, #queue-req: 0, 
[2025-10-07 10:09:29 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 57087, token usage: 0.03, #running-req: 1204, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP0] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP2] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP1] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP4] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP3] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP7] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP6] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP5] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:29 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 42108, token usage: 0.03, #running-req: 1270, #queue-req: 0, 
[2025-10-07 10:09:32] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:33 TP0] Decode batch. #running-req: 1318, #token: 136643, token usage: 0.04, cuda graph: False, gen throughput (token/s): 10711.92, #queue-req: 0, 
[2025-10-07 10:09:33] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:33] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:33] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:33] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:34] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:35] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:36] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37 TP0] Decode batch. #running-req: 1203, #token: 173023, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12295.79, #queue-req: 0, 
[2025-10-07 10:09:37] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:37] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:38] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP3] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP0] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP7] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP6] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP4] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP2] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP1] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:38 TP5] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (946, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (940, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP1] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP2] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP6] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP5] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP4] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP0] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP3] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39 TP7] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:39] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:39] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (868, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (850, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (841, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (827, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (816, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] Decode batch. #running-req: 802, #token: 148335, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11499.12, #queue-req: 0, 
[2025-10-07 10:09:40] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40 TP1] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP3] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40 TP7] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:40 TP4] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP5] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP6] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP2] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:40 TP0] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (754, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (727, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (674, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:41] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP1] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP7] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP3] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP4] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP5] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP6] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP0] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:41 TP2] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (652, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (634, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (575, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (565, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (550, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:42] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP1] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP3] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP7] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP5] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP4] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP0] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP2] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:42 TP6] [fused_moe] using default for (541, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP1] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP3] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP5] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP4] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP0] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP7] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP6] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43 TP2] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-07 10:09:43] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43 TP0] Decode batch. #running-req: 451, #token: 103647, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8612.10, #queue-req: 0, 
[2025-10-07 10:09:43] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:43] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:44] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45 TP0] Decode batch. #running-req: 272, #token: 75493, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8607.69, #queue-req: 0, 
[2025-10-07 10:09:45] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:45] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46 TP0] Decode batch. #running-req: 230, #token: 72737, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7736.82, #queue-req: 0, 
[2025-10-07 10:09:46] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:46] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:47 TP0] Decode batch. #running-req: 211, #token: 76647, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6989.13, #queue-req: 0, 
[2025-10-07 10:09:48] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:48] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:48] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:49 TP0] Decode batch. #running-req: 208, #token: 83901, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7128.15, #queue-req: 0, 
[2025-10-07 10:09:49] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:49] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:50 TP0] Decode batch. #running-req: 206, #token: 91398, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6870.46, #queue-req: 0, 
[2025-10-07 10:09:50] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:51] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:51 TP0] Decode batch. #running-req: 204, #token: 98641, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7034.64, #queue-req: 0, 
[2025-10-07 10:09:51] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:52 TP0] Decode batch. #running-req: 203, #token: 106264, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6873.46, #queue-req: 0, 
[2025-10-07 10:09:52] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:53] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:53 TP0] Decode batch. #running-req: 201, #token: 113186, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6631.84, #queue-req: 0, 
[2025-10-07 10:09:54] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:54] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-07 10:09:57] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-07 10:09:57] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
