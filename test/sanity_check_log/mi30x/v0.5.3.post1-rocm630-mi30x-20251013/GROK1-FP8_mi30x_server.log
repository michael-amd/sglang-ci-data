/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 10:05:30 __init__.py:179] Automatically detected platform rocm.
WARNING 10-13 10:05:30 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 10:05:32] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/lmzheng-grok-1', tokenizer_path='/mnt/raid/models/huggingface/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=524309503, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, crash_on_nan=False, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/lmzheng-grok-1', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.2, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-13 10:05:32] No chat template found, defaulting to 'string' content format
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 10:05:42 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:42 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 10:05:42 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:42 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 10:05:43 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:43 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:43 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:43 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 10:05:43 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 10:05:44 TP7] Process 83648 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 10:05:44 TP2] Process 83643 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 10:05:45 TP4] Process 83645 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-13 10:05:45 TP1] Process 83642 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 10:05:45 TP6] Process 83647 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 10:05:45 TP0] Process 83641 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 10:05:45 TP5] Process 83646 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 10:05:45 TP3] Process 83644 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-13 10:05:45 TP0] Init torch distributed begin.
[2025-10-13 10:05:46 TP0] sglang is using nccl==2.21.5
[2025-10-13 10:05:47 TP0] Init torch distributed ends. mem usage=2.07 GB
[2025-10-13 10:05:48 TP0] Load weight begin. avail mem=189.41 GB
[2025-10-13 10:05:48 TP0] #parameters (analytical): 316.49 B, #parameters (actual): 316.58 B
Loading pt checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
Loading pt checkpoint shards:   4% Completed | 1/25 [00:03<01:26,  3.61s/it]
Loading pt checkpoint shards:   8% Completed | 2/25 [00:07<01:21,  3.53s/it]
Loading pt checkpoint shards:  12% Completed | 3/25 [00:09<01:09,  3.16s/it]
Loading pt checkpoint shards:  16% Completed | 4/25 [00:13<01:07,  3.20s/it]
Loading pt checkpoint shards:  20% Completed | 5/25 [00:16<01:04,  3.21s/it]
Loading pt checkpoint shards:  24% Completed | 6/25 [00:19<01:02,  3.29s/it]
Loading pt checkpoint shards:  28% Completed | 7/25 [00:22<00:58,  3.27s/it]
Loading pt checkpoint shards:  32% Completed | 8/25 [00:26<00:57,  3.41s/it]
Loading pt checkpoint shards:  36% Completed | 9/25 [00:30<00:55,  3.44s/it]
Loading pt checkpoint shards:  40% Completed | 10/25 [00:33<00:50,  3.33s/it]
Loading pt checkpoint shards:  44% Completed | 11/25 [00:36<00:45,  3.27s/it]
Loading pt checkpoint shards:  52% Completed | 13/25 [00:39<00:30,  2.56s/it]
Loading pt checkpoint shards:  56% Completed | 14/25 [00:43<00:30,  2.75s/it]
Loading pt checkpoint shards:  60% Completed | 15/25 [00:46<00:29,  2.93s/it]
Loading pt checkpoint shards:  64% Completed | 16/25 [00:50<00:29,  3.29s/it]
Loading pt checkpoint shards:  68% Completed | 17/25 [00:54<00:26,  3.29s/it]
Loading pt checkpoint shards:  72% Completed | 18/25 [00:56<00:21,  3.00s/it]
Loading pt checkpoint shards:  76% Completed | 19/25 [00:59<00:18,  3.11s/it]
Loading pt checkpoint shards:  80% Completed | 20/25 [01:07<00:21,  4.39s/it]
Loading pt checkpoint shards:  84% Completed | 21/25 [01:09<00:15,  3.86s/it]
Loading pt checkpoint shards:  88% Completed | 22/25 [01:12<00:10,  3.54s/it]
Loading pt checkpoint shards:  92% Completed | 23/25 [01:15<00:06,  3.19s/it]
[2025-10-13 10:07:03 TP5] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:03 TP5] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:05 TP7] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:05 TP7] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:06 TP4] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:06 TP4] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards:  96% Completed | 24/25 [01:17<00:03,  3.05s/it]
[2025-10-13 10:07:06 TP6] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:06 TP6] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:07 TP3] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:07 TP3] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP2] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP2] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards: 100% Completed | 25/25 [01:19<00:00,  2.78s/it]
Loading pt checkpoint shards: 100% Completed | 25/25 [01:19<00:00,  3.20s/it]

[2025-10-13 10:07:08 TP0] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP0] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP1] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[2025-10-13 10:07:08 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=151.94 GB, mem usage=37.47 GB.
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP1] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 10:07:08 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 10:07:08 TP0] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP1] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP0] Memory pool end. avail mem=52.16 GB
[2025-10-13 10:07:08 TP3] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP2] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP7] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP6] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP4] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:08 TP5] KV Cache is allocated. #tokens: 3245954, K size: 49.53 GB, V size: 49.53 GB
[2025-10-13 10:07:12 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=50.43 GB
[2025-10-13 10:07:12 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.10 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[2025-10-13 10:07:15 TP5] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
hipcc -fPIC -DUSE_ROCM -DENABLE_FP8 -O3 -std=c++17 -DLEGACY_HIPBLAS_DIRECT -DUSE_PROF_API=1 -D__HIP_PLATFORM_HCC__=1 -D__HIP_PLATFORM_AMD__=1 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -mllvm --amdgpu-kernarg-preload-count=16 -Wno-unused-result -Wno-switch-bool -Wno-vla-cxx-extension -Wno-undefined-func-template -fgpu-flush-denormals-to-zero -mllvm --lsr-drop-solution=1 -fno-offload-uniform-block -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -amdgpu-coerce-illegal-types=1 --offload-arch=gfx942 -I/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include -c pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp -o pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP6] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP4] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP1] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP2] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP0] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP3] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-13 10:07:16 TP7] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
3 warnings generated when compiling for gfx942.
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for host.
hipcc -shared pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o -o lib.so
[aiter] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.38871024s
[2025-10-13 10:07:17 TP5] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.38871024s
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP5] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP5] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP6] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP6] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP4] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP4] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP2] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP2] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP1] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP1] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP0] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP0] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP7] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP7] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:07:18 TP3] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 10:07:18 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 10:07:18 TP3] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-13 10:07:18 TP5] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
clang (LLVM option parsing): Unknown command line argument '--amdgpu-use-amdgpu-trackers=1'.  Try: 'clang (LLVM option parsing) --help'
clang (LLVM option parsing): Did you mean '--amdgpu-use-aa-in-codegen=1'?
failed to execute:/opt/rocm/lib/llvm/bin/clang++  --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 -O3  -mllvm --amdgpu-use-amdgpu-trackers=1 -x hip -c /dev/null -o "/dev/null"
[aiter] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[2025-10-13 10:07:31 TP5] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 182.53082695s
[2025-10-13 10:10:20 TP5] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 182.53082695s
Capturing batches (bs=512 avail_mem=50.10 GB):   2%|▏         | 1/52 [03:12<2:43:48, 192.71s/it]Capturing batches (bs=496 avail_mem=49.27 GB):   2%|▏         | 1/52 [03:12<2:43:48, 192.71s/it][aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP5] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP4] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP1] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP0] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP2] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP3] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP7] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP6] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=496 avail_mem=49.27 GB):   4%|▍         | 2/52 [03:13<1:06:22, 79.65s/it] Capturing batches (bs=480 avail_mem=49.26 GB):   4%|▍         | 2/52 [03:13<1:06:22, 79.65s/it][aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP5] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP4] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP7] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP6] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP1] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP2] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP3] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP0] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=480 avail_mem=49.26 GB):   6%|▌         | 3/52 [03:13<35:25, 43.38s/it]  Capturing batches (bs=464 avail_mem=49.26 GB):   6%|▌         | 3/52 [03:13<35:25, 43.38s/it][aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP5] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP6] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP4] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP7] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP1] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP0] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP2] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP3] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=464 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:13<21:03, 26.33s/it]Capturing batches (bs=448 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:13<21:03, 26.33s/it][aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP4] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP6] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP7] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP5] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP0] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP1] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP2] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:25 TP3] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=448 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:13<13:14, 16.91s/it]Capturing batches (bs=432 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:13<13:14, 16.91s/it][aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP7] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP4] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP6] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP5] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP0] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP1] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP3] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP2] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=432 avail_mem=49.26 GB):  12%|█▏        | 6/52 [03:14<08:36, 11.24s/it]Capturing batches (bs=416 avail_mem=49.25 GB):  12%|█▏        | 6/52 [03:14<08:36, 11.24s/it][aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP4] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP7] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP5] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP6] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP0] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP1] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP2] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP3] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=416 avail_mem=49.25 GB):  13%|█▎        | 7/52 [03:14<05:43,  7.63s/it]Capturing batches (bs=400 avail_mem=49.25 GB):  13%|█▎        | 7/52 [03:14<05:43,  7.63s/it][aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP7] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP5] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP4] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP6] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP0] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP1] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP3] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP2] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=400 avail_mem=49.25 GB):  15%|█▌        | 8/52 [03:14<03:51,  5.27s/it]Capturing batches (bs=384 avail_mem=49.25 GB):  15%|█▌        | 8/52 [03:14<03:51,  5.27s/it][aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP4] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP6] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP5] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP7] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP0] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP3] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP1] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:26 TP2] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=384 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:14<02:38,  3.69s/it]Capturing batches (bs=368 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:14<02:38,  3.69s/it][aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP7] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP4] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP6] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP5] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP0] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP1] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP2] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP3] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=368 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:14<01:49,  2.61s/it]Capturing batches (bs=352 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:14<01:49,  2.61s/it][aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP6] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP7] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP5] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP1] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP0] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP2] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP3] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP4] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=352 avail_mem=49.25 GB):  21%|██        | 11/52 [03:15<01:16,  1.87s/it]Capturing batches (bs=336 avail_mem=49.24 GB):  21%|██        | 11/52 [03:15<01:16,  1.87s/it][aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP7] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP4] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP6] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP0] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP5] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP1] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP2] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP3] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=336 avail_mem=49.24 GB):  23%|██▎       | 12/52 [03:15<00:54,  1.36s/it]Capturing batches (bs=320 avail_mem=49.24 GB):  23%|██▎       | 12/52 [03:15<00:54,  1.36s/it][aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP1] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP4] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP0] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP7] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP3] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP6] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP2] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP5] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
Capturing batches (bs=320 avail_mem=49.24 GB):  25%|██▌       | 13/52 [03:15<00:39,  1.01s/it]Capturing batches (bs=304 avail_mem=49.24 GB):  25%|██▌       | 13/52 [03:15<00:39,  1.01s/it][aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP4] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP6] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP1] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP0] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP5] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP3] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP2] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:27 TP7] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=304 avail_mem=49.24 GB):  27%|██▋       | 14/52 [03:15<00:29,  1.30it/s]Capturing batches (bs=288 avail_mem=49.23 GB):  27%|██▋       | 14/52 [03:15<00:29,  1.30it/s][aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP1] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP0] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP7] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP3] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP4] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP5] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP6] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP2] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=288 avail_mem=49.23 GB):  29%|██▉       | 15/52 [03:15<00:22,  1.66it/s]Capturing batches (bs=272 avail_mem=49.23 GB):  29%|██▉       | 15/52 [03:15<00:22,  1.66it/s][aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP7] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP1] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP6] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP2] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP3] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP4] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP5] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP0] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=272 avail_mem=49.23 GB):  31%|███       | 16/52 [03:16<00:17,  2.06it/s]Capturing batches (bs=256 avail_mem=49.23 GB):  31%|███       | 16/52 [03:16<00:17,  2.06it/s][aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP1] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP3] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP0] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP4] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP5] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP7] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP2] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP6] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:16<00:14,  2.48it/s]Capturing batches (bs=248 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:16<00:14,  2.48it/s][aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP5] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP1] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP0] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP4] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP6] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP2] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP7] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP3] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=248 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:16<00:11,  2.89it/s]Capturing batches (bs=240 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:16<00:11,  2.89it/s][aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP1] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP5] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP0] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP4] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP7] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP2] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP3] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:28 TP6] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=240 avail_mem=49.23 GB):  37%|███▋      | 19/52 [03:16<00:09,  3.30it/s]Capturing batches (bs=232 avail_mem=49.22 GB):  37%|███▋      | 19/52 [03:16<00:09,  3.30it/s][aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP0] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP5] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP2] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP7] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP1] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP4] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP3] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP6] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=232 avail_mem=49.22 GB):  38%|███▊      | 20/52 [03:16<00:08,  3.67it/s]Capturing batches (bs=224 avail_mem=49.22 GB):  38%|███▊      | 20/52 [03:16<00:08,  3.67it/s][aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP5] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP1] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP0] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP4] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP3] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP7] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP2] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP6] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=224 avail_mem=49.22 GB):  40%|████      | 21/52 [03:17<00:07,  3.98it/s]Capturing batches (bs=216 avail_mem=49.22 GB):  40%|████      | 21/52 [03:17<00:07,  3.98it/s][aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP7] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP1] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP2] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP5] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP3] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP0] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP4] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP6] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=216 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:17<00:07,  4.22it/s]Capturing batches (bs=208 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:17<00:07,  4.22it/s][aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP5] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP0] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP6] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP4] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP7] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP1] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP2] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP3] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=208 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:17<00:06,  4.42it/s]Capturing batches (bs=200 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:17<00:06,  4.42it/s][aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP5] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP4] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP1] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP7] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP3] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP2] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP6] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:29 TP0] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=200 avail_mem=49.22 GB):  46%|████▌     | 24/52 [03:17<00:06,  4.55it/s]Capturing batches (bs=192 avail_mem=49.21 GB):  46%|████▌     | 24/52 [03:17<00:06,  4.55it/s][aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP0] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP1] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP3] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP4] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP7] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP5] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP2] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP6] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=192 avail_mem=49.21 GB):  48%|████▊     | 25/52 [03:17<00:05,  4.59it/s]Capturing batches (bs=184 avail_mem=49.21 GB):  48%|████▊     | 25/52 [03:17<00:05,  4.59it/s][aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP5] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP1] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP0] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP3] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP2] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP7] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP4] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP6] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=184 avail_mem=49.21 GB):  50%|█████     | 26/52 [03:18<00:05,  4.68it/s]Capturing batches (bs=176 avail_mem=49.21 GB):  50%|█████     | 26/52 [03:18<00:05,  4.68it/s][aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP5] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP1] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP7] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP0] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP6] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP3] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP4] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP2] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=176 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:18<00:05,  4.76it/s]Capturing batches (bs=168 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:18<00:05,  4.76it/s][aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP1] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP7] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP5] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP0] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP4] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP3] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP6] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP2] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=168 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:18<00:05,  4.74it/s]Capturing batches (bs=160 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:18<00:05,  4.74it/s][aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP5] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP0] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP1] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP6] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP2] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP4] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP7] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:30 TP3] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
Capturing batches (bs=160 avail_mem=49.21 GB):  56%|█████▌    | 29/52 [03:18<00:04,  4.79it/s]Capturing batches (bs=152 avail_mem=49.20 GB):  56%|█████▌    | 29/52 [03:18<00:04,  4.79it/s][aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP4] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP0] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP5] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP7] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP3] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP6] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP2] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP1] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=152 avail_mem=49.20 GB):  58%|█████▊    | 30/52 [03:19<00:04,  4.84it/s]Capturing batches (bs=144 avail_mem=49.20 GB):  58%|█████▊    | 30/52 [03:19<00:04,  4.84it/s][aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP1] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP0] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP3] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP2] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP5] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP6] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP4] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP7] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=144 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:19<00:04,  4.73it/s]Capturing batches (bs=136 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:19<00:04,  4.73it/s][aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP1] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP3] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP5] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP0] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP7] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP4] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP6] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP2] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=136 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:19<00:04,  4.80it/s]Capturing batches (bs=128 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:19<00:04,  4.80it/s][aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP3] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP0] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP5] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP1] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP7] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP4] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP2] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP6] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=49.20 GB):  63%|██████▎   | 33/52 [03:19<00:03,  4.85it/s]Capturing batches (bs=120 avail_mem=49.19 GB):  63%|██████▎   | 33/52 [03:19<00:03,  4.85it/s][aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP5] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP2] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP1] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP3] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP0] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP7] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP4] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:31 TP6] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=120 avail_mem=49.19 GB):  65%|██████▌   | 34/52 [03:19<00:03,  4.85it/s]Capturing batches (bs=112 avail_mem=49.19 GB):  65%|██████▌   | 34/52 [03:19<00:03,  4.85it/s][aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP1] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP3] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP5] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP4] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP6] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP0] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP7] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP2] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=112 avail_mem=49.19 GB):  67%|██████▋   | 35/52 [03:20<00:03,  4.88it/s]Capturing batches (bs=104 avail_mem=49.19 GB):  67%|██████▋   | 35/52 [03:20<00:03,  4.88it/s][aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP7] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP5] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP4] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP0] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP1] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP2] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP3] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP6] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=104 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:20<00:03,  4.79it/s]Capturing batches (bs=96 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:20<00:03,  4.79it/s] [aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP0] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP3] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP1] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP5] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP7] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP2] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP6] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP4] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=96 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:20<00:03,  4.84it/s]Capturing batches (bs=88 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:20<00:03,  4.84it/s][aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP7] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP0] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP2] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP4] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP5] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP3] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP6] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:32 TP1] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=88 avail_mem=49.19 GB):  73%|███████▎  | 38/52 [03:20<00:02,  4.85it/s]Capturing batches (bs=80 avail_mem=49.18 GB):  73%|███████▎  | 38/52 [03:20<00:02,  4.85it/s][aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP0] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP4] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP1] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP2] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP5] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP7] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP6] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP3] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=80 avail_mem=49.18 GB):  75%|███████▌  | 39/52 [03:20<00:02,  4.82it/s]Capturing batches (bs=72 avail_mem=49.18 GB):  75%|███████▌  | 39/52 [03:20<00:02,  4.82it/s][aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP0] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP1] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP2] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP3] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP7] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP4] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP5] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP6] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=72 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:21<00:02,  4.84it/s]Capturing batches (bs=64 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:21<00:02,  4.84it/s][aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP3] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP0] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP7] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP4] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP1] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP2] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP6] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP5] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=49.18 GB):  79%|███████▉  | 41/52 [03:21<00:02,  4.84it/s]Capturing batches (bs=56 avail_mem=49.17 GB):  79%|███████▉  | 41/52 [03:21<00:02,  4.84it/s][aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP0] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP1] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP4] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP3] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP2] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP5] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP7] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP6] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=56 avail_mem=49.17 GB):  81%|████████  | 42/52 [03:21<00:02,  4.88it/s]Capturing batches (bs=48 avail_mem=49.17 GB):  81%|████████  | 42/52 [03:21<00:02,  4.88it/s][aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP0] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP1] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP5] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP3] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP6] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP4] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP7] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:33 TP2] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=48 avail_mem=49.17 GB):  83%|████████▎ | 43/52 [03:21<00:01,  4.89it/s]Capturing batches (bs=40 avail_mem=49.17 GB):  83%|████████▎ | 43/52 [03:21<00:01,  4.89it/s][aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP1] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP0] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP4] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP3] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP2] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP5] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP6] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP7] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=40 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:21<00:01,  4.91it/s]Capturing batches (bs=32 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:21<00:01,  4.91it/s][aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP0] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP5] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP1] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP4] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP7] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP3] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP2] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP6] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:22<00:01,  4.92it/s]Capturing batches (bs=24 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:22<00:01,  4.92it/s][aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP1] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP7] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP0] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP6] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP5] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP4] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP3] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP2] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=24 avail_mem=49.17 GB):  88%|████████▊ | 46/52 [03:22<00:01,  4.92it/s]Capturing batches (bs=16 avail_mem=49.16 GB):  88%|████████▊ | 46/52 [03:22<00:01,  4.92it/s][aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP0] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP1] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP3] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP2] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP5] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP4] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP7] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP6] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
Capturing batches (bs=16 avail_mem=49.16 GB):  90%|█████████ | 47/52 [03:22<00:01,  4.34it/s]Capturing batches (bs=12 avail_mem=49.16 GB):  90%|█████████ | 47/52 [03:22<00:01,  4.34it/s][aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP0] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP1] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP3] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP6] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP4] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP2] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP7] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:34 TP5] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=12 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:22<00:00,  4.44it/s]Capturing batches (bs=8 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:22<00:00,  4.44it/s] [aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP0] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP1] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP2] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP7] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP6] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP5] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP3] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP4] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
Capturing batches (bs=8 avail_mem=49.16 GB):  94%|█████████▍| 49/52 [03:23<00:00,  4.59it/s]Capturing batches (bs=4 avail_mem=49.15 GB):  94%|█████████▍| 49/52 [03:23<00:00,  4.59it/s][aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP0] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP7] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP3] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP2] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP4] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP1] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP5] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP6] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=4 avail_mem=49.15 GB):  96%|█████████▌| 50/52 [03:23<00:00,  4.69it/s]Capturing batches (bs=2 avail_mem=49.15 GB):  96%|█████████▌| 50/52 [03:23<00:00,  4.69it/s][aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP1] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP7] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP5] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP3] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP6] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP0] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP4] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:35 TP2] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=2 avail_mem=49.15 GB):  98%|█████████▊| 51/52 [03:23<00:00,  4.76it/s]Capturing batches (bs=1 avail_mem=49.15 GB):  98%|█████████▊| 51/52 [03:23<00:00,  4.76it/s][aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP4] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP6] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP7] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP5] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP2] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP0] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP1] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:36 TP3] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:23<00:00,  3.19it/s]Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:23<00:00,  3.92s/it]
[2025-10-13 10:10:36 TP7] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP1] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP0] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP5] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP3] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP2] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP4] Registering 6708 cuda graph addresses
[2025-10-13 10:10:36 TP6] Registering 6708 cuda graph addresses
[2025-10-13 10:10:37 TP0] Capture cuda graph end. Time elapsed: 205.11 s. mem usage=1.29 GB. avail mem=49.14 GB.
[2025-10-13 10:10:37 TP0] max_total_num_tokens=3245954, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=49.14 GB
[2025-10-13 10:10:38] INFO:     Started server process [83411]
[2025-10-13 10:10:38] INFO:     Waiting for application startup.
[2025-10-13 10:10:38] INFO:     Application startup complete.
[2025-10-13 10:10:38] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-13 10:10:39] INFO:     127.0.0.1:49168 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:10:39 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP4] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP3] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP5] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP0] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP2] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP6] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP7] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:10:40 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40 TP1] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:40] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:10:40] The server is fired up and ready to roll!
[2025-10-13 10:10:48] INFO:     127.0.0.1:34222 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP3] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP2] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP0] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP4] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP6] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP5] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP7] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP1] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 796, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP2] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP3] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP0] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP1] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP5] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP4] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP7] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP6] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 39, #new-token: 2429, #cached-token: 31044, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP0] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP3] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP2] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP5] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP4] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP1] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP7] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP6] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 37, #new-token: 2284, #cached-token: 29527, token usage: 0.00, #running-req: 40, #queue-req: 0, 
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 174, #new-token: 11015, #cached-token: 138904, token usage: 0.00, #running-req: 77, #queue-req: 0, 
[2025-10-13 10:10:48 TP0] Prefill batch. #new-seq: 122, #new-token: 7620, #cached-token: 97414, token usage: 0.01, #running-req: 251, #queue-req: 0, 
[2025-10-13 10:10:49 TP0] Prefill batch. #new-seq: 267, #new-token: 16277, #cached-token: 213202, token usage: 0.01, #running-req: 373, #queue-req: 57, 
[2025-10-13 10:10:49 TP0] Prefill batch. #new-seq: 263, #new-token: 16374, #cached-token: 210057, token usage: 0.01, #running-req: 640, #queue-req: 30, 
[2025-10-13 10:10:50 TP0] Prefill batch. #new-seq: 256, #new-token: 16353, #cached-token: 204495, token usage: 0.02, #running-req: 903, #queue-req: 124, 
[2025-10-13 10:10:51 TP0] Prefill batch. #new-seq: 160, #new-token: 10379, #cached-token: 127828, token usage: 0.02, #running-req: 1159, #queue-req: 0, 
[2025-10-13 10:11:04 TP0] Decode batch. #running-req: 1319, #token: 126666, token usage: 0.04, cuda graph: False, gen throughput (token/s): 1549.17, #queue-req: 0, 
[2025-10-13 10:11:04] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:05] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:06] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:34432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:07] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:41658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:08] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09 TP0] Decode batch. #running-req: 1248, #token: 170779, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11037.96, #queue-req: 0, 
[2025-10-13 10:11:09] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:09] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:34414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:10] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:11] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (1013, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (978, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP4] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP2] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP6] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP5] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP1] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP0] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP3] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12 TP7] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:12] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:12] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:36306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:37636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (901, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:13] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP3] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP2] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP0] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP5] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP6] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP4] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP1] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:13 TP7] [fused_moe] using default for (896, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (873, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] Decode batch. #running-req: 883, #token: 157572, token usage: 0.05, cuda graph: False, gen throughput (token/s): 9192.89, #queue-req: 0, 
[2025-10-13 10:11:14] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (864, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (822, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:14] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP0] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP3] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP2] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP4] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP5] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP1] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP6] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:14 TP7] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (790, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (778, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (765, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (756, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (724, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:15] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP4] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP0] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP2] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP3] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP7] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP5] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP1] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:15 TP6] [fused_moe] using default for (716, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (699, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (675, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (662, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:38746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:44924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP0] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP5] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP1] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP3] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP7] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:41604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:16] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP2] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP6] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:16 TP4] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (605, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (571, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (564, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (557, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:44682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP0] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP7] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP3] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP1] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP6] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP4] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP5] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17 TP2] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:17] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:38230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:17] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18 TP0] Decode batch. #running-req: 509, #token: 112495, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7274.49, #queue-req: 0, 
[2025-10-13 10:11:18] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:18] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19 TP0] Decode batch. #running-req: 280, #token: 76423, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8726.37, #queue-req: 0, 
[2025-10-13 10:11:19] INFO:     127.0.0.1:35796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:19] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:20 TP0] Decode batch. #running-req: 227, #token: 71765, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7633.26, #queue-req: 0, 
[2025-10-13 10:11:21] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:21] INFO:     127.0.0.1:35808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22 TP0] Decode batch. #running-req: 206, #token: 73969, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7001.16, #queue-req: 0, 
[2025-10-13 10:11:22] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:22] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23 TP0] Decode batch. #running-req: 198, #token: 78628, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6761.46, #queue-req: 0, 
[2025-10-13 10:11:23] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:23] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:24] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:24 TP0] Decode batch. #running-req: 192, #token: 84299, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6340.86, #queue-req: 0, 
[2025-10-13 10:11:25] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:25] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:25 TP0] Decode batch. #running-req: 191, #token: 91000, token usage: 0.03, cuda graph: True, gen throughput (token/s): 5959.84, #queue-req: 0, 
[2025-10-13 10:11:26] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:27 TP0] Decode batch. #running-req: 189, #token: 98058, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4379.72, #queue-req: 0, 
[2025-10-13 10:11:28 TP0] Decode batch. #running-req: 189, #token: 105618, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6151.16, #queue-req: 0, 
[2025-10-13 10:11:30 TP0] Decode batch. #running-req: 189, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 6559.06, #queue-req: 0, 
[2025-10-13 10:11:30] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:40886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:30] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:43] INFO:     127.0.0.1:42098 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:11:43] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 37, #new-token: 37, #cached-token: 31815, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 42845, token usage: 0.00, #running-req: 38, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (50, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 31, #new-token: 31, #cached-token: 26777, token usage: 0.00, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51692, token usage: 0.00, #running-req: 119, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 50823, token usage: 0.00, #running-req: 179, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53110, token usage: 0.01, #running-req: 238, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP2] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP6] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP4] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP0] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP1] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP5] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP7] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:43 TP3] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55930, token usage: 0.01, #running-req: 300, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP2] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP6] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP4] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP1] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP5] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP3] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP7] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54305, token usage: 0.01, #running-req: 365, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP2] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP6] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP4] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP5] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP1] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP3] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP7] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 53967, token usage: 0.01, #running-req: 428, #queue-req: 0, 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56521, token usage: 0.01, #running-req: 491, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP2] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP1] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP3] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP6] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP4] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP7] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP5] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54241, token usage: 0.01, #running-req: 557, #queue-req: 0, 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 54923, token usage: 0.01, #running-req: 620, #queue-req: 0, 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 55075, token usage: 0.01, #running-req: 684, #queue-req: 0, 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54209, token usage: 0.02, #running-req: 748, #queue-req: 0, 
[2025-10-13 10:11:44 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 5274, token usage: 0.02, #running-req: 811, #queue-req: 0, 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP6] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP4] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP2] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP5] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP1] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP0] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP3] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:44 TP7] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51472, token usage: 0.02, #running-req: 817, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54214, token usage: 0.02, #running-req: 877, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53596, token usage: 0.02, #running-req: 940, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57752, token usage: 0.02, #running-req: 1002, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP0] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP1] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP2] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP3] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP6] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP4] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP7] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP5] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57652, token usage: 0.02, #running-req: 1069, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 55110, token usage: 0.02, #running-req: 1136, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54346, token usage: 0.03, #running-req: 1200, #queue-req: 0, 
[2025-10-13 10:11:45 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48235, token usage: 0.03, #running-req: 1263, #queue-req: 0, 
[2025-10-13 10:11:49] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:50 TP0] Decode batch. #running-req: 1318, #token: 136205, token usage: 0.04, cuda graph: False, gen throughput (token/s): 2560.23, #queue-req: 0, 
[2025-10-13 10:11:50] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:50] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:50] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:50] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:51] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:52] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:53] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54 TP0] Decode batch. #running-req: 1205, #token: 172927, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11407.48, #queue-req: 0, 
[2025-10-13 10:11:54] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:54] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:37348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:55] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP2] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP1] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP3] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP4] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP7] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP0] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP5] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP6] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP0] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP4] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP2] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP6] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP1] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP5] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP3] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP7] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP2] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP0] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP3] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP6] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP7] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP4] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP5] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP1] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:56] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP0] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP4] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP5] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP1] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP6] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP2] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP7] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:56 TP3] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (982, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:37316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (965, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (941, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:57] INFO:     127.0.0.1:37328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP6] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP2] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP4] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP0] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP5] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP1] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP3] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:57 TP7] [fused_moe] using default for (907, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (898, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (876, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (843, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP2] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP6] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP5] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP1] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP4] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP0] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP3] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58 TP7] [fused_moe] using default for (830, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:58] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:58] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (802, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] Decode batch. #running-req: 810, #token: 150825, token usage: 0.05, cuda graph: False, gen throughput (token/s): 8622.46, #queue-req: 0, 
[2025-10-13 10:11:59] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (791, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (779, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:42656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:11:59] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP2] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP6] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP1] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP5] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP0] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP4] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP3] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:11:59 TP7] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (722, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (717, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (697, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (692, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP2] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP0] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP6] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP4] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP1] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP5] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP3] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00 TP7] [fused_moe] using default for (669, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:00] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:00] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (663, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (583, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP1] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP5] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP4] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP0] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP2] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP6] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP3] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01 TP7] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:01] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:01] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP0] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP3] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP2] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP1] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP6] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP7] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP4] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP5] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP2] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP6] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP3] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP0] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP7] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP4] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP1] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP5] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP2] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP6] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP0] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP4] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP3] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP7] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP1] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP5] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP2] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP6] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP4] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP0] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP3] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP7] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP1] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP5] [fused_moe] using default for (531, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP2] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP6] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP4] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP0] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP3] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP7] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP1] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02 TP5] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:02] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:02] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03 TP0] Decode batch. #running-req: 455, #token: 105094, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6410.61, #queue-req: 0, 
[2025-10-13 10:12:03] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:03] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:45518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:04] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05 TP0] Decode batch. #running-req: 279, #token: 78133, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7391.81, #queue-req: 0, 
[2025-10-13 10:12:05] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:46320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:05] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06 TP0] Decode batch. #running-req: 229, #token: 73924, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7776.71, #queue-req: 0, 
[2025-10-13 10:12:06] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:06] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:07 TP0] Decode batch. #running-req: 209, #token: 75717, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7000.15, #queue-req: 0, 
[2025-10-13 10:12:07] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:08] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:08] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:08] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:08] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:08 TP0] Decode batch. #running-req: 203, #token: 81977, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7067.61, #queue-req: 0, 
[2025-10-13 10:12:08] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:09] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:09 TP0] Decode batch. #running-req: 201, #token: 89191, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6751.01, #queue-req: 0, 
[2025-10-13 10:12:10] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:10] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:10] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:11 TP0] Decode batch. #running-req: 198, #token: 95726, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6422.22, #queue-req: 0, 
[2025-10-13 10:12:12] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:12 TP0] Decode batch. #running-req: 197, #token: 103089, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6692.64, #queue-req: 0, 
[2025-10-13 10:12:13 TP0] Decode batch. #running-req: 197, #token: 110969, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6541.70, #queue-req: 0, 
[2025-10-13 10:12:14] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:14] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:28] INFO:     127.0.0.1:47258 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:12:28] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 43, #new-token: 43, #cached-token: 36947, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (43, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 44, #new-token: 44, #cached-token: 37736, token usage: 0.00, #running-req: 44, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (44, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 29, #new-token: 29, #cached-token: 25011, token usage: 0.00, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (29, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49103, token usage: 0.00, #running-req: 117, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48174, token usage: 0.00, #running-req: 174, #queue-req: 0, 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52280, token usage: 0.01, #running-req: 230, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48165, token usage: 0.01, #running-req: 291, #queue-req: 0, 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55897, token usage: 0.01, #running-req: 347, #queue-req: 0, 
[2025-10-13 10:12:28 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 40599, token usage: 0.01, #running-req: 412, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP4] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP0] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP2] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP6] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP1] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP3] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP5] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:28 TP7] [fused_moe] using default for (47, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 7746, token usage: 0.01, #running-req: 459, #queue-req: 0, 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP1] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP3] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP2] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP4] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP5] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP6] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP7] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] [fused_moe] using default for (9, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 81, #new-token: 81, #cached-token: 69214, token usage: 0.01, #running-req: 468, #queue-req: 0, 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP4] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP1] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP2] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP3] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP6] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP5] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP7] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49115, token usage: 0.01, #running-req: 549, #queue-req: 0, 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 49808, token usage: 0.01, #running-req: 606, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP4] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP1] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP2] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP3] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP6] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP5] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP7] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 48993, token usage: 0.01, #running-req: 664, #queue-req: 0, 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53337, token usage: 0.02, #running-req: 721, #queue-req: 0, 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48414, token usage: 0.02, #running-req: 783, #queue-req: 0, 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51273, token usage: 0.02, #running-req: 839, #queue-req: 0, 
[2025-10-13 10:12:29 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 58794, token usage: 0.02, #running-req: 899, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51833, token usage: 0.02, #running-req: 967, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 56036, token usage: 0.02, #running-req: 1027, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53092, token usage: 0.02, #running-req: 1092, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52665, token usage: 0.02, #running-req: 1154, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57811, token usage: 0.03, #running-req: 1215, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Prefill batch. #new-seq: 37, #new-token: 37, #cached-token: 31841, token usage: 0.03, #running-req: 1282, #queue-req: 0, 
[2025-10-13 10:12:30 TP0] Decode batch. #running-req: 1319, #token: 88193, token usage: 0.03, cuda graph: False, gen throughput (token/s): 593.25, #queue-req: 0, 
[2025-10-13 10:12:33] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35 TP0] Decode batch. #running-req: 1318, #token: 140737, token usage: 0.04, cuda graph: False, gen throughput (token/s): 11897.13, #queue-req: 0, 
[2025-10-13 10:12:35] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:35] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:36] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:37] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:55080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:50096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:38] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:39] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40 TP0] Decode batch. #running-req: 1169, #token: 171427, token usage: 0.05, cuda graph: False, gen throughput (token/s): 10647.81, #queue-req: 0, 
[2025-10-13 10:12:40] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:40] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:41] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP6] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP7] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP4] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP0] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP5] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP3] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP2] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:41 TP1] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (985, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (968, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (952, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (945, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (939, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:42] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP6] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP4] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP2] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP0] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP3] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP7] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP1] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:42 TP5] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (887, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP0] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP6] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP3] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP1] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP5] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP4] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP7] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43 TP2] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:43] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:43] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] Decode batch. #running-req: 780, #token: 148742, token usage: 0.05, cuda graph: False, gen throughput (token/s): 8470.07, #queue-req: 0, 
[2025-10-13 10:12:44] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (762, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (749, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP6] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP0] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP4] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP2] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP5] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP1] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP3] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44 TP7] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:44] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:44] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (702, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (670, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:45] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP4] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP6] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP2] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP0] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP5] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP1] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP3] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:45 TP7] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (596, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (539, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:49336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP7] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP0] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP3] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP1] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP5] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP6] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP2] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:46] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:46 TP4] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP5] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP1] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP7] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP3] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP2] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP6] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP0] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP1] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP5] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP4] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP3] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP7] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP0] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP2] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47 TP6] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-13 10:12:47] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:47] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48 TP0] Decode batch. #running-req: 447, #token: 102879, token usage: 0.03, cuda graph: True, gen throughput (token/s): 5944.32, #queue-req: 0, 
[2025-10-13 10:12:48] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:48] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:49] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50 TP0] Decode batch. #running-req: 264, #token: 75063, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8351.08, #queue-req: 0, 
[2025-10-13 10:12:50] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:50] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51 TP0] Decode batch. #running-req: 220, #token: 71478, token usage: 0.02, cuda graph: True, gen throughput (token/s): 5590.72, #queue-req: 0, 
[2025-10-13 10:12:51] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:51] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:52] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53 TP0] Decode batch. #running-req: 202, #token: 73818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6821.10, #queue-req: 0, 
[2025-10-13 10:12:53] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:53] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:54 TP0] Decode batch. #running-req: 197, #token: 80202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6857.95, #queue-req: 0, 
[2025-10-13 10:12:54] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:54] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:55] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:55] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:55 TP0] Decode batch. #running-req: 193, #token: 86272, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6540.84, #queue-req: 0, 
[2025-10-13 10:12:56] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:57] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:57 TP0] Decode batch. #running-req: 191, #token: 92976, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4602.00, #queue-req: 0, 
[2025-10-13 10:12:57] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:12:58 TP0] Decode batch. #running-req: 190, #token: 100108, token usage: 0.03, cuda graph: True, gen throughput (token/s): 5420.81, #queue-req: 0, 
[2025-10-13 10:12:59 TP0] Decode batch. #running-req: 190, #token: 107708, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6703.79, #queue-req: 0, 
[2025-10-13 10:13:00] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:00] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:13:06] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 10:13:09] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
