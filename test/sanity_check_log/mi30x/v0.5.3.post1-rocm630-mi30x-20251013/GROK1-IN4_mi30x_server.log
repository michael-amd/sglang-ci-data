/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:35 __init__.py:179] Automatically detected platform rocm.
WARNING 10-13 09:57:35 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 09:57:36] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/amd--grok-1-W4A8KV8', tokenizer_path='/mnt/raid/models/huggingface/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=675143909, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, crash_on_nan=False, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/amd--grok-1-W4A8KV8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.2, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-13 09:57:37] No chat template found, defaulting to 'string' content format
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:46 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-13 09:57:47 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 09:57:48 TP1] Process 67482 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 09:57:49 TP7] Process 67488 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-13 09:57:50 TP5] Process 67486 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 09:57:50 TP0] Process 67481 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 09:57:50 TP2] Process 67483 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 09:57:50 TP3] Process 67484 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 09:57:50 TP6] Process 67487 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 09:57:50 TP4] Process 67485 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-13 09:57:50 TP0] Init torch distributed begin.
[2025-10-13 09:57:51 TP0] sglang is using nccl==2.21.5
[2025-10-13 09:57:52 TP0] Init torch distributed ends. mem usage=2.07 GB
[2025-10-13 09:57:52 TP0] Load weight begin. avail mem=189.41 GB
[2025-10-13 09:57:52 TP0] Detected fp8 checkpoint.
[2025-10-13 09:57:53 TP0] #parameters (analytical): 316.49 B, #parameters (actual): 46.00 B
Loading pt checkpoint shards:   0% Completed | 0/33 [00:00<?, ?it/s]
[2025-10-13 09:57:53 TP5] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP5] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP7] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP7] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP5] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP5] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP4] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP4] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP6] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP6] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP7] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP7] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP4] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP4] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP0] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP2] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP0] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP0] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP0] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP2] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   3% Completed | 1/33 [00:00<00:21,  1.47it/s]
[2025-10-13 09:57:53 TP3] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP3] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP1] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP1] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP2] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP2] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP3] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP1] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP3] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:53 TP1] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   6% Completed | 2/33 [00:01<00:18,  1.72it/s]
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP5] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP6] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP7] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP0] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   9% Completed | 3/33 [00:01<00:15,  1.89it/s]
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP1] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP3] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP4] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:54 TP2] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  12% Completed | 4/33 [00:02<00:13,  2.08it/s]
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP1] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP5] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP0] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  15% Completed | 5/33 [00:02<00:14,  1.90it/s]
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP6] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP7] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP4] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP3] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:55 TP2] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  18% Completed | 6/33 [00:03<00:14,  1.91it/s]
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP3] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP0] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP1] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  21% Completed | 7/33 [00:03<00:12,  2.03it/s]
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP2] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP4] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP6] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP5] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:56 TP7] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  24% Completed | 8/33 [00:04<00:11,  2.10it/s]
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP0] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  27% Completed | 9/33 [00:04<00:11,  2.17it/s]
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP2] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP1] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP6] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP7] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:57 TP4] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  30% Completed | 10/33 [00:04<00:10,  2.16it/s]
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  33% Completed | 11/33 [00:05<00:09,  2.21it/s]
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP2] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP1] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP6] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP0] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  36% Completed | 12/33 [00:05<00:08,  2.36it/s]
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP7] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:58 TP4] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  39% Completed | 13/33 [00:06<00:07,  2.58it/s]
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  42% Completed | 14/33 [00:06<00:06,  2.75it/s]
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP0] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  45% Completed | 15/33 [00:06<00:06,  2.90it/s]
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP7] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP6] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP1] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP2] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:57:59 TP4] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP5] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP5] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP5] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP3] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP5] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP3] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP3] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP3] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  48% Completed | 16/33 [00:07<00:06,  2.70it/s]
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP1] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP1] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP1] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP1] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP6] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP0] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP4] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  52% Completed | 17/33 [00:07<00:07,  2.12it/s]
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:00 TP2] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP0] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP6] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP0] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP2] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP0] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP0] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  55% Completed | 18/33 [00:08<00:08,  1.74it/s]
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP1] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:01 TP4] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
Loading pt checkpoint shards:  58% Completed | 19/33 [00:09<00:07,  1.76it/s]
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  61% Completed | 20/33 [00:09<00:06,  2.06it/s]
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP1] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP4] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP6] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP3] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP5] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP0] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:02 TP2] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  64% Completed | 21/33 [00:09<00:05,  2.22it/s]
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP7] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP7] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP7] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP7] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  67% Completed | 22/33 [00:10<00:05,  1.99it/s]
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP6] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP2] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP3] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP5] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP0] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:03 TP1] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  70% Completed | 23/33 [00:10<00:04,  2.03it/s]
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP2] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP0] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  73% Completed | 24/33 [00:11<00:04,  1.98it/s]
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP3] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP6] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:04 TP5] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP3] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP5] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:05 TP7] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP4] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP4] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP4] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP4] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP7] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP5] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:06 TP3] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP1] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP1] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP1] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP1] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP2] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP2] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP2] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP2] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP5] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP7] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP3] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP0] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:07 TP0] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP0] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP0] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP6] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP6] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP6] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP6] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP5] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP7] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:08 TP3] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP1] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP1] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP1] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP1] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  76% Completed | 25/33 [00:16<00:15,  1.89s/it]
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP2] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP2] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP3] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP7] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP4] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:09 TP5] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP5] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP6] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  79% Completed | 26/33 [00:17<00:10,  1.52s/it]
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP6] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP7] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP3] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP7] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP4] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP0] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  82% Completed | 27/33 [00:17<00:07,  1.25s/it]
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP2] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:10 TP1] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP2] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP3] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  85% Completed | 28/33 [00:18<00:05,  1.03s/it]
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:11 TP2] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:11 TP3] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP0] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  88% Completed | 29/33 [00:18<00:03,  1.18it/s]
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP4] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:11 TP1] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP4] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP4] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP4] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:12 TP4] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  91% Completed | 30/33 [00:19<00:02,  1.37it/s]
[2025-10-13 09:58:12 TP1] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP1] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP1] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP1] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP1] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:12 TP1] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:12 TP0] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  94% Completed | 31/33 [00:19<00:01,  1.43it/s]
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  97% Completed | 32/33 [00:20<00:00,  1.70it/s]
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-10-13 09:58:13 TP0] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards: 100% Completed | 33/33 [00:20<00:00,  1.96it/s]
Loading pt checkpoint shards: 100% Completed | 33/33 [00:20<00:00,  1.61it/s]

[2025-10-13 09:58:13 TP0] #all_names: 1219, #hit_names: 1219, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:13 TP0] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-13 09:58:14 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=170.20 GB, mem usage=19.21 GB.
[2025-10-13 09:58:14 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 09:58:14 TP2] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP3] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP0] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP0] Memory pool end. avail mem=28.53 GB
[2025-10-13 09:58:14 TP7] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP6] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP4] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP5] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:14 TP1] KV Cache is allocated. #tokens: 4633795, K size: 70.71 GB, V size: 70.71 GB
[2025-10-13 09:58:15 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=27.95 GB
[2025-10-13 09:58:15 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=27.68 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] start build /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d
[2025-10-13 09:58:20 TP0] start build /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d
hipcc -fPIC -DUSE_ROCM -DENABLE_FP8 -O3 -std=c++17 -DLEGACY_HIPBLAS_DIRECT -DUSE_PROF_API=1 -D__HIP_PLATFORM_HCC__=1 -D__HIP_PLATFORM_AMD__=1 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -mllvm --amdgpu-kernarg-preload-count=16 -Wno-unused-result -Wno-switch-bool -Wno-vla-cxx-extension -Wno-undefined-func-template -fgpu-flush-denormals-to-zero -mllvm --lsr-drop-solution=1 -fno-offload-uniform-block -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -amdgpu-coerce-illegal-types=1 --offload-arch=gfx942 -I/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include -c pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d.cpp -o pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d.o
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP4] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP2] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP1] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP7] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP5] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP3] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
[2025-10-13 09:58:21 TP6] waiting for baton release at /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/lock
In file included from pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d.cpp:1:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for gfx942.
In file included from pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d.cpp:1:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for host.
hipcc -shared pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d.o -o lib.so
[aiter] finish build /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d, cost 2.32441244s
[2025-10-13 09:58:22 TP0] finish build /root/.aiter/build/pa_ragged_6fb5a21f6c081c6fd9f35860fddc366d, cost 2.32441244s
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP7] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP7] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP5] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP5] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP3] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP3] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP1] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP6] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP0] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP1] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP0] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP6] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP2] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 09:58:24 TP4] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP2] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-13 09:58:24 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-13 09:58:24 TP4] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[aiter] start build [module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
[2025-10-13 09:58:24 TP7] start build [module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2
clang (LLVM option parsing): Unknown command line argument '--amdgpu-use-amdgpu-trackers=1'.  Try: 'clang (LLVM option parsing) --help'
clang (LLVM option parsing): Did you mean '--amdgpu-use-aa-in-codegen=1'?
failed to execute:/opt/rocm/lib/llvm/bin/clang++  --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 -O3  -mllvm --amdgpu-use-amdgpu-trackers=1 -x hip -c /dev/null -o "/dev/null"
[aiter] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[2025-10-13 09:58:29 TP7] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2], cost 162.30193350s
[2025-10-13 10:01:06 TP7] finish build [module_moe_ck2stages_f8_i4_b16_gelu_per_token_mulWeightStage2], cost 162.30193350s
Capturing batches (bs=512 avail_mem=27.68 GB):   2%|         | 1/52 [02:54<2:28:02, 174.16s/it]Capturing batches (bs=496 avail_mem=26.85 GB):   2%|         | 1/52 [02:54<2:28:02, 174.16s/it][aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP3] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP0] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP2] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP1] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP4] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP5] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP6] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP7] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=496 avail_mem=26.85 GB):   4%|         | 2/52 [02:54<1:00:07, 72.15s/it] Capturing batches (bs=480 avail_mem=26.85 GB):   4%|         | 2/52 [02:54<1:00:07, 72.15s/it][aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP2] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP4] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP1] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP0] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP3] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP7] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP5] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:10 TP6] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=480 avail_mem=26.85 GB):   6%|         | 3/52 [02:55<32:06, 39.31s/it]  Capturing batches (bs=464 avail_mem=26.84 GB):   6%|         | 3/52 [02:55<32:06, 39.31s/it][aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP4] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP7] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP6] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP5] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP1] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP3] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP2] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP0] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=464 avail_mem=26.84 GB):   8%|         | 4/52 [02:55<19:06, 23.88s/it]Capturing batches (bs=448 avail_mem=26.84 GB):   8%|         | 4/52 [02:55<19:06, 23.88s/it][aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP2] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP4] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP3] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP1] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP0] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP7] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP5] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP6] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=448 avail_mem=26.84 GB):  10%|         | 5/52 [02:55<12:01, 15.34s/it]Capturing batches (bs=432 avail_mem=26.84 GB):  10%|         | 5/52 [02:55<12:01, 15.34s/it][aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP3] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP4] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP5] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP2] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP7] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP6] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP0] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP1] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=432 avail_mem=26.84 GB):  12%|        | 6/52 [02:55<07:49, 10.20s/it]Capturing batches (bs=416 avail_mem=26.84 GB):  12%|        | 6/52 [02:55<07:49, 10.20s/it][aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP4] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP2] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP1] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP0] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP5] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP7] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP3] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:11 TP6] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=416 avail_mem=26.84 GB):  13%|        | 7/52 [02:55<05:12,  6.93s/it]Capturing batches (bs=400 avail_mem=26.84 GB):  13%|        | 7/52 [02:55<05:12,  6.93s/it][aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP0] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP4] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP2] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP3] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP5] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP1] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP7] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP6] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=400 avail_mem=26.84 GB):  15%|        | 8/52 [02:56<03:30,  4.79s/it]Capturing batches (bs=384 avail_mem=26.83 GB):  15%|        | 8/52 [02:56<03:30,  4.79s/it][aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP2] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP3] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP0] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP4] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP1] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP5] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP7] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP6] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=384 avail_mem=26.83 GB):  17%|        | 9/52 [02:56<02:24,  3.36s/it]Capturing batches (bs=368 avail_mem=26.83 GB):  17%|        | 9/52 [02:56<02:24,  3.36s/it][aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP1] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP0] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP3] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP2] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP5] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP6] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP4] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP7] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=368 avail_mem=26.83 GB):  19%|        | 10/52 [02:56<01:40,  2.39s/it]Capturing batches (bs=352 avail_mem=26.83 GB):  19%|        | 10/52 [02:56<01:40,  2.39s/it][aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP2] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP0] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP3] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP4] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP5] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP1] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP6] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP7] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=352 avail_mem=26.83 GB):  21%|        | 11/52 [02:56<01:10,  1.72s/it]Capturing batches (bs=336 avail_mem=26.83 GB):  21%|        | 11/52 [02:56<01:10,  1.72s/it][aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP0] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP2] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP4] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP1] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP3] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP5] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP7] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:12 TP6] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=336 avail_mem=26.83 GB):  23%|       | 12/52 [02:57<00:50,  1.27s/it]Capturing batches (bs=320 avail_mem=26.83 GB):  23%|       | 12/52 [02:57<00:50,  1.27s/it][aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP2] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP0] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP4] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP1] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP3] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP5] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP6] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP7] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=320 avail_mem=26.83 GB):  25%|       | 13/52 [02:57<00:37,  1.05it/s]Capturing batches (bs=304 avail_mem=26.82 GB):  25%|       | 13/52 [02:57<00:37,  1.05it/s][aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP0] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP4] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP2] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP1] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP3] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP6] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP5] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP7] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=304 avail_mem=26.82 GB):  27%|       | 14/52 [02:57<00:27,  1.37it/s]Capturing batches (bs=288 avail_mem=26.82 GB):  27%|       | 14/52 [02:57<00:27,  1.37it/s][aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP3] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP0] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP1] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP2] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP4] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP7] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP5] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP6] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=288 avail_mem=26.82 GB):  29%|       | 15/52 [02:57<00:21,  1.72it/s]Capturing batches (bs=272 avail_mem=26.82 GB):  29%|       | 15/52 [02:57<00:21,  1.72it/s][aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP2] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP0] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP5] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP6] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP4] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP7] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP1] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:13 TP3] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=272 avail_mem=26.82 GB):  31%|       | 16/52 [02:57<00:17,  2.11it/s]Capturing batches (bs=256 avail_mem=26.81 GB):  31%|       | 16/52 [02:57<00:17,  2.11it/s][aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP2] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP3] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP7] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP6] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP0] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP4] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP5] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP1] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=26.81 GB):  33%|      | 17/52 [02:58<00:13,  2.53it/s]Capturing batches (bs=248 avail_mem=26.81 GB):  33%|      | 17/52 [02:58<00:13,  2.53it/s][aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP3] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP2] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP0] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP5] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP4] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP1] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP6] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP7] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=248 avail_mem=26.81 GB):  35%|      | 18/52 [02:58<00:11,  2.91it/s]Capturing batches (bs=240 avail_mem=26.81 GB):  35%|      | 18/52 [02:58<00:11,  2.91it/s][aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP7] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP3] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP2] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP0] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP1] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP5] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP4] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP6] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=240 avail_mem=26.81 GB):  37%|      | 19/52 [02:58<00:10,  3.25it/s]Capturing batches (bs=232 avail_mem=26.81 GB):  37%|      | 19/52 [02:58<00:10,  3.25it/s][aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP2] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP0] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP3] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP1] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP7] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP4] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP5] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP6] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=232 avail_mem=26.81 GB):  38%|      | 20/52 [02:58<00:08,  3.56it/s]Capturing batches (bs=224 avail_mem=26.81 GB):  38%|      | 20/52 [02:58<00:08,  3.56it/s][aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP3] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP2] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP0] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP7] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP5] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP4] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP1] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:14 TP6] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=224 avail_mem=26.81 GB):  40%|      | 21/52 [02:59<00:08,  3.74it/s]Capturing batches (bs=216 avail_mem=26.80 GB):  40%|      | 21/52 [02:59<00:08,  3.74it/s][aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP0] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP2] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP3] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP5] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP4] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP6] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP7] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP1] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=216 avail_mem=26.80 GB):  42%|     | 22/52 [02:59<00:07,  3.90it/s]Capturing batches (bs=208 avail_mem=26.80 GB):  42%|     | 22/52 [02:59<00:07,  3.90it/s][aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP2] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP3] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP0] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP1] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP5] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP4] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP7] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP6] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=208 avail_mem=26.80 GB):  44%|     | 23/52 [02:59<00:07,  4.02it/s]Capturing batches (bs=200 avail_mem=26.80 GB):  44%|     | 23/52 [02:59<00:07,  4.02it/s][aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP2] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP0] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP3] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP5] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP7] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP6] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP4] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP1] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=200 avail_mem=26.80 GB):  46%|     | 24/52 [02:59<00:06,  4.19it/s]Capturing batches (bs=192 avail_mem=26.80 GB):  46%|     | 24/52 [02:59<00:06,  4.19it/s][aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP0] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP3] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP2] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP1] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP7] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP4] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP5] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:15 TP6] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=192 avail_mem=26.80 GB):  48%|     | 25/52 [02:59<00:06,  4.24it/s]Capturing batches (bs=184 avail_mem=26.80 GB):  48%|     | 25/52 [02:59<00:06,  4.24it/s][aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP3] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP2] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP5] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP6] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP7] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP1] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP0] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP4] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=184 avail_mem=26.80 GB):  50%|     | 26/52 [03:00<00:05,  4.33it/s]Capturing batches (bs=176 avail_mem=26.79 GB):  50%|     | 26/52 [03:00<00:05,  4.33it/s][aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP3] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP2] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP0] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP5] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP6] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP4] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP7] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP1] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=176 avail_mem=26.79 GB):  52%|    | 27/52 [03:00<00:05,  4.38it/s]Capturing batches (bs=168 avail_mem=26.79 GB):  52%|    | 27/52 [03:00<00:05,  4.38it/s][aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP2] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP0] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP1] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP3] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP4] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP5] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP6] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP7] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=168 avail_mem=26.79 GB):  54%|    | 28/52 [03:00<00:05,  4.25it/s]Capturing batches (bs=160 avail_mem=26.79 GB):  54%|    | 28/52 [03:00<00:05,  4.25it/s][aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP3] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP2] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP0] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP5] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP7] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP6] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP1] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP4] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=160 avail_mem=26.79 GB):  56%|    | 29/52 [03:00<00:05,  4.35it/s]Capturing batches (bs=152 avail_mem=26.79 GB):  56%|    | 29/52 [03:00<00:05,  4.35it/s][aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP7] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP2] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP0] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP3] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP6] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP1] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP4] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:16 TP5] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=152 avail_mem=26.79 GB):  58%|    | 30/52 [03:01<00:05,  4.40it/s]Capturing batches (bs=144 avail_mem=26.78 GB):  58%|    | 30/52 [03:01<00:05,  4.40it/s][aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP3] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP0] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP2] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP7] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP6] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP1] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP4] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP5] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=144 avail_mem=26.78 GB):  60%|    | 31/52 [03:01<00:04,  4.46it/s]Capturing batches (bs=136 avail_mem=26.78 GB):  60%|    | 31/52 [03:01<00:04,  4.46it/s][aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP5] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP6] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP7] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP4] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP3] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP2] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP0] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP1] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=136 avail_mem=26.78 GB):  62%|   | 32/52 [03:01<00:04,  4.42it/s]Capturing batches (bs=128 avail_mem=26.78 GB):  62%|   | 32/52 [03:01<00:04,  4.42it/s][aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP2] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP0] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP7] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP3] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP1] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP4] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP5] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP6] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=26.78 GB):  63%|   | 33/52 [03:01<00:04,  4.50it/s]Capturing batches (bs=120 avail_mem=26.78 GB):  63%|   | 33/52 [03:01<00:04,  4.50it/s][aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP2] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP0] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP3] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP1] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP6] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP4] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP5] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:17 TP7] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=120 avail_mem=26.78 GB):  65%|   | 34/52 [03:02<00:03,  4.52it/s]Capturing batches (bs=112 avail_mem=26.78 GB):  65%|   | 34/52 [03:02<00:03,  4.52it/s][aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP2] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP0] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP3] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP5] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP1] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP6] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP4] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP7] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=112 avail_mem=26.78 GB):  67%|   | 35/52 [03:02<00:03,  4.48it/s]Capturing batches (bs=104 avail_mem=26.77 GB):  67%|   | 35/52 [03:02<00:03,  4.48it/s][aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP2] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP3] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP7] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP0] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP1] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP4] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP6] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP5] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=104 avail_mem=26.77 GB):  69%|   | 36/52 [03:02<00:03,  4.54it/s]Capturing batches (bs=96 avail_mem=26.77 GB):  69%|   | 36/52 [03:02<00:03,  4.54it/s] [aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP0] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP2] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP3] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP4] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP6] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP5] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP7] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP1] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=96 avail_mem=26.77 GB):  71%|   | 37/52 [03:02<00:03,  4.56it/s]Capturing batches (bs=88 avail_mem=26.77 GB):  71%|   | 37/52 [03:02<00:03,  4.56it/s][aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP2] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP1] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP4] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP3] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP5] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP7] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP6] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP0] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=88 avail_mem=26.77 GB):  73%|  | 38/52 [03:02<00:03,  4.57it/s]Capturing batches (bs=80 avail_mem=26.77 GB):  73%|  | 38/52 [03:02<00:03,  4.57it/s][aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP2] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP3] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP0] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP6] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP1] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP7] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP5] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:18 TP4] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=80 avail_mem=26.77 GB):  75%|  | 39/52 [03:03<00:02,  4.60it/s]Capturing batches (bs=72 avail_mem=26.76 GB):  75%|  | 39/52 [03:03<00:02,  4.60it/s][aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP3] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP2] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP7] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP6] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP0] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP5] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP1] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP4] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=72 avail_mem=26.76 GB):  77%|  | 40/52 [03:03<00:02,  4.57it/s]Capturing batches (bs=64 avail_mem=26.76 GB):  77%|  | 40/52 [03:03<00:02,  4.57it/s][aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP3] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP2] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP0] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP7] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP1] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP4] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP5] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP6] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=26.76 GB):  79%|  | 41/52 [03:03<00:02,  4.52it/s]Capturing batches (bs=56 avail_mem=26.76 GB):  79%|  | 41/52 [03:03<00:02,  4.52it/s][aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP2] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP5] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP3] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP4] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP6] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP0] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP1] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP7] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=56 avail_mem=26.76 GB):  81%|  | 42/52 [03:03<00:02,  4.52it/s]Capturing batches (bs=48 avail_mem=26.76 GB):  81%|  | 42/52 [03:03<00:02,  4.52it/s][aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP2] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP3] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP0] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP4] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP1] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP7] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP5] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:19 TP6] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=48 avail_mem=26.76 GB):  83%| | 43/52 [03:04<00:02,  4.36it/s]Capturing batches (bs=40 avail_mem=26.75 GB):  83%| | 43/52 [03:04<00:02,  4.36it/s][aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP0] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP3] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP2] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP7] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP5] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP4] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP1] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP6] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=40 avail_mem=26.75 GB):  85%| | 44/52 [03:04<00:01,  4.45it/s]Capturing batches (bs=32 avail_mem=26.75 GB):  85%| | 44/52 [03:04<00:01,  4.45it/s][aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP2] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP3] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP7] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP0] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP1] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP5] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP4] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP6] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=26.75 GB):  87%| | 45/52 [03:04<00:01,  4.46it/s]Capturing batches (bs=24 avail_mem=26.75 GB):  87%| | 45/52 [03:04<00:01,  4.46it/s][aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP3] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP2] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP0] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP1] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP5] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP7] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP6] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP4] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=24 avail_mem=26.75 GB):  88%| | 46/52 [03:04<00:01,  4.37it/s]Capturing batches (bs=16 avail_mem=26.75 GB):  88%| | 46/52 [03:04<00:01,  4.37it/s][aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP2] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP0] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP3] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP1] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP4] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP7] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP6] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:20 TP5] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=16 avail_mem=26.75 GB):  90%| | 47/52 [03:04<00:01,  4.34it/s]Capturing batches (bs=12 avail_mem=26.75 GB):  90%| | 47/52 [03:04<00:01,  4.34it/s][aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP2] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP3] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP0] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP7] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP4] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP1] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP6] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP5] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=12 avail_mem=26.75 GB):  92%|| 48/52 [03:05<00:00,  4.35it/s]Capturing batches (bs=8 avail_mem=26.74 GB):  92%|| 48/52 [03:05<00:00,  4.35it/s] [aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP2] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP0] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP3] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP7] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP1] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP4] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP5] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP6] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=8 avail_mem=26.74 GB):  94%|| 49/52 [03:05<00:00,  4.33it/s]Capturing batches (bs=4 avail_mem=26.74 GB):  94%|| 49/52 [03:05<00:00,  4.33it/s][aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP3] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP2] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP1] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP0] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP7] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP6] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP5] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP4] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=4 avail_mem=26.74 GB):  96%|| 50/52 [03:05<00:00,  4.42it/s]Capturing batches (bs=2 avail_mem=26.74 GB):  96%|| 50/52 [03:05<00:00,  4.42it/s][aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP2] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP3] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP0] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP7] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP4] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP1] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP6] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:21 TP5] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=2 avail_mem=26.74 GB):  98%|| 51/52 [03:05<00:00,  4.49it/s]Capturing batches (bs=1 avail_mem=26.74 GB):  98%|| 51/52 [03:05<00:00,  4.49it/s][aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP3] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP2] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP0] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP1] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP6] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP4] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP7] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:01:22 TP5] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=1 avail_mem=26.74 GB): 100%|| 52/52 [03:06<00:00,  3.02it/s]Capturing batches (bs=1 avail_mem=26.74 GB): 100%|| 52/52 [03:06<00:00,  3.58s/it]
[2025-10-13 10:01:23 TP3] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP7] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP0] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP2] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP1] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP5] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP4] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP6] Registering 6708 cuda graph addresses
[2025-10-13 10:01:23 TP0] Capture cuda graph end. Time elapsed: 187.52 s. mem usage=1.21 GB. avail mem=26.73 GB.
[2025-10-13 10:01:23 TP0] max_total_num_tokens=4633795, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=8192, available_gpu_mem=26.73 GB
[2025-10-13 10:01:24] INFO:     Started server process [67251]
[2025-10-13 10:01:24] INFO:     Waiting for application startup.
[2025-10-13 10:01:24] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 10:01:24] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 10:01:24] INFO:     Application startup complete.
[2025-10-13 10:01:24] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-13 10:01:25] INFO:     127.0.0.1:58946 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:01:25 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[aiter] start build [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/build/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
[2025-10-13 10:01:26 TP6] start build [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/build/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout], cost 52.65179494s
[2025-10-13 10:02:18 TP6] finish build [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout], cost 52.65179494s
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:18 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:18 TP6] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP5] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP4] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP7] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP2] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP0] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP3] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-13 10:02:19 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19 TP1] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:19] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:19] The server is fired up and ready to roll!
[2025-10-13 10:02:27] INFO:     127.0.0.1:53670 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:02:27 TP0] Prefill batch. #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP6] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP5] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP4] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP2] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP0] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP1] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP3] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP7] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:28 TP0] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 796, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP2] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP0] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP3] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP4] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP5] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP7] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP1] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP6] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP0] Prefill batch. #new-seq: 34, #new-token: 2146, #cached-token: 27064, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP0] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP2] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP3] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP4] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP1] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP5] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP7] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP6] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:28 TP0] Prefill batch. #new-seq: 48, #new-token: 3094, #cached-token: 38304, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-13 10:02:29 TP0] Prefill batch. #new-seq: 83, #new-token: 5247, #cached-token: 66252, token usage: 0.00, #running-req: 83, #queue-req: 0, 
[2025-10-13 10:02:29 TP0] Prefill batch. #new-seq: 102, #new-token: 6258, #cached-token: 81438, token usage: 0.00, #running-req: 166, #queue-req: 0, 
[2025-10-13 10:02:29 TP0] Prefill batch. #new-seq: 180, #new-token: 11426, #cached-token: 143726, token usage: 0.00, #running-req: 268, #queue-req: 0, 
[2025-10-13 10:02:35 TP0] Prefill batch. #new-seq: 271, #new-token: 16365, #cached-token: 216419, token usage: 0.01, #running-req: 448, #queue-req: 600, 
[2025-10-13 10:02:36 TP0] Prefill batch. #new-seq: 259, #new-token: 16375, #cached-token: 206867, token usage: 0.01, #running-req: 719, #queue-req: 341, 
[2025-10-13 10:02:37 TP0] Prefill batch. #new-seq: 256, #new-token: 16329, #cached-token: 204506, token usage: 0.01, #running-req: 978, #queue-req: 85, 
[2025-10-13 10:02:38 TP0] Prefill batch. #new-seq: 85, #new-token: 5480, #cached-token: 67906, token usage: 0.02, #running-req: 1234, #queue-req: 0, 
[2025-10-13 10:02:42 TP0] Decode batch. #running-req: 1319, #token: 126787, token usage: 0.03, cuda graph: False, gen throughput (token/s): 535.42, #queue-req: 0, 
[2025-10-13 10:02:43] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:43] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:44] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:45] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:46] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47 TP0] Decode batch. #running-req: 1256, #token: 170910, token usage: 0.04, cuda graph: False, gen throughput (token/s): 10784.27, #queue-req: 0, 
[2025-10-13 10:02:47] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:47] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:48] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:49] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (1023, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (994, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (979, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (970, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:34286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:50] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP4] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP0] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP6] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP5] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP2] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP1] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP7] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:50 TP3] [fused_moe] using default for (958, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (915, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (890, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:59202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (880, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:51] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP4] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP0] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP3] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP7] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP2] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP6] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP1] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:51 TP5] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (865, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] Decode batch. #running-req: 871, #token: 156175, token usage: 0.03, cuda graph: False, gen throughput (token/s): 9205.75, #queue-req: 0, 
[2025-10-13 10:02:52] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (854, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (836, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (824, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (817, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (806, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (795, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (786, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:52] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP2] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP0] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP3] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP7] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP1] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP4] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP6] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:52 TP5] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (743, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (696, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP3] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP0] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP2] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP1] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP4] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP6] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP7] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53 TP5] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:53] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:53] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (648, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:35792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (631, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36798 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (610, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (603, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (598, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (590, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:54] INFO:     127.0.0.1:36664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP5] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP4] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP1] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP3] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP7] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP0] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP6] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:54 TP2] [fused_moe] using default for (580, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (537, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP1] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP3] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP2] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP6] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP7] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP5] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP0] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55 TP4] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:02:55] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55 TP0] Decode batch. #running-req: 504, #token: 110994, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7443.22, #queue-req: 0, 
[2025-10-13 10:02:55] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:37076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:36306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:55] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:56] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57 TP0] Decode batch. #running-req: 244, #token: 65078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 8617.92, #queue-req: 0, 
[2025-10-13 10:02:57] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:57] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58 TP0] Decode batch. #running-req: 95, #token: 30202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 5776.57, #queue-req: 0, 
[2025-10-13 10:02:58] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:58] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59 TP0] Decode batch. #running-req: 35, #token: 13411, token usage: 0.00, cuda graph: True, gen throughput (token/s): 3146.55, #queue-req: 0, 
[2025-10-13 10:02:59] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:02:59] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00 TP0] Decode batch. #running-req: 21, #token: 9140, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1206.65, #queue-req: 0, 
[2025-10-13 10:03:00] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:33980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:00] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01 TP0] Decode batch. #running-req: 14, #token: 6837, token usage: 0.00, cuda graph: True, gen throughput (token/s): 817.39, #queue-req: 0, 
[2025-10-13 10:03:01] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01] INFO:     127.0.0.1:33926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:01 TP0] Decode batch. #running-req: 8, #token: 4463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 616.77, #queue-req: 0, 
[2025-10-13 10:03:02 TP0] Decode batch. #running-req: 8, #token: 4783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 519.75, #queue-req: 0, 
[2025-10-13 10:03:03 TP0] Decode batch. #running-req: 8, #token: 5103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 505.59, #queue-req: 0, 
[2025-10-13 10:03:03] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03] INFO:     127.0.0.1:36706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:03:03 TP0] Decode batch. #running-req: 8, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 509.20, #queue-req: 0, 
[2025-10-13 10:03:16] INFO:     127.0.0.1:58238 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:03:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[aiter] start build [mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/build/mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
[2025-10-13 10:03:16 TP3] start build [mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/build/mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout], cost 53.20736786s
[2025-10-13 10:04:09 TP3] finish build [mha_batch_prefill_bf16_logits_nbias_nmask_nlse_ndropout], cost 53.20736786s
[2025-10-13 10:04:09] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:04:09 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 19755, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 46247, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (54, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55918, token usage: 0.00, #running-req: 78, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54504, token usage: 0.00, #running-req: 143, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 73, #new-token: 73, #cached-token: 62592, token usage: 0.00, #running-req: 206, #queue-req: 0, 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (73, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 77, #new-token: 77, #cached-token: 66170, token usage: 0.00, #running-req: 279, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (77, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 56003, token usage: 0.01, #running-req: 356, #queue-req: 0, 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55785, token usage: 0.01, #running-req: 421, #queue-req: 0, 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 59125, token usage: 0.01, #running-req: 486, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 64486, token usage: 0.01, #running-req: 555, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP0] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP4] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP5] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP3] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP1] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP2] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP6] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:10 TP7] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56642, token usage: 0.01, #running-req: 630, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 80, #new-token: 80, #cached-token: 68782, token usage: 0.01, #running-req: 696, #queue-req: 0, 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 56234, token usage: 0.01, #running-req: 776, #queue-req: 0, 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 43563, token usage: 0.01, #running-req: 841, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (51, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2579, token usage: 0.01, #running-req: 892, #queue-req: 0, 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (3, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48261, token usage: 0.01, #running-req: 895, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 51153, token usage: 0.01, #running-req: 951, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53391, token usage: 0.01, #running-req: 1010, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP0] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP3] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP7] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP4] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP1] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP5] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP6] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:11 TP2] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54159, token usage: 0.02, #running-req: 1072, #queue-req: 0, 
[2025-10-13 10:04:12 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 54996, token usage: 0.02, #running-req: 1135, #queue-req: 0, 
[2025-10-13 10:04:12 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53558, token usage: 0.02, #running-req: 1199, #queue-req: 0, 
[2025-10-13 10:04:12 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 49981, token usage: 0.02, #running-req: 1261, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP0] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP4] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP6] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP2] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP3] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP5] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP7] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:12 TP1] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:16 TP0] Decode batch. #running-req: 1319, #token: 136485, token usage: 0.03, cuda graph: False, gen throughput (token/s): 710.01, #queue-req: 0, 
[2025-10-13 10:04:17] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:17] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:18] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:44062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:19] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:20] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21 TP0] Decode batch. #running-req: 1205, #token: 172426, token usage: 0.04, cuda graph: False, gen throughput (token/s): 11262.11, #queue-req: 0, 
[2025-10-13 10:04:21] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:21] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:22] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (1017, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (1006, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:23] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP3] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP7] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP6] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP2] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP0] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP4] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP1] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:23 TP5] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (947, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (924, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (910, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (867, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:24] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP2] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP1] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP0] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP3] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP4] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP5] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP7] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:24 TP6] [fused_moe] using default for (853, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:44526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (844, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (803, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] Decode batch. #running-req: 796, #token: 146818, token usage: 0.03, cuda graph: False, gen throughput (token/s): 8840.97, #queue-req: 0, 
[2025-10-13 10:04:25] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP2] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP3] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP0] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP4] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP7] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP6] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP5] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25 TP1] [fused_moe] using default for (767, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:25] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:25] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (698, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (677, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:26] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP4] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP6] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP2] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP3] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP0] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP7] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP1] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:26 TP5] [fused_moe] using default for (671, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (658, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (638, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (629, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:44200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:46626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (618, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (588, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:27] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP1] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP5] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP7] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP3] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP4] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP0] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP6] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:27 TP2] [fused_moe] using default for (553, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP1] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP3] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP5] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP4] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP7] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP0] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP2] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP6] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP4] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP6] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP2] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP1] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP5] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP0] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP3] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP7] [fused_moe] using default for (522, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP0] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP2] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP4] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP6] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP1] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP5] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP3] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP7] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP5] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP1] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP0] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP4] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP2] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP3] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP7] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28 TP6] [fused_moe] using default for (513, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:28] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28 TP0] Decode batch. #running-req: 429, #token: 99210, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7663.99, #queue-req: 0, 
[2025-10-13 10:04:28] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:28] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:29] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30 TP0] Decode batch. #running-req: 207, #token: 58016, token usage: 0.01, cuda graph: True, gen throughput (token/s): 8320.17, #queue-req: 0, 
[2025-10-13 10:04:30] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:48762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:30] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31 TP0] Decode batch. #running-req: 83, #token: 26029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 5285.98, #queue-req: 0, 
[2025-10-13 10:04:31] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:31] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32 TP0] Decode batch. #running-req: 31, #token: 12215, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2657.60, #queue-req: 0, 
[2025-10-13 10:04:32] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:32] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33 TP0] Decode batch. #running-req: 19, #token: 8568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1083.15, #queue-req: 0, 
[2025-10-13 10:04:33] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:51242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:33 TP0] Decode batch. #running-req: 10, #token: 5269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 762.77, #queue-req: 0, 
[2025-10-13 10:04:34] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:34] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:34] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:34] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:34 TP0] Decode batch. #running-req: 6, #token: 3702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 491.55, #queue-req: 0, 
[2025-10-13 10:04:35] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:35 TP0] Decode batch. #running-req: 5, #token: 3362, token usage: 0.00, cuda graph: True, gen throughput (token/s): 358.06, #queue-req: 0, 
[2025-10-13 10:04:35 TP0] Decode batch. #running-req: 5, #token: 3562, token usage: 0.00, cuda graph: True, gen throughput (token/s): 335.13, #queue-req: 0, 
[2025-10-13 10:04:36] INFO:     127.0.0.1:46858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:36] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:36] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:36] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:36] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:49] INFO:     127.0.0.1:59882 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:04:49] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 35, #new-token: 35, #cached-token: 30072, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP3] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP4] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP2] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP1] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP6] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP5] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP7] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 44694, token usage: 0.00, #running-req: 36, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP3] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP2] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP6] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP5] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP1] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP4] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP7] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 20, #new-token: 20, #cached-token: 17247, token usage: 0.00, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP4] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP2] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP6] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP5] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP3] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP1] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP7] [fused_moe] using default for (20, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54185, token usage: 0.00, #running-req: 108, #queue-req: 0, 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47413, token usage: 0.00, #running-req: 171, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP4] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP5] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP2] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP6] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP3] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP1] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP7] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53057, token usage: 0.00, #running-req: 226, #queue-req: 0, 
[2025-10-13 10:04:49 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53312, token usage: 0.00, #running-req: 288, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52463, token usage: 0.01, #running-req: 350, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP4] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP5] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP3] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP6] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP2] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP1] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP7] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52635, token usage: 0.01, #running-req: 411, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 53830, token usage: 0.01, #running-req: 472, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5889, token usage: 0.01, #running-req: 535, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP4] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP5] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP3] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP1] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP2] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP6] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP7] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP4] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP2] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP6] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP3] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP7] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP5] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP1] [fused_moe] using default for (542, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 19, #new-token: 19, #cached-token: 16376, token usage: 0.01, #running-req: 542, #queue-req: 0, 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP4] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP6] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP5] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP3] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP2] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP7] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP1] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47426, token usage: 0.01, #running-req: 561, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 48017, token usage: 0.01, #running-req: 616, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55946, token usage: 0.01, #running-req: 672, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52448, token usage: 0.01, #running-req: 737, #queue-req: 0, 
[2025-10-13 10:04:50 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52572, token usage: 0.01, #running-req: 798, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53014, token usage: 0.01, #running-req: 859, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53695, token usage: 0.01, #running-req: 921, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52931, token usage: 0.01, #running-req: 983, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 64355, token usage: 0.02, #running-req: 1044, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 55022, token usage: 0.02, #running-req: 1119, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52543, token usage: 0.02, #running-req: 1183, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54518, token usage: 0.02, #running-req: 1244, #queue-req: 0, 
[2025-10-13 10:04:51 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 10224, token usage: 0.02, #running-req: 1307, #queue-req: 0, 
[2025-10-13 10:04:52 TP0] Decode batch. #running-req: 1319, #token: 93577, token usage: 0.02, cuda graph: False, gen throughput (token/s): 547.36, #queue-req: 0, 
[2025-10-13 10:04:56] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:56] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:56] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:56] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57 TP0] Decode batch. #running-req: 1315, #token: 145740, token usage: 0.03, cuda graph: False, gen throughput (token/s): 11482.09, #queue-req: 0, 
[2025-10-13 10:04:57] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:57] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:41686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:58] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:34476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:41694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:04:59] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:40604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:00] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01 TP0] Decode batch. #running-req: 1121, #token: 169792, token usage: 0.04, cuda graph: False, gen throughput (token/s): 10735.96, #queue-req: 0, 
[2025-10-13 10:05:01] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:36418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:38824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:01] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:40424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP4] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP5] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP1] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP0] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP3] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP7] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP2] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP6] [fused_moe] using default for (1019, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP2] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP3] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP1] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP7] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP5] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP6] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP4] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP0] [fused_moe] using default for (1008, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:02] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP1] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP4] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP0] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP2] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP5] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:02 TP6] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (1000, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (988, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (948, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (932, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP4] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP1] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP5] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP3] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP7] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP0] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP2] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03 TP6] [fused_moe] using default for (923, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:03] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:03] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (913, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:42166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:42652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (899, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:42004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (857, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:04] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP4] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP1] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP5] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP0] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP2] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP6] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP3] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:04 TP7] [fused_moe] using default for (819, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (804, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (757, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:41152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:05] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP2] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP6] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP1] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP5] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP0] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP4] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP3] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:05 TP7] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (715, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] Decode batch. #running-req: 726, #token: 140925, token usage: 0.03, cuda graph: False, gen throughput (token/s): 8267.45, #queue-req: 0, 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (706, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (679, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (637, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:06] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP2] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP4] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP6] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP0] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP5] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP1] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP3] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:06 TP7] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (616, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (578, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (549, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:07] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP4] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP3] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP7] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP1] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP5] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP0] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP6] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:07 TP2] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-10-13 10:05:08] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08 TP0] Decode batch. #running-req: 368, #token: 87354, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7928.40, #queue-req: 0, 
[2025-10-13 10:05:08] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:08] INFO:     127.0.0.1:39880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:41652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:35796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:09] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:36872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10 TP0] Decode batch. #running-req: 172, #token: 48876, token usage: 0.01, cuda graph: True, gen throughput (token/s): 7788.60, #queue-req: 0, 
[2025-10-13 10:05:10] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:36690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:32906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:36098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:41248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:10] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11 TP0] Decode batch. #running-req: 73, #token: 24624, token usage: 0.01, cuda graph: True, gen throughput (token/s): 4544.04, #queue-req: 0, 
[2025-10-13 10:05:11] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:39456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:34594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:11] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12 TP0] Decode batch. #running-req: 32, #token: 12411, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2512.61, #queue-req: 0, 
[2025-10-13 10:05:12] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:12 TP0] Decode batch. #running-req: 13, #token: 6115, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1058.11, #queue-req: 0, 
[2025-10-13 10:05:12] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:13] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:13 TP0] Decode batch. #running-req: 11, #token: 5701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 607.82, #queue-req: 0, 
[2025-10-13 10:05:13] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:14] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:14 TP0] Decode batch. #running-req: 9, #token: 5160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 542.32, #queue-req: 0, 
[2025-10-13 10:05:14] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:15 TP0] Decode batch. #running-req: 8, #token: 4933, token usage: 0.00, cuda graph: True, gen throughput (token/s): 514.31, #queue-req: 0, 
[2025-10-13 10:05:15 TP0] Decode batch. #running-req: 8, #token: 5253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 516.97, #queue-req: 0, 
[2025-10-13 10:05:15] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:16] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 10:05:22] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 10:05:25] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
