/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:45 __init__.py:179] Automatically detected platform rocm.
WARNING 10-06 09:21:45 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
WARNING:sglang.srt.server_args:[33m
########################################################################
# For contributors and developers:                                    #
# Please move environment variable definitions to sglang.srt.environ  #
# using the following pattern:                                        #
#     SGLANG_XXX = EnvBool(False)                                     #
#                                                                     #
########################################################################
[0m
[2025-10-06 09:21:46] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/lmzheng-grok-1', tokenizer_path='/mnt/raid/models/huggingface/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=945385329, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/lmzheng-grok-1', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-06 09:21:47] No chat template found, defaulting to 'string' content format
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:55 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:55 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:55 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:55 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:55 __init__.py:179] Automatically detected platform rocm.
INFO 10-06 09:21:56 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:56 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-06 09:21:56 __init__.py:179] Automatically detected platform rocm.
INFO 10-06 09:21:56 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-06 09:21:57 TP0] Process 82262 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-06 09:21:57 TP3] Process 82265 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-06 09:21:58 TP5] Process 82267 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-06 09:21:58 TP7] Process 82269 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-06 09:21:58 TP2] Process 82264 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-06 09:21:58 TP0] Init torch distributed begin.
[2025-10-06 09:21:58 TP6] Process 82268 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-10-06 09:21:58 TP1] Process 82263 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-06 09:21:58 TP4] Process 82266 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-06 09:21:58 TP0] sglang is using nccl==2.21.5
[2025-10-06 09:22:00 TP0] Init torch distributed ends. mem usage=2.09 GB
[2025-10-06 09:22:00 TP6] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP6] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP1] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP1] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP4] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP4] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP3] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP3] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP7] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP7] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP5] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP5] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP2] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP2] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP0] Ignore import error when loading sglang.srt.models.kimi_vl: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP0] Ignore import error when loading sglang.srt.models.kimi_vl_moonvit: cannot import name 'GELUTanh' from 'transformers.activations' (/usr/local/lib/python3.12/dist-packages/transformers/activations.py)
[2025-10-06 09:22:00 TP0] Load weight begin. avail mem=189.38 GB
[2025-10-06 09:22:01 TP0] #parameters (analytical): 316.49 B, #parameters (actual): 316.58 B
Loading pt checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
Loading pt checkpoint shards:   4% Completed | 1/25 [00:03<01:17,  3.25s/it]
Loading pt checkpoint shards:   8% Completed | 2/25 [00:06<01:17,  3.36s/it]
Loading pt checkpoint shards:  12% Completed | 3/25 [00:09<01:09,  3.14s/it]
Loading pt checkpoint shards:  16% Completed | 4/25 [00:12<01:07,  3.20s/it]
Loading pt checkpoint shards:  20% Completed | 5/25 [00:15<00:59,  2.99s/it]
Loading pt checkpoint shards:  24% Completed | 6/25 [00:18<00:55,  2.94s/it]
Loading pt checkpoint shards:  28% Completed | 7/25 [00:20<00:50,  2.81s/it]
Loading pt checkpoint shards:  32% Completed | 8/25 [00:23<00:47,  2.79s/it]
Loading pt checkpoint shards:  36% Completed | 9/25 [00:26<00:44,  2.78s/it]
Loading pt checkpoint shards:  40% Completed | 10/25 [00:29<00:41,  2.76s/it]
Loading pt checkpoint shards:  44% Completed | 11/25 [00:31<00:39,  2.79s/it]
Loading pt checkpoint shards:  52% Completed | 13/25 [00:34<00:25,  2.16s/it]
Loading pt checkpoint shards:  56% Completed | 14/25 [00:37<00:25,  2.28s/it]
Loading pt checkpoint shards:  60% Completed | 15/25 [00:40<00:24,  2.44s/it]
Loading pt checkpoint shards:  64% Completed | 16/25 [00:43<00:23,  2.67s/it]
Loading pt checkpoint shards:  68% Completed | 17/25 [00:46<00:21,  2.71s/it]
Loading pt checkpoint shards:  72% Completed | 18/25 [00:49<00:18,  2.71s/it]
Loading pt checkpoint shards:  76% Completed | 19/25 [00:52<00:17,  2.92s/it]
Loading pt checkpoint shards:  80% Completed | 20/25 [01:00<00:21,  4.32s/it]
Loading pt checkpoint shards:  84% Completed | 21/25 [01:03<00:15,  3.96s/it]
Loading pt checkpoint shards:  88% Completed | 22/25 [01:06<00:11,  3.76s/it]
Loading pt checkpoint shards:  92% Completed | 23/25 [01:09<00:07,  3.52s/it]
Loading pt checkpoint shards:  96% Completed | 24/25 [01:13<00:03,  3.54s/it]
[2025-10-06 09:23:15 TP5] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[2025-10-06 09:23:15 TP6] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:15 TP5] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:15 TP6] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:15 TP7] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:15 TP7] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:16 TP4] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:16 TP4] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards: 100% Completed | 25/25 [01:16<00:00,  3.43s/it]
Loading pt checkpoint shards: 100% Completed | 25/25 [01:16<00:00,  3.05s/it]

[2025-10-06 09:23:17 TP0] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:17 TP0] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:17 TP3] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[2025-10-06 09:23:17 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=151.94 GB, mem usage=37.45 GB.
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:17 TP3] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:18 TP1] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:18 TP1] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:19 TP2] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:19 TP2] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-06 09:23:19 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-06 09:23:19 TP6] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP4] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP5] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP7] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP3] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP1] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP0] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:19 TP0] Memory pool end. avail mem=52.16 GB
[2025-10-06 09:23:19 TP2] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-06 09:23:22 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=50.43 GB
[2025-10-06 09:23:22 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.10 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP4] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[2025-10-06 09:23:26 TP6] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP7] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP5] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP3] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP1] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP2] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
hipcc -fPIC -DUSE_ROCM -DENABLE_FP8 -O3 -std=c++17 -DLEGACY_HIPBLAS_DIRECT -DUSE_PROF_API=1 -D__HIP_PLATFORM_HCC__=1 -D__HIP_PLATFORM_AMD__=1 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -mllvm --amdgpu-kernarg-preload-count=16 -Wno-unused-result -Wno-switch-bool -Wno-vla-cxx-extension -Wno-undefined-func-template -fgpu-flush-denormals-to-zero -mllvm --lsr-drop-solution=1 -fno-offload-uniform-block -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -amdgpu-coerce-illegal-types=1 --offload-arch=gfx942 -I/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include -c pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp -o pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-06 09:23:26 TP0] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for gfx942.
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for host.
hipcc -shared pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o -o lib.so
[aiter] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.35860590s
[2025-10-06 09:23:29 TP4] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.35860590s
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP4] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP4] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP6] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP6] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP7] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP7] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP5] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP5] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP2] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP0] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP3] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:23:29 TP1] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-06 09:23:29 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP0] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP3] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP1] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-06 09:23:29 TP2] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-06 09:23:29 TP4] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
clang (LLVM option parsing): Unknown command line argument '--amdgpu-use-amdgpu-trackers=1'.  Try: 'clang (LLVM option parsing) --help'
clang (LLVM option parsing): Did you mean '--amdgpu-use-aa-in-codegen=1'?
failed to execute:/opt/rocm/lib/llvm/bin/clang++  --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 -O3  -mllvm --amdgpu-use-amdgpu-trackers=1 -x hip -c /dev/null -o "/dev/null"
[aiter] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[2025-10-06 09:23:39 TP4] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 182.95067295s
[2025-10-06 09:26:32 TP4] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 182.95067295s
Capturing batches (bs=512 avail_mem=50.10 GB):   2%|▏         | 1/52 [03:12<2:43:49, 192.74s/it]Capturing batches (bs=496 avail_mem=49.27 GB):   2%|▏         | 1/52 [03:12<2:43:49, 192.74s/it][aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP5] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP0] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP1] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP2] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP4] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP6] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP3] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP7] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=496 avail_mem=49.27 GB):   4%|▍         | 2/52 [03:13<1:06:27, 79.75s/it] Capturing batches (bs=480 avail_mem=49.27 GB):   4%|▍         | 2/52 [03:13<1:06:27, 79.75s/it][aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP5] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP4] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP7] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP6] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP0] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP1] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP3] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP2] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=480 avail_mem=49.27 GB):   6%|▌         | 3/52 [03:13<35:29, 43.47s/it]  Capturing batches (bs=464 avail_mem=49.26 GB):   6%|▌         | 3/52 [03:13<35:29, 43.47s/it][aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP5] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP7] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP6] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP4] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP0] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP1] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP2] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:36 TP3] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=464 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:13<21:06, 26.38s/it]Capturing batches (bs=448 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:13<21:06, 26.38s/it][aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP5] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP7] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP1] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP2] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP3] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP4] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP6] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP0] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=448 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:14<13:16, 16.94s/it]Capturing batches (bs=432 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:14<13:16, 16.94s/it][aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP5] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP3] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP1] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP2] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP7] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP0] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP4] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP6] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=432 avail_mem=49.26 GB):  12%|█▏        | 6/52 [03:14<08:37, 11.25s/it]Capturing batches (bs=416 avail_mem=49.26 GB):  12%|█▏        | 6/52 [03:14<08:37, 11.25s/it][aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP1] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP3] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP5] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP2] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP7] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP0] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP6] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP4] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=416 avail_mem=49.26 GB):  13%|█▎        | 7/52 [03:14<05:43,  7.64s/it]Capturing batches (bs=400 avail_mem=49.26 GB):  13%|█▎        | 7/52 [03:14<05:43,  7.64s/it][aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP2] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP0] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP3] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP5] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP1] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP7] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP4] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP6] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=400 avail_mem=49.26 GB):  15%|█▌        | 8/52 [03:14<03:51,  5.27s/it]Capturing batches (bs=384 avail_mem=49.25 GB):  15%|█▌        | 8/52 [03:14<03:51,  5.27s/it][aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP0] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP3] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP2] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP5] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP1] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP7] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP6] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:37 TP4] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=384 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:14<02:38,  3.69s/it]Capturing batches (bs=368 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:14<02:38,  3.69s/it][aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP3] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP0] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP2] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP7] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP1] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP4] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP6] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP5] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=368 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:15<01:49,  2.61s/it]Capturing batches (bs=352 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:15<01:49,  2.61s/it][aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP3] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP1] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP2] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP5] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP4] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP6] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP7] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP0] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=352 avail_mem=49.25 GB):  21%|██        | 11/52 [03:15<01:16,  1.87s/it]Capturing batches (bs=336 avail_mem=49.25 GB):  21%|██        | 11/52 [03:15<01:16,  1.87s/it][aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP3] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP5] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP0] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP2] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP7] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP4] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP1] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP6] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=336 avail_mem=49.25 GB):  23%|██▎       | 12/52 [03:15<00:54,  1.36s/it]Capturing batches (bs=320 avail_mem=49.25 GB):  23%|██▎       | 12/52 [03:15<00:54,  1.36s/it][aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP3] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP2] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP5] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP7] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP1] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP0] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP4] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP6] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
Capturing batches (bs=320 avail_mem=49.25 GB):  25%|██▌       | 13/52 [03:15<00:39,  1.01s/it]Capturing batches (bs=304 avail_mem=49.24 GB):  25%|██▌       | 13/52 [03:15<00:39,  1.01s/it][aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP0] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP2] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP3] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP1] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP7] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP5] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP4] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:38 TP6] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=304 avail_mem=49.24 GB):  27%|██▋       | 14/52 [03:15<00:29,  1.30it/s]Capturing batches (bs=288 avail_mem=49.24 GB):  27%|██▋       | 14/52 [03:15<00:29,  1.30it/s][aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP5] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP3] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP2] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP0] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP1] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP7] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP4] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP6] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=288 avail_mem=49.24 GB):  29%|██▉       | 15/52 [03:16<00:22,  1.67it/s]Capturing batches (bs=272 avail_mem=49.24 GB):  29%|██▉       | 15/52 [03:16<00:22,  1.67it/s][aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP3] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP2] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP5] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP0] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP7] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP4] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP1] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP6] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=272 avail_mem=49.24 GB):  31%|███       | 16/52 [03:16<00:17,  2.09it/s]Capturing batches (bs=256 avail_mem=49.23 GB):  31%|███       | 16/52 [03:16<00:17,  2.09it/s][aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP2] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP3] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP5] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP0] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP1] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP7] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP6] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP4] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:16<00:13,  2.51it/s]Capturing batches (bs=248 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:16<00:13,  2.51it/s][aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP3] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP0] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP7] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP2] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP1] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP4] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP6] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP5] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=248 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:16<00:11,  2.95it/s]Capturing batches (bs=240 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:16<00:11,  2.95it/s][aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP3] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP1] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP5] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP2] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP7] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP0] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP4] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:39 TP6] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=240 avail_mem=49.23 GB):  37%|███▋      | 19/52 [03:16<00:09,  3.36it/s]Capturing batches (bs=232 avail_mem=49.23 GB):  37%|███▋      | 19/52 [03:16<00:09,  3.36it/s][aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP2] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP3] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP5] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP4] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP1] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP7] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP0] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP6] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=232 avail_mem=49.23 GB):  38%|███▊      | 20/52 [03:17<00:08,  3.73it/s]Capturing batches (bs=224 avail_mem=49.23 GB):  38%|███▊      | 20/52 [03:17<00:08,  3.73it/s][aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP3] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP5] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP2] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP0] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP7] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP1] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP4] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP6] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=224 avail_mem=49.23 GB):  40%|████      | 21/52 [03:17<00:07,  4.03it/s]Capturing batches (bs=216 avail_mem=49.22 GB):  40%|████      | 21/52 [03:17<00:07,  4.03it/s][aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP5] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP3] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP0] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP2] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP6] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP1] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP7] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP4] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=216 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:17<00:07,  4.24it/s]Capturing batches (bs=208 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:17<00:07,  4.24it/s][aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP3] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP5] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP0] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP1] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP7] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP2] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP4] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP6] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=208 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:17<00:06,  4.44it/s]Capturing batches (bs=200 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:17<00:06,  4.44it/s][aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP3] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP2] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP5] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP6] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP7] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP1] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP4] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:40 TP0] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=200 avail_mem=49.22 GB):  46%|████▌     | 24/52 [03:17<00:06,  4.52it/s]Capturing batches (bs=192 avail_mem=49.22 GB):  46%|████▌     | 24/52 [03:17<00:06,  4.52it/s][aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP3] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP5] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP0] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP1] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP7] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP6] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP2] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP4] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=192 avail_mem=49.22 GB):  48%|████▊     | 25/52 [03:18<00:05,  4.64it/s]Capturing batches (bs=184 avail_mem=49.22 GB):  48%|████▊     | 25/52 [03:18<00:05,  4.64it/s][aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP5] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP6] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP4] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP7] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP0] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP3] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP2] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP1] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=184 avail_mem=49.22 GB):  50%|█████     | 26/52 [03:18<00:05,  4.64it/s]Capturing batches (bs=176 avail_mem=49.21 GB):  50%|█████     | 26/52 [03:18<00:05,  4.64it/s][aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP5] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP0] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP4] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP2] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP3] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP6] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP1] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP7] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=176 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:18<00:05,  4.74it/s]Capturing batches (bs=168 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:18<00:05,  4.74it/s][aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP0] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP5] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP4] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP3] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP2] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP7] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP6] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP1] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=168 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:18<00:04,  4.80it/s]Capturing batches (bs=160 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:18<00:04,  4.80it/s][aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP5] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP0] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP3] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP2] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP1] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP6] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP4] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:41 TP7] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
Capturing batches (bs=160 avail_mem=49.21 GB):  56%|█████▌    | 29/52 [03:18<00:04,  4.84it/s]Capturing batches (bs=152 avail_mem=49.21 GB):  56%|█████▌    | 29/52 [03:18<00:04,  4.84it/s][aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP5] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP3] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP1] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP6] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP2] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP0] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP7] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP4] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=152 avail_mem=49.21 GB):  58%|█████▊    | 30/52 [03:19<00:04,  4.89it/s]Capturing batches (bs=144 avail_mem=49.20 GB):  58%|█████▊    | 30/52 [03:19<00:04,  4.89it/s][aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP2] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP3] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP1] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP0] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP5] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP6] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP4] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP7] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=144 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:19<00:04,  4.86it/s]Capturing batches (bs=136 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:19<00:04,  4.86it/s][aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP1] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP0] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP2] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP3] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP5] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP7] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP6] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP4] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=136 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:19<00:04,  4.82it/s]Capturing batches (bs=128 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:19<00:04,  4.82it/s][aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP3] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP0] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP1] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP2] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP5] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP7] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP4] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP6] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=49.20 GB):  63%|██████▎   | 33/52 [03:19<00:03,  4.88it/s]Capturing batches (bs=120 avail_mem=49.20 GB):  63%|██████▎   | 33/52 [03:19<00:03,  4.88it/s][aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP1] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP3] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP5] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP2] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP0] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP4] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP6] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:42 TP7] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=120 avail_mem=49.20 GB):  65%|██████▌   | 34/52 [03:19<00:03,  4.87it/s]Capturing batches (bs=112 avail_mem=49.20 GB):  65%|██████▌   | 34/52 [03:19<00:03,  4.87it/s][aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP3] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP7] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP1] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP2] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP0] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP5] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP4] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP6] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=112 avail_mem=49.20 GB):  67%|██████▋   | 35/52 [03:20<00:03,  4.90it/s]Capturing batches (bs=104 avail_mem=49.19 GB):  67%|██████▋   | 35/52 [03:20<00:03,  4.90it/s][aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP3] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP0] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP7] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP1] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP2] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP4] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP5] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP6] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=104 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:20<00:03,  4.90it/s]Capturing batches (bs=96 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:20<00:03,  4.90it/s] [aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP3] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP2] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP1] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP7] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP5] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP4] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP6] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP0] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=96 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:20<00:03,  4.88it/s]Capturing batches (bs=88 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:20<00:03,  4.88it/s][aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP3] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP5] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP1] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP2] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP7] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP4] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP6] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:43 TP0] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=88 avail_mem=49.19 GB):  73%|███████▎  | 38/52 [03:20<00:02,  4.93it/s]Capturing batches (bs=80 avail_mem=49.19 GB):  73%|███████▎  | 38/52 [03:20<00:02,  4.93it/s][aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP3] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP1] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP7] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP2] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP4] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP6] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP5] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP0] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=80 avail_mem=49.19 GB):  75%|███████▌  | 39/52 [03:21<00:02,  4.93it/s]Capturing batches (bs=72 avail_mem=49.18 GB):  75%|███████▌  | 39/52 [03:21<00:02,  4.93it/s][aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP5] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP7] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP3] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP2] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP0] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP4] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP6] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP1] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=72 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:21<00:02,  4.80it/s]Capturing batches (bs=64 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:21<00:02,  4.80it/s][aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP3] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP5] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP0] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP7] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP1] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP4] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP2] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP6] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=49.18 GB):  79%|███████▉  | 41/52 [03:21<00:02,  4.84it/s]Capturing batches (bs=56 avail_mem=49.18 GB):  79%|███████▉  | 41/52 [03:21<00:02,  4.84it/s][aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP3] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP5] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP1] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP4] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP2] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP7] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP0] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP6] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=56 avail_mem=49.18 GB):  81%|████████  | 42/52 [03:21<00:02,  4.81it/s]Capturing batches (bs=48 avail_mem=49.18 GB):  81%|████████  | 42/52 [03:21<00:02,  4.81it/s][aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP2] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP3] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP5] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP0] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP1] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP7] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP4] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:44 TP6] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=48 avail_mem=49.18 GB):  83%|████████▎ | 43/52 [03:21<00:01,  4.84it/s]Capturing batches (bs=40 avail_mem=49.17 GB):  83%|████████▎ | 43/52 [03:21<00:01,  4.84it/s][aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP3] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP2] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP1] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP5] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP7] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP0] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP6] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP4] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=40 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:22<00:01,  4.88it/s]Capturing batches (bs=32 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:22<00:01,  4.88it/s][aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP5] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP1] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP7] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP2] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP0] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP6] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP4] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP3] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:22<00:01,  4.82it/s]Capturing batches (bs=24 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:22<00:01,  4.82it/s][aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP3] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP5] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP2] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP4] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP7] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP1] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP6] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP0] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=24 avail_mem=49.17 GB):  88%|████████▊ | 46/52 [03:22<00:01,  4.87it/s]Capturing batches (bs=16 avail_mem=49.17 GB):  88%|████████▊ | 46/52 [03:22<00:01,  4.87it/s][aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP3] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP1] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP2] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP5] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP0] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP4] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP6] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP7] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
Capturing batches (bs=16 avail_mem=49.17 GB):  90%|█████████ | 47/52 [03:22<00:01,  4.87it/s]Capturing batches (bs=12 avail_mem=49.16 GB):  90%|█████████ | 47/52 [03:22<00:01,  4.87it/s][aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP5] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP3] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP1] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP2] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP7] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP4] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP0] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:45 TP6] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=12 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:22<00:00,  4.90it/s]Capturing batches (bs=8 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:22<00:00,  4.90it/s] [aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP3] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP5] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP2] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP1] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP4] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP7] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP6] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP0] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
Capturing batches (bs=8 avail_mem=49.16 GB):  94%|█████████▍| 49/52 [03:23<00:00,  4.91it/s]Capturing batches (bs=4 avail_mem=49.16 GB):  94%|█████████▍| 49/52 [03:23<00:00,  4.91it/s][aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP5] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP1] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP7] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP2] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP4] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP6] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP3] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP0] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=4 avail_mem=49.16 GB):  96%|█████████▌| 50/52 [03:23<00:00,  4.93it/s]Capturing batches (bs=2 avail_mem=49.16 GB):  96%|█████████▌| 50/52 [03:23<00:00,  4.93it/s][aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP3] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP5] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP7] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP6] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP2] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP1] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP0] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP4] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=2 avail_mem=49.16 GB):  98%|█████████▊| 51/52 [03:23<00:00,  4.94it/s]Capturing batches (bs=1 avail_mem=49.15 GB):  98%|█████████▊| 51/52 [03:23<00:00,  4.94it/s][aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP0] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP3] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP4] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP1] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP2] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP7] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP5] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:46 TP6] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:24<00:00,  3.31it/s]Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:24<00:00,  3.92s/it]
[2025-10-06 09:26:47 TP7] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP3] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP4] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP1] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP5] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP6] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP0] Registering 6708 cuda graph addresses
[2025-10-06 09:26:47 TP2] Registering 6708 cuda graph addresses
[2025-10-06 09:26:48 TP0] Capture cuda graph end. Time elapsed: 205.39 s. mem usage=1.29 GB. avail mem=49.15 GB.
[2025-10-06 09:26:48 TP0] max_total_num_tokens=3246295, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=49.15 GB
[2025-10-06 09:26:49] INFO:     Started server process [82032]
[2025-10-06 09:26:49] INFO:     Waiting for application startup.
[2025-10-06 09:26:49] INFO:     Application startup complete.
[2025-10-06 09:26:49] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-06 09:26:50] INFO:     127.0.0.1:42100 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-06 09:26:50 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP7] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP2] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP0] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP1] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP6] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP3] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP5] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-06 09:26:50 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:50 TP4] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:51] INFO:     127.0.0.1:42116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:26:51] The server is fired up and ready to roll!
[2025-10-06 09:26:57] INFO:     127.0.0.1:43322 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP6] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP4] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP2] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP5] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP3] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP0] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP1] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP7] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 5, #new-token: 316, #cached-token: 3980, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP1] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP4] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP6] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP3] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP0] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP2] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP5] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP7] [fused_moe] using default for (316, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 20, #new-token: 1222, #cached-token: 15963, token usage: 0.00, #running-req: 5, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP7] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP5] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP4] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP6] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP1] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP2] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP3] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP0] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 30, #new-token: 1744, #cached-token: 23944, token usage: 0.00, #running-req: 25, #queue-req: 0, 
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 88, #new-token: 5693, #cached-token: 70242, token usage: 0.00, #running-req: 55, #queue-req: 0, 
[2025-10-06 09:26:58 TP0] Prefill batch. #new-seq: 52, #new-token: 3371, #cached-token: 41516, token usage: 0.00, #running-req: 143, #queue-req: 0, 
[2025-10-06 09:26:59 TP0] Prefill batch. #new-seq: 173, #new-token: 10601, #cached-token: 138155, token usage: 0.00, #running-req: 195, #queue-req: 0, 
[2025-10-06 09:26:59 TP0] Prefill batch. #new-seq: 109, #new-token: 6926, #cached-token: 87020, token usage: 0.01, #running-req: 368, #queue-req: 0, 
[2025-10-06 09:26:59 TP0] Prefill batch. #new-seq: 269, #new-token: 16342, #cached-token: 214846, token usage: 0.01, #running-req: 477, #queue-req: 68, 
[2025-10-06 09:27:01 TP0] Prefill batch. #new-seq: 257, #new-token: 16307, #cached-token: 205275, token usage: 0.01, #running-req: 746, #queue-req: 316, 
[2025-10-06 09:27:02 TP0] Prefill batch. #new-seq: 257, #new-token: 16380, #cached-token: 205302, token usage: 0.02, #running-req: 1003, #queue-req: 59, 
[2025-10-06 09:27:07 TP0] Prefill batch. #new-seq: 59, #new-token: 3778, #cached-token: 47143, token usage: 0.02, #running-req: 1260, #queue-req: 0, 
[2025-10-06 09:27:11 TP0] Decode batch. #running-req: 1319, #token: 124154, token usage: 0.04, cuda graph: False, gen throughput (token/s): 1718.76, #queue-req: 0, 
[2025-10-06 09:27:11] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:12] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:43364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:44548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:13] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:43824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:45782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:14] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15 TP0] Decode batch. #running-req: 1259, #token: 168907, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12710.46, #queue-req: 0, 
[2025-10-06 09:27:15] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:15] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:43880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:16] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:17] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (1010, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (1001, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (989, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (986, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:45936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (962, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:18] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP4] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP6] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP2] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP3] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP5] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP1] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP0] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:18 TP7] [fused_moe] using default for (938, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:44062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (926, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:45842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (912, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] Decode batch. #running-req: 916, #token: 162534, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11821.70, #queue-req: 0, 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (893, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (879, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (874, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:44426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (863, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (838, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (828, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:19] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP5] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP1] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP3] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP2] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP4] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP7] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP6] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:19 TP0] [fused_moe] using default for (820, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (812, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (800, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (788, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (783, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (774, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (755, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (748, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (730, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (721, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:20] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP1] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP2] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP4] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP3] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP5] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP0] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP6] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:20 TP7] [fused_moe] using default for (711, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (704, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (680, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (649, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (632, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (623, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (614, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:21] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP4] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP3] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP2] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP6] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP0] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP7] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP5] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:21 TP1] [fused_moe] using default for (591, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (582, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (573, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:44016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (568, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (556, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (547, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] Decode batch. #running-req: 547, #token: 120438, token usage: 0.04, cuda graph: False, gen throughput (token/s): 8939.12, #queue-req: 0, 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (533, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (525, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP4] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP6] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP5] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP7] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP1] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP3] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP2] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22 TP0] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:22] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:44682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:22] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:23] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24 TP0] Decode batch. #running-req: 301, #token: 80890, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8779.51, #queue-req: 0, 
[2025-10-06 09:27:24] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:24] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:45518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25 TP0] Decode batch. #running-req: 239, #token: 74377, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7749.98, #queue-req: 0, 
[2025-10-06 09:27:25] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:25] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:26] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27 TP0] Decode batch. #running-req: 221, #token: 77285, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7228.78, #queue-req: 0, 
[2025-10-06 09:27:27] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:27] INFO:     127.0.0.1:44166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:28] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:28 TP0] Decode batch. #running-req: 213, #token: 83312, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6680.92, #queue-req: 0, 
[2025-10-06 09:27:29] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:29] INFO:     127.0.0.1:47812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:29 TP0] Decode batch. #running-req: 211, #token: 90959, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6682.62, #queue-req: 0, 
[2025-10-06 09:27:29] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:29] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:30] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:30 TP0] Decode batch. #running-req: 208, #token: 97954, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7136.20, #queue-req: 0, 
[2025-10-06 09:27:31] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:31] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:31 TP0] Decode batch. #running-req: 206, #token: 105238, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6966.98, #queue-req: 0, 
[2025-10-06 09:27:32] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:33 TP0] Decode batch. #running-req: 205, #token: 112970, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6781.72, #queue-req: 0, 
[2025-10-06 09:27:34 TP0] Decode batch. #running-req: 205, #token: 119431, token usage: 0.04, cuda graph: True, gen throughput (token/s): 6784.18, #queue-req: 0, 
[2025-10-06 09:27:34] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:46858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:34] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:43] INFO:     127.0.0.1:52984 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-06 09:27:44] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 4291, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 854, token usage: 0.00, #running-req: 5, #queue-req: 0, 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP6] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP7] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP4] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP5] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP1] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP2] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP3] [fused_moe] using default for (5, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 157, #new-token: 157, #cached-token: 135076, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP6] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP4] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP1] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP3] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP2] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP5] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP7] [fused_moe] using default for (157, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55894, token usage: 0.00, #running-req: 163, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP4] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP2] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP3] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP5] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP1] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP7] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP6] [fused_moe] using default for (65, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 81, #new-token: 81, #cached-token: 69412, token usage: 0.01, #running-req: 228, #queue-req: 0, 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP4] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP5] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP6] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP7] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP1] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP3] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP2] [fused_moe] using default for (81, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56900, token usage: 0.01, #running-req: 309, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP0] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP4] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP6] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP5] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP3] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP7] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP1] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:44 TP2] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57765, token usage: 0.01, #running-req: 375, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55753, token usage: 0.01, #running-req: 442, #queue-req: 0, 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 59008, token usage: 0.01, #running-req: 507, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 76, #new-token: 76, #cached-token: 65394, token usage: 0.01, #running-req: 576, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (76, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56672, token usage: 0.01, #running-req: 652, #queue-req: 0, 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55908, token usage: 0.02, #running-req: 718, #queue-req: 0, 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54411, token usage: 0.02, #running-req: 783, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 55513, token usage: 0.02, #running-req: 846, #queue-req: 0, 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54631, token usage: 0.02, #running-req: 911, #queue-req: 0, 
[2025-10-06 09:27:45 TP0] Prefill batch. #new-seq: 25, #new-token: 25, #cached-token: 21531, token usage: 0.02, #running-req: 974, #queue-req: 0, 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (25, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP4] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP5] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP6] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP0] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP2] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP7] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP3] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:45 TP1] [fused_moe] using default for (999, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 42, #new-token: 42, #cached-token: 36485, token usage: 0.02, #running-req: 999, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP6] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP4] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP2] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP5] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP7] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP1] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP3] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 49889, token usage: 0.02, #running-req: 1041, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP2] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP6] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP1] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP4] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP5] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP3] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP7] [fused_moe] using default for (58, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 50436, token usage: 0.02, #running-req: 1099, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP4] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP1] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP5] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP7] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP3] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP2] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP6] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53638, token usage: 0.02, #running-req: 1158, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP1] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP4] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP3] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP5] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP7] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP6] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP2] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54309, token usage: 0.03, #running-req: 1220, #queue-req: 0, 
[2025-10-06 09:27:46 TP0] Prefill batch. #new-seq: 36, #new-token: 36, #cached-token: 30977, token usage: 0.03, #running-req: 1283, #queue-req: 0, 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP0] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP6] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP2] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP4] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP5] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP1] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP7] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:46 TP3] [fused_moe] using default for (36, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:49] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:50 TP0] Decode batch. #running-req: 1318, #token: 132295, token usage: 0.04, cuda graph: False, gen throughput (token/s): 3045.78, #queue-req: 0, 
[2025-10-06 09:27:50] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:51] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:52] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:53] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54 TP0] Decode batch. #running-req: 1225, #token: 172072, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12749.23, #queue-req: 0, 
[2025-10-06 09:27:54] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:54] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:55] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (1022, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (1009, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (1002, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (995, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (981, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (973, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP5] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:56 TP1] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP4] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP2] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP0] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP6] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP3] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:56 TP7] [fused_moe] using default for (935, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (927, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (918, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (902, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (892, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (885, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (861, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] Decode batch. #running-req: 852, #token: 155138, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11654.51, #queue-req: 0, 
[2025-10-06 09:27:57] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:57] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP4] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP2] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP6] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP0] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP3] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP7] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP1] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:57 TP5] [fused_moe] using default for (811, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (789, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (780, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (766, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (760, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (752, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (742, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (739, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (729, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (723, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:58] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP2] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP4] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP6] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP0] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP1] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP3] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP5] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:58 TP7] [fused_moe] using default for (720, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (690, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (664, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (654, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (633, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (622, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (612, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (604, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:27:59] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP4] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP2] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP6] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP0] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP3] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP7] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP1] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:27:59 TP5] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (570, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (562, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (554, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (529, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (526, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP6] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP4] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP0] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP2] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP1] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP5] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP3] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00 TP7] [fused_moe] using default for (515, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:00] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00 TP0] Decode batch. #running-req: 503, #token: 113345, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8691.87, #queue-req: 0, 
[2025-10-06 09:28:00] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:00] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:01] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02 TP0] Decode batch. #running-req: 294, #token: 81116, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8802.05, #queue-req: 0, 
[2025-10-06 09:28:02] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:02] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:03 TP0] Decode batch. #running-req: 245, #token: 77645, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7594.11, #queue-req: 0, 
[2025-10-06 09:28:03] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:04] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05 TP0] Decode batch. #running-req: 228, #token: 81351, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7519.88, #queue-req: 0, 
[2025-10-06 09:28:05] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:05] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06 TP0] Decode batch. #running-req: 217, #token: 85917, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7063.49, #queue-req: 0, 
[2025-10-06 09:28:06] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:06] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:07 TP0] Decode batch. #running-req: 214, #token: 93690, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7040.60, #queue-req: 0, 
[2025-10-06 09:28:07] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:08 TP0] Decode batch. #running-req: 213, #token: 101771, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6923.66, #queue-req: 0, 
[2025-10-06 09:28:08] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:10 TP0] Decode batch. #running-req: 212, #token: 109744, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7044.56, #queue-req: 0, 
[2025-10-06 09:28:11 TP0] Decode batch. #running-req: 212, #token: 118224, token usage: 0.04, cuda graph: True, gen throughput (token/s): 7038.89, #queue-req: 0, 
[2025-10-06 09:28:12] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:12] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:21] INFO:     127.0.0.1:51430 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-06 09:28:21] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 4291, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 854, token usage: 0.00, #running-req: 5, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 81, #new-token: 81, #cached-token: 69521, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47378, token usage: 0.00, #running-req: 87, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP0] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP4] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP6] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP5] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP2] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP7] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP3] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP1] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 56118, token usage: 0.00, #running-req: 142, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 53893, token usage: 0.01, #running-req: 207, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 55075, token usage: 0.01, #running-req: 270, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54078, token usage: 0.01, #running-req: 334, #queue-req: 0, 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 61313, token usage: 0.01, #running-req: 397, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP0] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP2] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP3] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP4] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP6] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP7] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP5] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP1] [fused_moe] using default for (71, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:21 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56517, token usage: 0.01, #running-req: 468, #queue-req: 0, 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 78, #new-token: 78, #cached-token: 66976, token usage: 0.01, #running-req: 534, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (78, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 35, #new-token: 35, #cached-token: 29974, token usage: 0.01, #running-req: 612, #queue-req: 0, 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (647, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] Decode batch. #running-req: 647, #token: 42012, token usage: 0.01, cuda graph: False, gen throughput (token/s): 751.57, #queue-req: 0, 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 31, #new-token: 31, #cached-token: 26721, token usage: 0.01, #running-req: 647, #queue-req: 0, 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (31, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47269, token usage: 0.01, #running-req: 678, #queue-req: 0, 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49011, token usage: 0.02, #running-req: 733, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 50908, token usage: 0.02, #running-req: 790, #queue-req: 0, 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 49608, token usage: 0.02, #running-req: 849, #queue-req: 0, 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52804, token usage: 0.02, #running-req: 907, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP6] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP2] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP4] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP5] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP7] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP1] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP3] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:22 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52776, token usage: 0.02, #running-req: 968, #queue-req: 0, 
[2025-10-06 09:28:23 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53406, token usage: 0.02, #running-req: 1029, #queue-req: 0, 
[2025-10-06 09:28:23 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56473, token usage: 0.02, #running-req: 1091, #queue-req: 0, 
[2025-10-06 09:28:23 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53713, token usage: 0.02, #running-req: 1157, #queue-req: 0, 
[2025-10-06 09:28:23 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54229, token usage: 0.03, #running-req: 1219, #queue-req: 0, 
[2025-10-06 09:28:23 TP0] Prefill batch. #new-seq: 37, #new-token: 37, #cached-token: 31841, token usage: 0.03, #running-req: 1282, #queue-req: 0, 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP0] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP1] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP5] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP6] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP7] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP4] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP2] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:23 TP3] [fused_moe] using default for (37, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:26] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:27 TP0] Decode batch. #running-req: 1318, #token: 136511, token usage: 0.04, cuda graph: False, gen throughput (token/s): 10432.35, #queue-req: 0, 
[2025-10-06 09:28:27] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:27] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:28] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:29] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:30] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:60996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31 TP0] Decode batch. #running-req: 1197, #token: 172017, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12444.87, #queue-req: 0, 
[2025-10-06 09:28:31] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:31] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:32] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (984, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (934, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (928, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:33] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP1] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP5] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP6] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP2] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP3] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP7] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP4] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:33 TP0] [fused_moe] using default for (920, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (894, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (872, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (860, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (848, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (835, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34 TP0] Decode batch. #running-req: 826, #token: 152580, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11417.77, #queue-req: 0, 
[2025-10-06 09:28:34] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:34] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP5] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP6] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP7] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP2] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP4] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP0] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP1] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:34 TP3] [fused_moe] using default for (799, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (775, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (750, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (740, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (731, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (712, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP2] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP6] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP5] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35 TP0] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP4] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP1] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP7] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35 TP3] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:35] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:35] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (681, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (668, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (661, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (643, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (630, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (608, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (592, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP0] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP4] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP7] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP3] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP1] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP5] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP2] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36 TP6] [fused_moe] using default for (579, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:36] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:36] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (560, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (544, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (530, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP2] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP6] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP4] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP0] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP1] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP5] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP7] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37 TP3] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-06 09:28:37] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37 TP0] Decode batch. #running-req: 468, #token: 107921, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8519.29, #queue-req: 0, 
[2025-10-06 09:28:37] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:37] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:38] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39 TP0] Decode batch. #running-req: 290, #token: 79258, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8412.45, #queue-req: 0, 
[2025-10-06 09:28:39] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:39] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40 TP0] Decode batch. #running-req: 242, #token: 76985, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7969.49, #queue-req: 0, 
[2025-10-06 09:28:40] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:40] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:41] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42 TP0] Decode batch. #running-req: 223, #token: 79787, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7463.30, #queue-req: 0, 
[2025-10-06 09:28:42] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:42] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:43 TP0] Decode batch. #running-req: 216, #token: 86562, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7134.24, #queue-req: 0, 
[2025-10-06 09:28:43] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:44] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:44 TP0] Decode batch. #running-req: 214, #token: 94353, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7309.04, #queue-req: 0, 
[2025-10-06 09:28:45] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:45] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:45] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:45 TP0] Decode batch. #running-req: 211, #token: 101405, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6891.99, #queue-req: 0, 
[2025-10-06 09:28:47 TP0] Decode batch. #running-req: 211, #token: 109845, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6942.84, #queue-req: 0, 
[2025-10-06 09:28:48 TP0] Decode batch. #running-req: 211, #token: 118285, token usage: 0.04, cuda graph: True, gen throughput (token/s): 6997.11, #queue-req: 0, 
[2025-10-06 09:28:49] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:49] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-06 09:28:51] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-06 09:28:55] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
