/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:36 __init__.py:179] Automatically detected platform rocm.
WARNING 10-10 10:04:36 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-10 10:04:37] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/lmzheng-grok-1', tokenizer_path='/mnt/raid/models/huggingface/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=248937, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, crash_on_nan=False, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/lmzheng-grok-1', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-10 10:04:38] No chat template found, defaulting to 'string' content format
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:46 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
/usr/local/lib/python3.12/dist-packages/vllm/platforms/__init__.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 10-10 10:04:47 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-10 10:04:48 TP6] Process 83014 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-10 10:04:49 TP4] Process 83012 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-10 10:04:49 TP0] Process 83008 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-10 10:04:49 TP3] Process 83011 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-10 10:04:49 TP5] Process 83013 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-10 10:04:49 TP2] Process 83010 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-10 10:04:50 TP1] Process 83009 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-10 10:04:50 TP7] Process 83015 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`torch_dtype` is deprecated! Use `dtype` instead!
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-10-10 10:04:50 TP0] Init torch distributed begin.
[2025-10-10 10:04:51 TP0] sglang is using nccl==2.21.5
[2025-10-10 10:04:53 TP0] Init torch distributed ends. mem usage=2.09 GB
[2025-10-10 10:04:53 TP0] Load weight begin. avail mem=189.38 GB
[2025-10-10 10:04:53 TP0] #parameters (analytical): 316.49 B, #parameters (actual): 316.58 B
Loading pt checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
Loading pt checkpoint shards:   4% Completed | 1/25 [00:03<01:23,  3.48s/it]
Loading pt checkpoint shards:   8% Completed | 2/25 [00:06<01:17,  3.38s/it]
Loading pt checkpoint shards:  12% Completed | 3/25 [00:09<01:09,  3.17s/it]
Loading pt checkpoint shards:  16% Completed | 4/25 [00:12<01:06,  3.17s/it]
Loading pt checkpoint shards:  20% Completed | 5/25 [00:15<00:59,  3.00s/it]
Loading pt checkpoint shards:  24% Completed | 6/25 [00:18<00:57,  3.05s/it]
Loading pt checkpoint shards:  28% Completed | 7/25 [00:21<00:55,  3.06s/it]
Loading pt checkpoint shards:  32% Completed | 8/25 [00:25<00:53,  3.15s/it]
Loading pt checkpoint shards:  36% Completed | 9/25 [00:28<00:52,  3.27s/it]
Loading pt checkpoint shards:  40% Completed | 10/25 [00:31<00:48,  3.22s/it]
Loading pt checkpoint shards:  44% Completed | 11/25 [00:34<00:44,  3.19s/it]
Loading pt checkpoint shards:  52% Completed | 13/25 [00:38<00:29,  2.49s/it]
Loading pt checkpoint shards:  56% Completed | 14/25 [00:41<00:29,  2.68s/it]
Loading pt checkpoint shards:  60% Completed | 15/25 [00:44<00:28,  2.88s/it]
Loading pt checkpoint shards:  64% Completed | 16/25 [00:49<00:28,  3.20s/it]
Loading pt checkpoint shards:  68% Completed | 17/25 [00:52<00:25,  3.24s/it]
Loading pt checkpoint shards:  72% Completed | 18/25 [00:55<00:22,  3.23s/it]
Loading pt checkpoint shards:  76% Completed | 19/25 [00:59<00:20,  3.42s/it]
Loading pt checkpoint shards:  80% Completed | 20/25 [01:07<00:23,  4.77s/it]
Loading pt checkpoint shards:  84% Completed | 21/25 [01:10<00:17,  4.32s/it]
Loading pt checkpoint shards:  88% Completed | 22/25 [01:14<00:12,  4.23s/it]
Loading pt checkpoint shards:  92% Completed | 23/25 [01:17<00:07,  3.78s/it]
[2025-10-10 10:06:13 TP5] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:13 TP5] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards:  96% Completed | 24/25 [01:20<00:03,  3.67s/it]
[2025-10-10 10:06:15 TP4] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:15 TP4] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:15 TP7] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:15 TP7] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:16 TP6] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:16 TP6] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP1] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP1] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP3] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP3] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
Loading pt checkpoint shards: 100% Completed | 25/25 [01:23<00:00,  3.33s/it]
Loading pt checkpoint shards: 100% Completed | 25/25 [01:23<00:00,  3.34s/it]

[2025-10-10 10:06:17 TP0] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP0] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP2] #all_names: 835, #hit_names: 579, #missing_exclude_scales: 0
[aiter] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP2] type hints mismatch, override to --> dynamic_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-10-10 10:06:17 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=151.94 GB, mem usage=37.45 GB.
[2025-10-10 10:06:17 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-10 10:06:17 TP0] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP5] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP6] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP3] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP0] Memory pool end. avail mem=52.16 GB
[2025-10-10 10:06:17 TP2] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP1] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP7] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:17 TP4] KV Cache is allocated. #tokens: 3246295, K size: 49.53 GB, V size: 49.53 GB
[2025-10-10 10:06:20 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=50.43 GB
[2025-10-10 10:06:20 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.10 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[2025-10-10 10:06:24 TP0] start build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:24 TP3] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:24 TP1] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:24 TP2] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:25 TP5] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:25 TP7] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:25 TP4] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
hipcc -fPIC -DUSE_ROCM -DENABLE_FP8 -O3 -std=c++17 -DLEGACY_HIPBLAS_DIRECT -DUSE_PROF_API=1 -D__HIP_PLATFORM_HCC__=1 -D__HIP_PLATFORM_AMD__=1 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -mllvm --amdgpu-kernarg-preload-count=16 -Wno-unused-result -Wno-switch-bool -Wno-vla-cxx-extension -Wno-undefined-func-template -fgpu-flush-denormals-to-zero -mllvm --lsr-drop-solution=1 -fno-offload-uniform-block -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -amdgpu-coerce-illegal-types=1 --offload-arch=gfx942 -I/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include -c pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp -o pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o
[aiter] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
[2025-10-10 10:06:25 TP6] waiting for baton release at /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/lock
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for gfx942.
In file included from pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.cpp:1:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_ragged.cuh:19:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_kernels.cuh:3:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/pa_common.cuh:9:
In file included from /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/quant_utils.cuh:19:
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:80:1: warning: non-void function does not return a value [-Wreturn-type]
   80 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:89:1: warning: non-void function does not return a value [-Wreturn-type]
   89 | }
      | ^
/root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd/include/vec_convert.h:98:1: warning: non-void function does not return a value [-Wreturn-type]
   98 | }
      | ^
3 warnings generated when compiling for host.
hipcc -shared pa_ragged_e2f7ef6b685e12a35ae341214cad68bd.o -o lib.so
[aiter] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.37861037s
[2025-10-10 10:06:27 TP0] finish build /root/.aiter/build/pa_ragged_e2f7ef6b685e12a35ae341214cad68bd, cost 2.37861037s
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP0] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP0] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP5] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP5] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP1] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP3] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP7] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP1] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP3] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP7] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP2] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP6] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:06:27 TP4] [fused_moe] using default for (512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-10 10:06:27 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3.co GetFunction: _ZN5aiter45fmoe_stage1_bf16_pertokenFp8_g1u1_128x128_pf3E Success
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP2] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP4] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-10-10 10:06:27 TP6] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:27 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[aiter] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
[2025-10-10 10:06:28 TP0] start build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/build/module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
clang (LLVM option parsing): Unknown command line argument '--amdgpu-use-amdgpu-trackers=1'.  Try: 'clang (LLVM option parsing) --help'
clang (LLVM option parsing): Did you mean '--amdgpu-use-aa-in-codegen=1'?
failed to execute:/opt/rocm/lib/llvm/bin/clang++  --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 --offload-arch=gfx942 -O3  -mllvm --amdgpu-use-amdgpu-trackers=1 -x hip -c /dev/null -o "/dev/null"
[aiter] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[2025-10-10 10:06:44 TP0] -mllvm --amdgpu-use-amdgpu-trackers=1 is not supported by hipcc.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 189.94785158s
[2025-10-10 10:09:37 TP0] finish build [module_moe_ck2stages_f8_f8_b16_gelu_per_token_mulWeightStage2], cost 189.94785158s
Capturing batches (bs=512 avail_mem=50.10 GB):   2%|▏         | 1/52 [03:20<2:50:00, 200.01s/it]Capturing batches (bs=496 avail_mem=49.27 GB):   2%|▏         | 1/52 [03:20<2:50:00, 200.01s/it][aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP6] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP5] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP7] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP2] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP0] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP1] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP4] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP3] [fused_moe] using default for (496, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=496 avail_mem=49.27 GB):   4%|▍         | 2/52 [03:20<1:08:53, 82.68s/it] Capturing batches (bs=480 avail_mem=49.27 GB):   4%|▍         | 2/52 [03:20<1:08:53, 82.68s/it][aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP4] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP7] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP5] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP6] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP0] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP2] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP3] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:41 TP1] [fused_moe] using default for (480, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=480 avail_mem=49.27 GB):   6%|▌         | 3/52 [03:20<36:46, 45.03s/it]  Capturing batches (bs=464 avail_mem=49.26 GB):   6%|▌         | 3/52 [03:20<36:46, 45.03s/it][aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP7] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP5] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP4] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP6] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP3] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP1] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP2] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP0] [fused_moe] using default for (464, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=464 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:21<21:52, 27.34s/it]Capturing batches (bs=448 avail_mem=49.26 GB):   8%|▊         | 4/52 [03:21<21:52, 27.34s/it][aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP7] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP6] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP5] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP4] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP1] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP2] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP0] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP3] [fused_moe] using default for (448, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=448 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:21<13:45, 17.56s/it]Capturing batches (bs=432 avail_mem=49.26 GB):  10%|▉         | 5/52 [03:21<13:45, 17.56s/it][aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP7] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP4] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP5] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP6] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP3] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP2] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP1] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP0] [fused_moe] using default for (432, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=432 avail_mem=49.26 GB):  12%|█▏        | 6/52 [03:21<08:56, 11.66s/it]Capturing batches (bs=416 avail_mem=49.26 GB):  12%|█▏        | 6/52 [03:21<08:56, 11.66s/it][aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP4] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP5] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP7] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP6] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP3] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP2] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP1] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP0] [fused_moe] using default for (416, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=416 avail_mem=49.26 GB):  13%|█▎        | 7/52 [03:21<05:55,  7.91s/it]Capturing batches (bs=400 avail_mem=49.26 GB):  13%|█▎        | 7/52 [03:21<05:55,  7.91s/it][aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP7] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP4] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP5] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP3] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP1] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP6] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP0] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:42 TP2] [fused_moe] using default for (400, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=400 avail_mem=49.26 GB):  15%|█▌        | 8/52 [03:21<04:00,  5.45s/it]Capturing batches (bs=384 avail_mem=49.25 GB):  15%|█▌        | 8/52 [03:21<04:00,  5.45s/it][aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP7] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP1] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP4] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP3] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP5] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP0] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP6] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP2] [fused_moe] using default for (384, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=384 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:22<02:43,  3.81s/it]Capturing batches (bs=368 avail_mem=49.25 GB):  17%|█▋        | 9/52 [03:22<02:43,  3.81s/it][aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP3] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP1] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP2] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP6] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP5] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP7] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP0] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP4] [fused_moe] using default for (368, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=368 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:22<01:53,  2.70s/it]Capturing batches (bs=352 avail_mem=49.25 GB):  19%|█▉        | 10/52 [03:22<01:53,  2.70s/it][aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP6] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP7] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP4] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP5] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP1] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP2] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP0] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP3] [fused_moe] using default for (352, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=352 avail_mem=49.25 GB):  21%|██        | 11/52 [03:22<01:19,  1.93s/it]Capturing batches (bs=336 avail_mem=49.25 GB):  21%|██        | 11/52 [03:22<01:19,  1.93s/it][aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP1] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP3] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP2] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP7] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP5] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP4] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP6] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP0] [fused_moe] using default for (336, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=336 avail_mem=49.25 GB):  23%|██▎       | 12/52 [03:22<00:56,  1.41s/it]Capturing batches (bs=320 avail_mem=49.25 GB):  23%|██▎       | 12/52 [03:22<00:56,  1.41s/it][aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP1] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP7] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP6] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP2] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP5] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP4] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP3] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:43 TP0] [fused_moe] using default for (320, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3.co GetFunction: _ZN5aiter44fmoe_stage1_bf16_pertokenFp8_g1u1_64x256_pf3E Success
Capturing batches (bs=320 avail_mem=49.25 GB):  25%|██▌       | 13/52 [03:22<00:40,  1.04s/it]Capturing batches (bs=304 avail_mem=49.24 GB):  25%|██▌       | 13/52 [03:22<00:40,  1.04s/it][aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP6] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP5] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP4] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP7] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP0] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP1] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP2] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP3] [fused_moe] using default for (304, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=304 avail_mem=49.24 GB):  27%|██▋       | 14/52 [03:23<00:30,  1.27it/s]Capturing batches (bs=288 avail_mem=49.24 GB):  27%|██▋       | 14/52 [03:23<00:30,  1.27it/s][aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP1] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP7] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP5] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP2] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP3] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP6] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP4] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP0] [fused_moe] using default for (288, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=288 avail_mem=49.24 GB):  29%|██▉       | 15/52 [03:23<00:22,  1.64it/s]Capturing batches (bs=272 avail_mem=49.24 GB):  29%|██▉       | 15/52 [03:23<00:22,  1.64it/s][aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP1] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP6] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP2] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP7] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP0] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP3] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP4] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP5] [fused_moe] using default for (272, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=272 avail_mem=49.24 GB):  31%|███       | 16/52 [03:23<00:17,  2.05it/s]Capturing batches (bs=256 avail_mem=49.23 GB):  31%|███       | 16/52 [03:23<00:17,  2.05it/s][aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP7] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP5] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP6] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP4] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP1] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP3] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP2] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP0] [fused_moe] using default for (256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:23<00:14,  2.48it/s]Capturing batches (bs=248 avail_mem=49.23 GB):  33%|███▎      | 17/52 [03:23<00:14,  2.48it/s][aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP5] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP1] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP6] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP7] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP4] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP0] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP2] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:44 TP3] [fused_moe] using default for (248, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=248 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:23<00:11,  2.93it/s]Capturing batches (bs=240 avail_mem=49.23 GB):  35%|███▍      | 18/52 [03:23<00:11,  2.93it/s][aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP3] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP7] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP6] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP1] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP5] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP2] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP4] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP0] [fused_moe] using default for (240, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=240 avail_mem=49.23 GB):  37%|███▋      | 19/52 [03:24<00:09,  3.32it/s]Capturing batches (bs=232 avail_mem=49.23 GB):  37%|███▋      | 19/52 [03:24<00:09,  3.32it/s][aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP7] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP5] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP6] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP4] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP1] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP3] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP0] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP2] [fused_moe] using default for (232, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=232 avail_mem=49.23 GB):  38%|███▊      | 20/52 [03:24<00:08,  3.71it/s]Capturing batches (bs=224 avail_mem=49.23 GB):  38%|███▊      | 20/52 [03:24<00:08,  3.71it/s][aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP7] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP5] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP3] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP1] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP6] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP2] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP0] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP4] [fused_moe] using default for (224, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=224 avail_mem=49.23 GB):  40%|████      | 21/52 [03:24<00:07,  4.03it/s]Capturing batches (bs=216 avail_mem=49.22 GB):  40%|████      | 21/52 [03:24<00:07,  4.03it/s][aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP1] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP7] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP5] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP3] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP6] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP2] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP4] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP0] [fused_moe] using default for (216, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=216 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:24<00:07,  4.28it/s]Capturing batches (bs=208 avail_mem=49.22 GB):  42%|████▏     | 22/52 [03:24<00:07,  4.28it/s][aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP1] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP7] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP3] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP4] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP2] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP6] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP0] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:45 TP5] [fused_moe] using default for (208, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=208 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:24<00:06,  4.48it/s]Capturing batches (bs=200 avail_mem=49.22 GB):  44%|████▍     | 23/52 [03:24<00:06,  4.48it/s][aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP7] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP1] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP6] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP5] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP4] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP2] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP3] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP0] [fused_moe] using default for (200, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=200 avail_mem=49.22 GB):  46%|████▌     | 24/52 [03:25<00:06,  4.55it/s]Capturing batches (bs=192 avail_mem=49.22 GB):  46%|████▌     | 24/52 [03:25<00:06,  4.55it/s][aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP1] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP6] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP7] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP2] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP4] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP0] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP5] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP3] [fused_moe] using default for (192, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=192 avail_mem=49.22 GB):  48%|████▊     | 25/52 [03:25<00:05,  4.62it/s]Capturing batches (bs=184 avail_mem=49.22 GB):  48%|████▊     | 25/52 [03:25<00:05,  4.62it/s][aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP1] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP6] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP7] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP3] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP4] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP2] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP5] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP0] [fused_moe] using default for (184, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=184 avail_mem=49.22 GB):  50%|█████     | 26/52 [03:25<00:05,  4.69it/s]Capturing batches (bs=176 avail_mem=49.21 GB):  50%|█████     | 26/52 [03:25<00:05,  4.69it/s][aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP1] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP7] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP5] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP0] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP2] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP4] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP6] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP3] [fused_moe] using default for (176, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=176 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:25<00:05,  4.79it/s]Capturing batches (bs=168 avail_mem=49.21 GB):  52%|█████▏    | 27/52 [03:25<00:05,  4.79it/s][aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP1] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP7] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP6] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP2] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP3] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP5] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP4] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:46 TP0] [fused_moe] using default for (168, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=168 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:25<00:04,  4.86it/s]Capturing batches (bs=160 avail_mem=49.21 GB):  54%|█████▍    | 28/52 [03:25<00:04,  4.86it/s][aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP7] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP1] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP3] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP6] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP5] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP0] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP4] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP2] [fused_moe] using default for (160, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x256_2tg_pf2E Success
Capturing batches (bs=160 avail_mem=49.21 GB):  56%|█████▌    | 29/52 [03:26<00:04,  4.89it/s]Capturing batches (bs=152 avail_mem=49.21 GB):  56%|█████▌    | 29/52 [03:26<00:04,  4.89it/s][aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP1] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP7] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP6] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP3] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP5] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP4] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP2] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP0] [fused_moe] using default for (152, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=152 avail_mem=49.21 GB):  58%|█████▊    | 30/52 [03:26<00:04,  4.94it/s]Capturing batches (bs=144 avail_mem=49.20 GB):  58%|█████▊    | 30/52 [03:26<00:04,  4.94it/s][aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP7] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP3] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP1] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP6] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP2] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP4] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP5] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP0] [fused_moe] using default for (144, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=144 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:26<00:04,  4.97it/s]Capturing batches (bs=136 avail_mem=49.20 GB):  60%|█████▉    | 31/52 [03:26<00:04,  4.97it/s][aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP1] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP7] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP3] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP4] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP6] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP5] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP2] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP0] [fused_moe] using default for (136, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=136 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:26<00:04,  5.00it/s]Capturing batches (bs=128 avail_mem=49.20 GB):  62%|██████▏   | 32/52 [03:26<00:04,  5.00it/s][aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP7] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP3] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP6] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP4] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP0] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP1] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP2] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:47 TP5] [fused_moe] using default for (128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=49.20 GB):  63%|██████▎   | 33/52 [03:26<00:03,  4.93it/s]Capturing batches (bs=120 avail_mem=49.20 GB):  63%|██████▎   | 33/52 [03:26<00:03,  4.93it/s][aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP3] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP6] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP7] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP2] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP0] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP5] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP4] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP1] [fused_moe] using default for (120, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=120 avail_mem=49.20 GB):  65%|██████▌   | 34/52 [03:27<00:03,  4.88it/s]Capturing batches (bs=112 avail_mem=49.20 GB):  65%|██████▌   | 34/52 [03:27<00:03,  4.88it/s][aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP1] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP0] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP7] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP4] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP2] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP3] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP6] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP5] [fused_moe] using default for (112, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=112 avail_mem=49.20 GB):  67%|██████▋   | 35/52 [03:27<00:03,  4.92it/s]Capturing batches (bs=104 avail_mem=49.19 GB):  67%|██████▋   | 35/52 [03:27<00:03,  4.92it/s][aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP7] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP1] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP3] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP6] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP4] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP2] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP0] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP5] [fused_moe] using default for (104, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=104 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:27<00:03,  4.94it/s]Capturing batches (bs=96 avail_mem=49.19 GB):  69%|██████▉   | 36/52 [03:27<00:03,  4.94it/s] [aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP7] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP3] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP6] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP4] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP0] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP5] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP2] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP1] [fused_moe] using default for (96, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=96 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:27<00:03,  4.97it/s]Capturing batches (bs=88 avail_mem=49.19 GB):  71%|███████   | 37/52 [03:27<00:03,  4.97it/s][aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP7] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP3] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP2] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP6] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP0] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP4] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP5] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:48 TP1] [fused_moe] using default for (88, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=88 avail_mem=49.19 GB):  73%|███████▎  | 38/52 [03:27<00:02,  4.99it/s]Capturing batches (bs=80 avail_mem=49.19 GB):  73%|███████▎  | 38/52 [03:27<00:02,  4.99it/s][aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP3] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP1] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP7] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP2] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP0] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP5] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP6] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP4] [fused_moe] using default for (80, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=80 avail_mem=49.19 GB):  75%|███████▌  | 39/52 [03:28<00:02,  5.00it/s]Capturing batches (bs=72 avail_mem=49.18 GB):  75%|███████▌  | 39/52 [03:28<00:02,  5.00it/s][aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP7] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP3] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP1] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP5] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP2] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP0] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP4] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP6] [fused_moe] using default for (72, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=72 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:28<00:02,  5.00it/s]Capturing batches (bs=64 avail_mem=49.18 GB):  77%|███████▋  | 40/52 [03:28<00:02,  5.00it/s][aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP7] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP1] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP5] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP6] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP2] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP3] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP0] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP4] [fused_moe] using default for (64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=49.18 GB):  79%|███████▉  | 41/52 [03:28<00:02,  4.98it/s]Capturing batches (bs=56 avail_mem=49.18 GB):  79%|███████▉  | 41/52 [03:28<00:02,  4.98it/s][aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP1] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP7] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP6] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP0] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP5] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP3] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP4] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP2] [fused_moe] using default for (56, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=56 avail_mem=49.18 GB):  81%|████████  | 42/52 [03:28<00:02,  4.92it/s]Capturing batches (bs=48 avail_mem=49.18 GB):  81%|████████  | 42/52 [03:28<00:02,  4.92it/s][aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP1] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP3] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP7] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP0] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP4] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP2] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP6] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:49 TP5] [fused_moe] using default for (48, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=48 avail_mem=49.18 GB):  83%|████████▎ | 43/52 [03:28<00:01,  4.85it/s]Capturing batches (bs=40 avail_mem=49.17 GB):  83%|████████▎ | 43/52 [03:28<00:01,  4.85it/s][aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP1] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP5] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP6] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP0] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP2] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP4] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP3] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP7] [fused_moe] using default for (40, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=40 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:29<00:01,  4.91it/s]Capturing batches (bs=32 avail_mem=49.17 GB):  85%|████████▍ | 44/52 [03:29<00:01,  4.91it/s][aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP7] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP3] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP1] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP5] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP2] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP6] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP0] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP4] [fused_moe] using default for (32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:29<00:01,  4.87it/s]Capturing batches (bs=24 avail_mem=49.17 GB):  87%|████████▋ | 45/52 [03:29<00:01,  4.87it/s][aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP1] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP5] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP0] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP2] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP3] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP4] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP6] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP7] [fused_moe] using default for (24, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=24 avail_mem=49.17 GB):  88%|████████▊ | 46/52 [03:29<00:01,  4.92it/s]Capturing batches (bs=16 avail_mem=49.17 GB):  88%|████████▊ | 46/52 [03:29<00:01,  4.92it/s][aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP3] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP1] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP7] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP2] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP6] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP0] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP4] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP5] [fused_moe] using default for (16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_64x128_2tg_pf2E Success
Capturing batches (bs=16 avail_mem=49.17 GB):  90%|█████████ | 47/52 [03:29<00:01,  4.95it/s]Capturing batches (bs=12 avail_mem=49.16 GB):  90%|█████████ | 47/52 [03:29<00:01,  4.95it/s][aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP7] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP1] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP3] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP6] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP2] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP0] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP5] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:50 TP4] [fused_moe] using default for (12, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=12 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:29<00:00,  4.98it/s]Capturing batches (bs=8 avail_mem=49.16 GB):  92%|█████████▏| 48/52 [03:29<00:00,  4.98it/s] [aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP1] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP3] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP5] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP7] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP2] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP0] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP4] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP6] [fused_moe] using default for (8, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe_2stages/fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2.co GetFunction: _ZN5aiter48fmoe_stage1_bf16_pertokenFp8_g1u1_32x128_3tg_pf2E Success
Capturing batches (bs=8 avail_mem=49.16 GB):  94%|█████████▍| 49/52 [03:30<00:00,  4.98it/s]Capturing batches (bs=4 avail_mem=49.16 GB):  94%|█████████▍| 49/52 [03:30<00:00,  4.98it/s][aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP3] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP5] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP2] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP1] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP4] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP7] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP6] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP0] [fused_moe] using default for (4, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=4 avail_mem=49.16 GB):  96%|█████████▌| 50/52 [03:30<00:00,  4.92it/s]Capturing batches (bs=2 avail_mem=49.16 GB):  96%|█████████▌| 50/52 [03:30<00:00,  4.92it/s][aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP6] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP5] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP0] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP3] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP7] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP1] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP2] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:51 TP4] [fused_moe] using default for (2, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=2 avail_mem=49.16 GB):  98%|█████████▊| 51/52 [03:30<00:00,  4.95it/s]Capturing batches (bs=1 avail_mem=49.15 GB):  98%|█████████▊| 51/52 [03:30<00:00,  4.95it/s][aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP6] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP1] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP0] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP4] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP3] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP7] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP2] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:52 TP5] [fused_moe] using default for (1, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:31<00:00,  3.32it/s]Capturing batches (bs=1 avail_mem=49.15 GB): 100%|██████████| 52/52 [03:31<00:00,  4.06s/it]
[2025-10-10 10:09:52 TP7] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP3] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP1] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP5] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP2] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP4] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP6] Registering 6708 cuda graph addresses
[2025-10-10 10:09:52 TP0] Registering 6708 cuda graph addresses
[2025-10-10 10:09:53 TP0] Capture cuda graph end. Time elapsed: 212.26 s. mem usage=1.29 GB. avail mem=49.15 GB.
[2025-10-10 10:09:53 TP0] max_total_num_tokens=3246295, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=49.15 GB
[2025-10-10 10:09:54] INFO:     Started server process [82778]
[2025-10-10 10:09:54] INFO:     Waiting for application startup.
[2025-10-10 10:09:54] INFO:     Application startup complete.
[2025-10-10 10:09:54] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-10 10:09:55] INFO:     127.0.0.1:46776 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-10 10:09:55 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP0] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP2] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP3] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP4] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP6] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP1] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:55 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:55 TP5] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-10-10 10:09:56 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:56 TP7] [fused_moe] using default for (6, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:09:56] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:09:56] The server is fired up and ready to roll!
[2025-10-10 10:10:03] INFO:     127.0.0.1:46794 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-10 10:10:03 TP0] Prefill batch. #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP3] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP0] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP6] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP5] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP2] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP7] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP1] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:03 TP4] [fused_moe] using default for (796, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 796, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP5] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP1] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP3] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP2] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP4] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP7] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP6] [fused_moe] using default for (68, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 33, #new-token: 2201, #cached-token: 26268, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP5] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP4] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP2] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP6] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP1] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP3] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP7] [fused_moe] using default for (1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 4, #new-token: 274, #cached-token: 3192, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP3] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP2] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP6] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP5] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP1] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP7] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP4] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] [fused_moe] using default for (274, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 39, #new-token: 2193, #cached-token: 31131, token usage: 0.00, #running-req: 38, #queue-req: 0, 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 68, #new-token: 4217, #cached-token: 54285, token usage: 0.00, #running-req: 77, #queue-req: 0, 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 63, #new-token: 4304, #cached-token: 50297, token usage: 0.00, #running-req: 145, #queue-req: 0, 
[2025-10-10 10:10:04 TP0] Prefill batch. #new-seq: 151, #new-token: 9207, #cached-token: 120580, token usage: 0.00, #running-req: 208, #queue-req: 0, 
[2025-10-10 10:10:05 TP0] Prefill batch. #new-seq: 137, #new-token: 8626, #cached-token: 109391, token usage: 0.01, #running-req: 359, #queue-req: 0, 
[2025-10-10 10:10:05 TP0] Prefill batch. #new-seq: 269, #new-token: 16308, #cached-token: 214845, token usage: 0.01, #running-req: 496, #queue-req: 5, 
[2025-10-10 10:10:05 TP0] Prefill batch. #new-seq: 253, #new-token: 16350, #cached-token: 202080, token usage: 0.01, #running-req: 765, #queue-req: 25, 
[2025-10-10 10:10:06 TP0] Prefill batch. #new-seq: 258, #new-token: 16354, #cached-token: 206103, token usage: 0.02, #running-req: 1018, #queue-req: 43, 
[2025-10-10 10:10:07 TP0] Prefill batch. #new-seq: 43, #new-token: 2636, #cached-token: 34360, token usage: 0.02, #running-req: 1276, #queue-req: 0, 
[2025-10-10 10:10:18 TP0] Decode batch. #running-req: 1319, #token: 126666, token usage: 0.04, cuda graph: False, gen throughput (token/s): 1681.11, #queue-req: 0, 
[2025-10-10 10:10:18] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:19] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:19] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:20] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:21] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22 TP0] Decode batch. #running-req: 1245, #token: 169844, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12546.81, #queue-req: 0, 
[2025-10-10 10:10:22] INFO:     127.0.0.1:48618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:22] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:23] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:24] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (1021, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (1007, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (996, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (987, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (975, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (966, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (961, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (949, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (937, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (925, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (916, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:25] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP4] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP5] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP0] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP6] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP2] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP1] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP3] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:25 TP7] [fused_moe] using default for (904, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (895, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (883, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (875, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (866, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (858, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] Decode batch. #running-req: 866, #token: 155195, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11782.01, #queue-req: 0, 
[2025-10-10 10:10:26] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (851, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (842, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (829, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (818, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (810, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (805, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (794, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP1] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP3] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:26 TP4] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP5] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP7] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP0] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP2] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:26 TP6] [fused_moe] using default for (782, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (771, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (761, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (753, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (746, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (738, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (728, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (718, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (709, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (703, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (691, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (682, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:27] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP4] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP5] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP1] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP0] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP3] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP7] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP2] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:27 TP6] [fused_moe] using default for (672, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (665, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (653, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (644, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (627, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (619, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (607, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (594, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (584, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (572, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (566, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (558, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (546, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:28] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP6] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP5] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP4] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP2] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP1] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP0] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP3] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:28 TP7] [fused_moe] using default for (540, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP1] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP4] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP0] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP5] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP3] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP7] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP6] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP2] [fused_moe] using default for (532, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP1] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP4] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP5] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP3] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP7] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP0] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP2] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP6] [fused_moe] using default for (520, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP5] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP1] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP3] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP4] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP7] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP0] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP2] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29 TP6] [fused_moe] using default for (516, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:29] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29 TP0] Decode batch. #running-req: 497, #token: 110544, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8930.42, #queue-req: 0, 
[2025-10-10 10:10:29] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:29] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:30] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31 TP0] Decode batch. #running-req: 306, #token: 81574, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8940.70, #queue-req: 0, 
[2025-10-10 10:10:31] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:31] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32 TP0] Decode batch. #running-req: 246, #token: 77488, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7999.00, #queue-req: 0, 
[2025-10-10 10:10:32] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:32] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33 TP0] Decode batch. #running-req: 235, #token: 83062, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7300.58, #queue-req: 0, 
[2025-10-10 10:10:33] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:33] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:34] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:35 TP0] Decode batch. #running-req: 224, #token: 88452, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7552.68, #queue-req: 0, 
[2025-10-10 10:10:35] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:35] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:36 TP0] Decode batch. #running-req: 222, #token: 96116, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7557.48, #queue-req: 0, 
[2025-10-10 10:10:36] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:37] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:37 TP0] Decode batch. #running-req: 220, #token: 104469, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6661.55, #queue-req: 0, 
[2025-10-10 10:10:37] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:38 TP0] Decode batch. #running-req: 219, #token: 112727, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6908.57, #queue-req: 0, 
[2025-10-10 10:10:40 TP0] Decode batch. #running-req: 219, #token: 121487, token usage: 0.04, cuda graph: True, gen throughput (token/s): 7047.61, #queue-req: 0, 
[2025-10-10 10:10:40] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41 TP0] Decode batch. #running-req: 218, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 7247.37, #queue-req: 0, 
[2025-10-10 10:10:41] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:41] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:53] INFO:     127.0.0.1:57170 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-10 10:10:53 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-10 10:10:54] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 19752, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP3] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP1] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP5] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP4] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP2] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP7] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP6] [fused_moe] using default for (23, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 6040, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP1] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP3] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP2] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP5] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP4] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP7] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP6] [fused_moe] using default for (7, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52372, token usage: 0.00, #running-req: 31, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP3] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP1] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP2] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP5] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP4] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP7] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP6] [fused_moe] using default for (61, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 47265, token usage: 0.00, #running-req: 92, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP5] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP4] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP3] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP1] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP2] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP6] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP7] [fused_moe] using default for (55, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52726, token usage: 0.00, #running-req: 147, #queue-req: 0, 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52235, token usage: 0.01, #running-req: 208, #queue-req: 0, 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52375, token usage: 0.01, #running-req: 269, #queue-req: 0, 
[2025-10-10 10:10:54 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53324, token usage: 0.01, #running-req: 330, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP0] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP3] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP5] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP2] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP1] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP4] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP6] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:54 TP7] [fused_moe] using default for (62, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53569, token usage: 0.01, #running-req: 392, #queue-req: 0, 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52098, token usage: 0.01, #running-req: 454, #queue-req: 0, 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 56642, token usage: 0.01, #running-req: 515, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP3] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP2] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP1] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP5] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP6] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP7] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP4] [fused_moe] using default for (66, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 53335, token usage: 0.01, #running-req: 581, #queue-req: 0, 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 199, #new-token: 199, #cached-token: 171364, token usage: 0.02, #running-req: 643, #queue-req: 0, 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP5] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP1] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP3] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP4] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP2] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP7] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP6] [fused_moe] using default for (199, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 64059, token usage: 0.02, #running-req: 842, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP5] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP4] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP3] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP1] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP7] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP6] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP2] [fused_moe] using default for (75, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 45208, token usage: 0.02, #running-req: 917, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP5] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP4] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP2] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP6] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP1] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP3] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP7] [fused_moe] using default for (52, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP4] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP5] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP7] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP1] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP3] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP0] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP6] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:55 TP2] [fused_moe] using default for (969, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 19, #new-token: 19, #cached-token: 16198, token usage: 0.02, #running-req: 969, #queue-req: 0, 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP5] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP6] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP4] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP7] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP3] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP1] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP2] [fused_moe] using default for (19, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:52190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 70, #new-token: 70, #cached-token: 60612, token usage: 0.02, #running-req: 988, #queue-req: 0, 
[2025-10-10 10:10:56] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56 TP5] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP1] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP3] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP4] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP2] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56 TP6] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP7] [fused_moe] using default for (70, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:10:56] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP6] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP4] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP2] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 38, #new-token: 38, #cached-token: 32804, token usage: 0.02, #running-req: 1058, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP5] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP1] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP7] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP3] [fused_moe] using default for (38, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 64173, token usage: 0.02, #running-req: 1096, #queue-req: 0, 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51902, token usage: 0.02, #running-req: 1171, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP5] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP4] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP3] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP1] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP7] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP6] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP2] [fused_moe] using default for (60, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 55163, token usage: 0.02, #running-req: 1231, #queue-req: 0, 
[2025-10-10 10:10:56 TP0] Prefill batch. #new-seq: 24, #new-token: 24, #cached-token: 20668, token usage: 0.02, #running-req: 1295, #queue-req: 0, 
[2025-10-10 10:11:00] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:01 TP0] Decode batch. #running-req: 1168, #token: 116933, token usage: 0.04, cuda graph: False, gen throughput (token/s): 2159.07, #queue-req: 0, 
[2025-10-10 10:11:01] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:01] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:01] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:01] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:02] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:03] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:49472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04 TP0] Decode batch. #running-req: 1076, #token: 151054, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11976.55, #queue-req: 0, 
[2025-10-10 10:11:04] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:04] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP0] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP3] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP1] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP5] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP6] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP4] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP2] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP7] [fused_moe] using default for (1014, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP3] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP1] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP5] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP4] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP0] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP7] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP6] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP2] [fused_moe] using default for (998, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP5] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP1] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP3] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP7] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP4] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP0] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP2] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP6] [fused_moe] using default for (976, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:55080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:49446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP1] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP3] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP5] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP4] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP7] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP0] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP2] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP6] [fused_moe] using default for (956, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:05] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP1] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP5] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP4] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP0] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP2] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP6] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP3] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:05 TP7] [fused_moe] using default for (954, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (943, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (930, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (917, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (911, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (905, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (900, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (884, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (871, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (862, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:06] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (856, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP0] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP1] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP3] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP7] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP5] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP4] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP2] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:06 TP6] [fused_moe] using default for (846, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (840, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (832, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (821, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (808, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (797, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (776, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (764, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (759, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (747, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP5] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP3] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP7] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:07 TP4] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP1] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP0] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP6] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:07 TP2] [fused_moe] using default for (735, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (726, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (719, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] Decode batch. #running-req: 726, #token: 133426, token usage: 0.04, cuda graph: False, gen throughput (token/s): 10821.32, #queue-req: 0, 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (713, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (705, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (693, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (683, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (666, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (659, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (646, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (641, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (628, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:08] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP5] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP0] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP6] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP2] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP4] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP1] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP3] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:08 TP7] [fused_moe] using default for (621, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (617, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (615, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (606, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (602, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (595, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (589, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (581, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (559, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (551, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (543, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (535, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:09] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP1] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP4] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP3] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP5] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP7] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP0] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP2] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:09 TP6] [fused_moe] using default for (528, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP1] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP5] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP4] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP3] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP7] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP0] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP2] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10 TP6] [fused_moe] using default for (514, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:10] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10 TP0] Decode batch. #running-req: 434, #token: 98653, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8415.75, #queue-req: 0, 
[2025-10-10 10:11:10] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:10] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:11] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:12] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13 TP0] Decode batch. #running-req: 281, #token: 76808, token usage: 0.02, cuda graph: True, gen throughput (token/s): 6006.93, #queue-req: 0, 
[2025-10-10 10:11:13] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:13] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:59734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14 TP0] Decode batch. #running-req: 242, #token: 76611, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7605.29, #queue-req: 0, 
[2025-10-10 10:11:14] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:14] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15 TP0] Decode batch. #running-req: 226, #token: 80211, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7407.54, #queue-req: 0, 
[2025-10-10 10:11:15] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:15] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:16] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:17 TP0] Decode batch. #running-req: 220, #token: 87167, token usage: 0.03, cuda graph: True, gen throughput (token/s): 5921.25, #queue-req: 0, 
[2025-10-10 10:11:17] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:17] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:18] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:18 TP0] Decode batch. #running-req: 217, #token: 94589, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6299.89, #queue-req: 0, 
[2025-10-10 10:11:19] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:19] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:19] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:19 TP0] Decode batch. #running-req: 215, #token: 101801, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7315.55, #queue-req: 0, 
[2025-10-10 10:11:20 TP0] Decode batch. #running-req: 214, #token: 110361, token usage: 0.03, cuda graph: True, gen throughput (token/s): 7187.98, #queue-req: 0, 
[2025-10-10 10:11:22 TP0] Decode batch. #running-req: 214, #token: 118921, token usage: 0.04, cuda graph: True, gen throughput (token/s): 6965.29, #queue-req: 0, 
[2025-10-10 10:11:23] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:23] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:35] INFO:     127.0.0.1:57738 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-10 10:11:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 795, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-10 10:11:35] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 863, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 42, #new-token: 42, #cached-token: 36024, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (42, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 45, #new-token: 45, #cached-token: 38659, token usage: 0.00, #running-req: 43, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (45, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 35, #new-token: 35, #cached-token: 30152, token usage: 0.00, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (35, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51717, token usage: 0.00, #running-req: 123, #queue-req: 0, 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51647, token usage: 0.00, #running-req: 183, #queue-req: 0, 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 54010, token usage: 0.01, #running-req: 243, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (63, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57656, token usage: 0.01, #running-req: 306, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (67, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49148, token usage: 0.01, #running-req: 373, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (57, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51401, token usage: 0.01, #running-req: 430, #queue-req: 0, 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 59060, token usage: 0.01, #running-req: 490, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP1] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP3] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP5] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP4] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP6] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP2] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP7] [fused_moe] using default for (69, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52557, token usage: 0.01, #running-req: 559, #queue-req: 0, 
[2025-10-10 10:11:36 TP0] Prefill batch. #new-seq: 72, #new-token: 72, #cached-token: 61741, token usage: 0.01, #running-req: 620, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52521, token usage: 0.01, #running-req: 692, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 40, #new-token: 40, #cached-token: 34416, token usage: 0.02, #running-req: 753, #queue-req: 0, 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP4] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP5] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP1] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP3] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP7] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP6] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP2] [fused_moe] using default for (793, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] Decode batch. #running-req: 793, #token: 52186, token usage: 0.02, cuda graph: False, gen throughput (token/s): 614.55, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 33, #new-token: 33, #cached-token: 28544, token usage: 0.02, #running-req: 793, #queue-req: 0, 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP4] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP5] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP6] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP7] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP3] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP2] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP1] [fused_moe] using default for (33, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 50620, token usage: 0.02, #running-req: 826, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP4] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP5] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP6] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP3] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP2] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP7] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP1] [fused_moe] using default for (59, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 49049, token usage: 0.02, #running-req: 885, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51799, token usage: 0.02, #running-req: 942, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 51842, token usage: 0.02, #running-req: 1002, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52475, token usage: 0.02, #running-req: 1062, #queue-req: 0, 
[2025-10-10 10:11:37 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52403, token usage: 0.02, #running-req: 1123, #queue-req: 0, 
[2025-10-10 10:11:38 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 52587, token usage: 0.02, #running-req: 1184, #queue-req: 0, 
[2025-10-10 10:11:38 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 57897, token usage: 0.03, #running-req: 1245, #queue-req: 0, 
[2025-10-10 10:11:38 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5959, token usage: 0.03, #running-req: 1312, #queue-req: 0, 
[2025-10-10 10:11:41] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42 TP0] Decode batch. #running-req: 1318, #token: 138267, token usage: 0.04, cuda graph: False, gen throughput (token/s): 11008.93, #queue-req: 0, 
[2025-10-10 10:11:42] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:42] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:40042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:43] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:44] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:33694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:45] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46 TP0] Decode batch. #running-req: 1180, #token: 170040, token usage: 0.05, cuda graph: False, gen throughput (token/s): 12231.86, #queue-req: 0, 
[2025-10-10 10:11:46] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:46] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP3] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP0] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP4] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP5] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP2] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP1] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP6] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47 TP7] [fused_moe] using default for (1020, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:47] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:35282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:47] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (990, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (963, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (955, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (942, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (914, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (909, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (903, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (897, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:48] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP0] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP3] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP7] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP5] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP1] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP6] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP2] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:48 TP4] [fused_moe] using default for (889, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (878, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (852, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (839, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (833, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (826, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (813, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (798, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (787, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] Decode batch. #running-req: 798, #token: 149722, token usage: 0.05, cuda graph: False, gen throughput (token/s): 11062.61, #queue-req: 0, 
[2025-10-10 10:11:49] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:38242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:49] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP2] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP4] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP6] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP3] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP7] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP0] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP1] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:49 TP5] [fused_moe] using default for (777, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (770, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (745, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (736, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (734, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (725, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (714, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (708, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (700, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (694, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP4] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP2] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP6] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP3] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP0] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP7] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP1] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50 TP5] [fused_moe] using default for (688, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:50] INFO:     127.0.0.1:57924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:50] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (673, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (657, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (645, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (636, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (599, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (597, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (574, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (563, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP1] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP5] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP4] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:40408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51 TP3] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP7] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP0] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP2] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51 TP6] [fused_moe] using default for (552, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:51] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:51] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP3] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP1] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP4] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP5] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP0] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP6] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP2] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP7] [fused_moe] using default for (538, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP3] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP2] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP5] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP1] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP4] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP7] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP0] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP6] [fused_moe] using default for (534, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP4] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP1] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP5] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP0] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP3] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP7] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP2] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP6] [fused_moe] using default for (527, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP4] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP5] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP1] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP0] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP3] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP7] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP2] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52 TP6] [fused_moe] using default for (518, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_Token', True, False) 
[2025-10-10 10:11:52] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52 TP0] Decode batch. #running-req: 454, #token: 104416, token usage: 0.03, cuda graph: True, gen throughput (token/s): 8394.93, #queue-req: 0, 
[2025-10-10 10:11:52] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:34598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:36820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:37016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:52] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:38096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:53] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54 TP0] Decode batch. #running-req: 272, #token: 75446, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8602.85, #queue-req: 0, 
[2025-10-10 10:11:54] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:54] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55 TP0] Decode batch. #running-req: 235, #token: 74823, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7487.30, #queue-req: 0, 
[2025-10-10 10:11:55] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:55] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:33590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:56] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57 TP0] Decode batch. #running-req: 217, #token: 77581, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7335.42, #queue-req: 0, 
[2025-10-10 10:11:57] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:57] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:58 TP0] Decode batch. #running-req: 209, #token: 84096, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6844.73, #queue-req: 0, 
[2025-10-10 10:11:58] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:59] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:11:59 TP0] Decode batch. #running-req: 207, #token: 91572, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6465.18, #queue-req: 0, 
[2025-10-10 10:11:59] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:00] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:00] INFO:     127.0.0.1:59306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:00 TP0] Decode batch. #running-req: 204, #token: 98341, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6964.52, #queue-req: 0, 
[2025-10-10 10:12:01 TP0] Decode batch. #running-req: 204, #token: 106501, token usage: 0.03, cuda graph: True, gen throughput (token/s): 6646.04, #queue-req: 0, 
[2025-10-10 10:12:03 TP0] Decode batch. #running-req: 204, #token: 114661, token usage: 0.04, cuda graph: True, gen throughput (token/s): 6889.19, #queue-req: 0, 
[2025-10-10 10:12:03] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:03] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:34938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:37980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:04] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-10 10:12:10] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-10 10:12:10] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
